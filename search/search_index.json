{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Introduction", "text": "<p>This is my digital and public garden, inspired by digital gardens fromderelict.garden project and the blue book initiative by Lyz from which this memo quickly bootstraped.</p> <p>A memorandum or a briefing note is a written message that is typically used in a professional setting. Commonly abbreviated \"memo,\"  these messages are usually brief and are designed to be easily and quickly understood.</p> <p>Thanks to the above definition of memo, that's why this wiki/memo/ blue-book was born in the first place. I'll try to be brief but not shallow. For longer posts and other themes, you can always check my blog</p> <p>To follow the updates of this site, subscribe to any of the RSS feeds.</p>"}, {"location": "#visiting-the-garden", "title": "Visiting the garden", "text": "<p>Welcome, foreigner! This content is not extensive but serves to aid my long term goals in learning. </p> <p>You can choose any category or article that sparks you curiosity on the left navigation pane. </p> <p>You can also use it as a reference, by using the top search field or by cloning the original author's git repository and using grep like tools.</p>"}, {"location": "#history", "title": "History", "text": "<p>Like the previous author (Lyz) did, I also tried writing blogs with no sucess in the past. The content is most static - even though you can  jump in and perform minor adjustments/changes. But if you are a prolific writer, as time passes by, your content are bound to increase and get lost in time. </p> <p>The sheer amount of content I read and work on during day to day situations are huge. Having incremental writing could aid tackle these issues and remove  guilty effect of missing out on something. Gwern's Long Content principle does apply to my style and I'm trying to follow it!</p> <p>Lyz created the mkdocs-newsletter so you can subscribe and follow along with the changes.</p>"}, {"location": "#contributing", "title": "Contributing", "text": "<p>If you find a mistake or want to add new content, please make the changes. You can use the edit button on the top right of any article to add them in a pull request, if you don't know what that means, you can always open an issue or send me an email.</p>"}, {"location": "#thank-you", "title": "Thank you", "text": ""}, {"location": "ansible_snippets/", "title": "Ansible Snippets", "text": ""}, {"location": "ansible_snippets/#get-the-hosts-of-a-dynamic-ansible-inventory", "title": "Get the hosts of a dynamic ansible inventory", "text": "<pre><code>ansible-inventory -i environments/production --graph\n</code></pre> <p>You can also use the <code>--list</code> flag to get more info of the hosts.</p>"}, {"location": "ansible_snippets/#speed-up-the-stat-module", "title": "Speed up the stat module", "text": "<p>The <code>stat</code> module calculates the checksum and the md5 of the file in order to get the required data. If you just want to check if the file exists use:</p> <pre><code>- name: Verify swapfile status\nstat:\npath: \"{{ common_swapfile_location }}\"\nget_checksum: no\nget_md5: no\nget_mime: no\nget_attributes: no\nregister: swap_status\nchanged_when: not swap_status.stat.exists\n</code></pre>"}, {"location": "ansible_snippets/#stop-running-docker-containers", "title": "Stop running docker containers", "text": "<pre><code>- name: Get running containers\ndocker_host_info:\ncontainers: yes\nregister: docker_info\n\n- name: Stop running containers\ndocker_container:\nname: \"{{ item }}\"\nstate: stopped\nloop: \"{{ docker_info.containers | map(attribute='Id') | list }}\"\n</code></pre>"}, {"location": "ansible_snippets/#moving-a-file-remotely", "title": "Moving a file remotely", "text": "<p>Funnily enough, you can't without a <code>command</code>. You could use the <code>copy</code> module with:</p> <pre><code>- name: Copy files from foo to bar\ncopy:\nremote_src: True\nsrc: /path/to/foo\ndest: /path/to/bar\n\n- name: Remove old files foo\nfile: path=/path/to/foo state=absent\n</code></pre> <p>But that doesn't move, it copies and removes, which is not the same.</p> <p>To make the <code>command</code> idempotent you can use a <code>stat</code> task before.</p> <pre><code>- name: stat foo\nstat: path=/path/to/foo\nregister: foo_stat\n\n- name: Move foo to bar\ncommand: mv /path/to/foo /path/to/bar\nwhen: foo_stat.stat.exists\n</code></pre>"}, {"location": "authentik/", "title": "Authentik", "text": "<p>Authentik is an open-source Identity Provider focused on flexibility and versatility.</p> <p>What I like:</p> <ul> <li>Is maintained and popular</li> <li>It has a clean interface</li> <li>They have their own terraform provider Oo!</li> </ul> <p>What I don't like:</p> <ul> <li> <p>It's heavy focused on GUI interaction, but you can export the configuration to YAML files to be applied without the GUI interaction.</p> </li> <li> <p>The documentation is oriented to developers and not users. It's a little difficult to get a grasp on how to do things in the platform without following blog posts.</p> </li> </ul>"}, {"location": "authentik/#installation", "title": "Installation", "text": "<p>You can install it with Kubernetes or with <code>docker-compose</code>. I'm going to do the second.</p> <p>Download the latest <code>docker-compose.yml</code> from here. Place it in a directory of your choice.</p> <p>If this is a fresh authentik install run the following commands to generate a password:</p> <pre><code># You can also use openssl instead: `openssl rand -base64 36`\nsudo apt-get install -y pwgen\n# Because of a PostgreSQL limitation, only passwords up to 99 chars are supported\n# See https://www.postgresql.org/message-id/09512C4F-8CB9-4021-B455-EF4C4F0D55A0@amazon.com\necho \"PG_PASS=$(pwgen -s 40 1)\" &gt;&gt; .env\necho \"AUTHENTIK_SECRET_KEY=$(pwgen -s 50 1)\" &gt;&gt; .env\n</code></pre> <p>It is also recommended to configure global email credentials. These are used by authentik to notify you about alerts and configuration issues. They can also be used by Email stages to send verification/recovery emails.</p> <p>Append this block to your .env file</p> <pre><code># SMTP Host Emails are sent to\nAUTHENTIK_EMAIL__HOST=localhost\nAUTHENTIK_EMAIL__PORT=25\n# Optionally authenticate (don't add quotation marks to your password)\nAUTHENTIK_EMAIL__USERNAME=\nAUTHENTIK_EMAIL__PASSWORD=\n# Use StartTLS\nAUTHENTIK_EMAIL__USE_TLS=false\n# Use SSL\nAUTHENTIK_EMAIL__USE_SSL=false\nAUTHENTIK_EMAIL__TIMEOUT=10\n# Email address authentik will send from, should have a correct @domain\nAUTHENTIK_EMAIL__FROM=authentik@localhost\n</code></pre> <p>By default, authentik listens on port 9000 for HTTP and 9443 for HTTPS. To change this, you can set the following variables in .env:</p> <pre><code>AUTHENTIK_PORT_HTTP=80\nAUTHENTIK_PORT_HTTPS=443\n</code></pre> <p>You may need to tweak the <code>volumes</code> and the <code>networks</code> sections of the <code>docker-compose.yml</code> to your liking.</p> <p>Once everything is set you can run <code>docker-compose up</code> to test everything is working.</p> <p>In your browser, navigate to authentik\u2019s initial setup page https://auth.home.yourdomain.com/if/flow/initial-setup/.</p> <p>Set the email and password for the default admin user, <code>akadmin</code>. You\u2019re now logged in.</p>"}, {"location": "authentik/#usage", "title": "Usage", "text": ""}, {"location": "authentik/#terraform", "title": "Terraform", "text": "<p>You can use <code>terraform</code> to configure authentik! <code>&lt;3</code>.</p>"}, {"location": "authentik/#configure-the-provider", "title": "Configure the provider", "text": "<p>To configure the provider you need to specify the url and an Authentik API token, keeping in mind that whoever gets access to this information will have access and full permissions on your Authentik instance it's critical that you store this information well. We'll use <code>sops</code> to encrypt the token with GPG..</p> <p>First create an Authentik user under <code>Admin interface/Directory/Users</code> with the next attributes:</p> <ul> <li>Username: <code>terraform</code></li> <li>Name: <code>Terraform</code></li> <li>Path: <code>infra</code></li> <li>Groups: <code>Admin</code></li> </ul> <p>Then create a token with name <code>Terraform</code> under <code>Directory/Tokens &amp; App passwords</code>, copy it to your clipboard.</p> <p>Configure <code>sops</code> by defining the gpg keys in a <code>.sops.yaml</code> file at the top of your repository:</p> <pre><code>---\ncreation_rules:\n- pgp: &gt;-\n2829BASDFHWEGWG23WDSLKGL323534J35LKWERQS,\n2GEFDBW349YHEDOH2T0GE9RH0NEORIG342RFSLHH\n</code></pre> <p>Then create the secrets file with the command <code>sops secrets.enc.json</code> somewhere in your terraform repository. For example:</p> <pre><code>{\n\"authentik_token\": \"paste the token here\"\n}\n</code></pre> <pre><code>terraform {\n  required_providers {\n    authentik = {\n      source = \"goauthentik/authentik\"\n      version = \"~&gt; 2023.1.1\"\n    }\n    sops = {\n      source = \"carlpett/sops\"\n      version = \"~&gt; 0.5\"\n    }\n  }\n}\n\nprovider \"authentik\" {\n  url   = \"https://oauth.your-domain.org\"\n  token = data.sops_file.secrets.data[\"authentik_token\"]\n}\n</code></pre>"}, {"location": "authentik/#configure-some-common-applications", "title": "Configure some common applications", "text": "<p>You have some guides to connect some popular applications</p>"}, {"location": "authentik/#gitea", "title": "Gitea", "text": "<p>You can follow the Authentik Gitea docs or you can use the next terraform snippet:</p> <pre><code># ----------------\n# --    Data    --\n# ----------------\n\ndata \"authentik_flow\" \"default-authorization-flow\" {\n  slug = \"default-provider-authorization-implicit-consent\"\n}\n\n# -----------------------\n# --    Application    --\n# -----------------------\n\nresource \"authentik_application\" \"gitea\" {\n  name              = \"Gitea\"\n  slug              = \"gitea\"\n  protocol_provider = authentik_provider_oauth2.gitea.id\n  meta_icon = \"application-icons/gitea.svg\"\n  lifecycle {\n    ignore_changes = [\n      # The terraform provider is continuously changing the attribute even though it's set\n      meta_icon,\n    ]\n  }\n}\n\n# --------------------------\n# --    Oauth provider    --\n# --------------------------\n\nresource \"authentik_provider_oauth2\" \"gitea\" {\n  name               = \"Gitea\"\n  client_id = \"gitea\"\n  authorization_flow = data.authentik_flow.default-authorization-flow.id\n  property_mappings = [\n    authentik_scope_mapping.gitea.id,\n    data.authentik_scope_mapping.email.id,\n    data.authentik_scope_mapping.openid.id,\n    data.authentik_scope_mapping.profile.id,\n  ]\n  redirect_uris = [\n    \"https://git.your-domain.org/user/oauth2/authentik/callback\",\n  ]\n  signing_key = data.authentik_certificate_key_pair.default.id\n}\n\ndata \"authentik_certificate_key_pair\" \"default\" {\n  name = \"authentik Self-signed Certificate\"\n}\n\n# -------------------------\n# --    Scope mapping    --\n# -------------------------\n\nresource \"authentik_scope_mapping\" \"gitea\" {\n  name       = \"Gitea\"\n  scope_name = \"gitea\"\n  expression = &lt;&lt;EOF\ngitea_claims = {}\nif request.user.ak_groups.filter(name=\"Users\").exists():\n    gitea_claims[\"gitea\"]= \"user\"\nif request.user.ak_groups.filter(name=\"Admins\").exists():\n    gitea_claims[\"gitea\"]= \"admin\"\n\nreturn gitea_claims\nEOF\n}\n\ndata \"authentik_scope_mapping\" \"email\" {\n  managed = \"goauthentik.io/providers/oauth2/scope-email\"\n}\n\ndata \"authentik_scope_mapping\" \"openid\" {\n  managed = \"goauthentik.io/providers/oauth2/scope-openid\"\n}\n\ndata \"authentik_scope_mapping\" \"profile\" {\n  managed = \"goauthentik.io/providers/oauth2/scope-profile\"\n}\n\n# -------------------\n# --    Outputs    --\n# -------------------\n\noutput \"gitea_oauth_id\" {\n  value = authentik_provider_oauth2.gitea.client_id\n}\n\noutput \"gitea_oauth_secret\" {\n  value = authentik_provider_oauth2.gitea.client_secret\n}\n</code></pre> <p>It assumes that:</p> <ul> <li>You've changed <code>git.your-domain.org</code> with your gitea domain.</li> <li>The gitea logo is mounted in the docker directory <code>/media/application-icons/gitea.svg</code>.</li> </ul> <p>Gitea can be configured through terraform too. There is an official provider that doesn't work, there's a [fork that does though[(https://registry.terraform.io/providers/Lerentis/gitea/latest/docs). Sadly it doesn't yet support configuring Oauth Authentication sources. So you'll need to configure it manually.</p> <p>Be careful <code>gitea_oauth2_app</code> looks to be the right resource to do that, but instead it configures Gitea to be the Oauth provider, not a consumer.</p>"}, {"location": "authentik/#configure-the-invitation-flow", "title": "Configure the invitation flow", "text": "<p>Let's assume that we have two groups (Admins and Users) created under <code>Directory/Groups</code> and that we want to configure an invitation link for a user to be added directly on the <code>Admins</code> group.</p> <p>Authentik works by defining Stages and Flows. Stages are the steps you need to follow to complete a procedure, and a flow is the procedure itself.</p> <p>You create Stages by: * Going to the Admin interface * Going to Flows &amp; Stages/Stages * Click on Create</p> <p>To be able to complete the invitation through link we need to define the next stages:</p> <ul> <li> <p>An Invitation Stage: This stage represents the moment an admin chooses to create an invitation for a user.    Graphically you would need to:</p> </li> <li> <p>Click on Create</p> </li> <li>Select Invitation Stage</li> <li>Fill the form with the next data:<ul> <li>Name: enrollment-invitation</li> <li>Uncheck the <code>Continue flow without invitation</code> as we don't want users to be able to register without the invitation.</li> </ul> </li> <li>Click Finish</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_stage_invitation\" \"default\" {\nname                             = \"enrollment-invitation\"\ncontinue_flow_without_invitation = false\n}\n</code></pre> <ul> <li> <p>An User Write Stage: This is when the user will be created but it won't show up as the username and password are not yet selected.   Graphically you would need to:</p> <ul> <li>Click on Create</li> <li>Select User Write Stage</li> <li>Click on Next</li> <li>Fill the form with the next data:</li> <li>Name: enrollment-invitation-admin-write</li> <li>Enable the <code>Can Create Users</code> flag.</li> <li>If you want users to validate their email leave \"Create users as inactive\" enabled, otherwise disable it.</li> <li>Select the group you want the user to be added to. I don't yet know how to select more than one group</li> <li>Click on Finish</li> </ul> </li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_stage_user_write\" \"admin_write\" {\nname                     = \"enrollment-invitation-admin-write\"\ncreate_users_as_inactive = true\ncreate_users_group       = authentik_group.admins.id\n}\n</code></pre> <p>Where <code>authentik_group.admin</code> is defined as:</p> <pre><code>resource \"authentik_group\" \"admins\" {\nname         = \"Admins\"\nis_superuser = true\nusers = [\ndata.authentik_user.user_1.id,\ndata.authentik_user.user_2.id,\n]\n}\n\ndata \"authentik_user\" \"user_1\" {\nusername = \"user_1\"\n}\n\ndata \"authentik_user\" \"user_2\" {\nusername = \"user_2\"\n}\n</code></pre> <ul> <li>Email Confirmation Stage: This is when the user gets an email to confirm that it has access to it</li> </ul> <p>Graphically you would need to:</p> <ul> <li>Click on Create</li> <li>Select Email Stage</li> <li>Click on Next<ul> <li>Name: email-account-confirmation</li> <li>Subject: Account confirmation</li> <li>Template: Account confirmation</li> </ul> </li> <li>Click on Finish</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_stage_email\" \"account_confirmation\" {\nname                     = \"email-account-confirmation\"\nactivate_user_on_success = true\nsubject                  = \"Authentik Account Confirmation\"\ntemplate                 = \"email/account_confirmation.html\"\ntimeout                  = 10\n}\n</code></pre> <p>Create the invitation Flow:</p> <p>Graphically you would need to:</p> <ul> <li>Go to <code>Flows &amp; Stages/Flows</code></li> <li>Click on Create</li> <li>Fill the form with the next data:<ul> <li>Name: Enrollment Invitation Admin</li> <li>Title: Enrollment Invitation Admin</li> <li>Designation: Enrollment</li> <li>Unfold the Behavior settings to enable the Compatibility mode</li> </ul> </li> <li>Click Create</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_flow\" \"enrollment_admin\" {\nname        = \"Enrollment invitation admin\"\ntitle       = \"Enrollment invitation admin\"\nslug        = \"enrollment-invitation-admin\"\ndesignation = \"enrollment\"\n}\n</code></pre> <p>We need to define how the flow is going to behave by adding the different the stage bindings:</p> <ul> <li>Bind the Invitation admin stage:</li> </ul> <p>Graphically you would need to:</p> <ul> <li>Click on the flow we just created <code>enrollment-invitation-admin</code></li> <li>Click on <code>Stage Bindings</code></li> <li>Click on <code>Bind Stage</code></li> <li>Fill the form with the next data:<ul> <li>Stage: select <code>enrollment-invitation-admin</code></li> <li>Order: 10</li> </ul> </li> <li>Click Create</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_flow_stage_binding\" \"invitation_creation\" {\ntarget = authentik_flow.enrollment_admin.uuid\nstage  = authentik_stage_invitation.default.id\norder  = 10\n}\n</code></pre> <ul> <li>Bind the Enrollment prompt stage: This is a builtin stage where the user is asked for their login information</li> </ul> <p>Graphically you would need to:</p> <ul> <li>Click on <code>Bind Stage</code></li> <li>Fill the form with the next data:<ul> <li>Stage: select <code>default-source-enrollment-prompt</code></li> <li>Order: 20</li> </ul> </li> <li>Click Create</li> <li>Click Edit Stage and configure it wit:<ul> <li>On the fields select: </li> <li>username</li> <li>name</li> <li>email</li> <li>password</li> <li>password_repeat</li> <li>Select the validation policy you have one</li> </ul> </li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_stage_prompt\" \"user_data\" {\nname = \"enrollment-user-data-prompt\"\nfields = [ authentik_stage_prompt_field.username.id,\nauthentik_stage_prompt_field.name.id,\nauthentik_stage_prompt_field.email.id,\nauthentik_stage_prompt_field.password.id,\nauthentik_stage_prompt_field.password_repeat.id,\n]\n}\n\nresource \"authentik_stage_prompt_field\" \"username\" {\nfield_key = \"username\"\nlabel     = \"Username\"\ntype      = \"text\"\norder = 200\nplaceholder = &lt;&lt;EOT\ntry:\n    return user.username\nexcept:\n    return ''\nEOT\nplaceholder_expression = true\nrequired = true\n}\n\nresource \"authentik_stage_prompt_field\" \"name\" {\nfield_key = \"name\"\nlabel     = \"Name\"\ntype      = \"text\"\norder = 201\nplaceholder = &lt;&lt;EOT\ntry:\n    return user.name\nexcept:\n    return ''\nEOT\nplaceholder_expression = true\nrequired = true\n}\n\nresource \"authentik_stage_prompt_field\" \"email\" {\nfield_key = \"email\"\nlabel     = \"Email\"\ntype      = \"email\"\norder = 202\nplaceholder = &lt;&lt;EOT\ntry:\n    return user.email\nexcept:\n    return ''\nEOT\nplaceholder_expression = true\nrequired = true\n}\n\nresource \"authentik_stage_prompt_field\" \"password\" {\nfield_key = \"password\"\nlabel     = \"Password\"\ntype      = \"password\"\norder = 300\nplaceholder = \"Password\"\nplaceholder_expression = false\nrequired = true\n}\n\nresource \"authentik_stage_prompt_field\" \"password_repeat\" {\nfield_key = \"password_repeat\"\nlabel     = \"Password (repeat)\"\ntype      = \"password\"\norder = 301\nplaceholder = \"Password (repeat)\"\nplaceholder_expression = false\nrequired = true\n}\n</code></pre> <p>We had to redefine all the <code>authentik_stage_prompt_field</code> because the terraform provider doesn't yet support the <code>data</code> resource of the <code>authentik_stage_prompt_field</code></p> <ul> <li>Bind the User write stage:</li> </ul> <p>Graphically you would need to:</p> <ul> <li>Click on <code>Bind Stage</code></li> <li>Fill the form with the next data:<ul> <li>Stage: select <code>enrollment-invitation-admin-write</code></li> <li>Order: 30</li> </ul> </li> <li>Click Create</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_flow_stage_binding\" \"invitation_user_write\" {\ntarget = authentik_flow.enrollment_admin.uuid\nstage  = authentik_stage_user_write.admin_write.id\norder  = 30\n}\n</code></pre> <ul> <li>Bind the email account confirmation stage: </li> </ul> <p>Graphically you would need to:</p> <ul> <li>Click on <code>Bind Stage</code></li> <li>Fill the form with the next data:<ul> <li>Stage: select <code>email-account-confirmation</code></li> <li>Order: 40</li> </ul> </li> <li>Click Create</li> <li>Edit the stage and make sure that you have enabled:<ul> <li>Activate pending user on success</li> <li>Use global settings</li> </ul> </li> <li>Click Update</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_flow_stage_binding\" \"invitation_account_confirmation\" {\ntarget = authentik_flow.enrollment_admin.uuid\nstage  = authentik_stage_email.account_confirmation.id\norder  = 40\n}\n</code></pre> <ul> <li>Bind the User login stage: This is a builtin stage where the user is asked to log in</li> </ul> <p>Graphically you would need to:</p> <ul> <li>Click on <code>Bind Stage</code></li> <li>Fill the form with the next data:<ul> <li>Stage: select <code>default-source-enrollment-login</code></li> <li>Order: 50</li> </ul> </li> <li>Click Create</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_flow_stage_binding\" \"invitation_login\" {\ntarget = authentik_flow.enrollment_admin.uuid\nstage  = data.authentik_stage.default_source_enrollment_login.id\norder  = 50\n}\n</code></pre>"}, {"location": "authentik/#configure-password-recovery", "title": "Configure password recovery", "text": "<p>Recovery of password is not enabled by default, to configure it you need to create two new stages:</p> <ul> <li>An identification stage:</li> </ul> <pre><code>data \"authentik_source\" \"built_in\" {\nmanaged = \"goauthentik.io/sources/inbuilt\"\n}\n\nresource \"authentik_stage_identification\" \"recovery\" {\nname           = \"recovery-authentication-identification\"\nuser_fields    = [\"username\", \"email\"]\nsources = [data.authentik_source.built_in.uuid]\ncase_insensitive_matching = true\n}\n</code></pre> <ul> <li>An Email recovery stage: </li> </ul> <pre><code>resource \"authentik_stage_email\" \"recovery\" {\nname                     = \"recovery-email\"\nactivate_user_on_success = true\nsubject                  = \"Password Recovery\"\ntemplate                 = \"email/password_reset.html\"\ntimeout                  = 10\n}\n</code></pre> <ul> <li>We will reuse two existing stages too:</li> </ul> <pre><code>data \"authentik_stage\" \"default_password_change_prompt\" {\nname = \"default-password-change-prompt\"\n}\n\ndata \"authentik_stage\" \"default_password_change_write\" {\nname = \"default-password-change-write\"\n}\n</code></pre> <p>Then we need to create the recovery flow and bind all the stages:</p> <pre><code>resource \"authentik_flow\" \"password_recovery\" {\nname        = \"Password Recovery\"\ntitle       = \"Password Recovery\"\nslug        = \"password-recovery\"\ndesignation = \"recovery\"\n}\n\nresource \"authentik_flow_stage_binding\" \"recovery_identification\" {\ntarget = authentik_flow.password_recovery.uuid\nstage  = authentik_stage_identification.recovery.id\norder  = 0\n}\n\nresource \"authentik_flow_stage_binding\" \"recovery_email\" {\ntarget = authentik_flow.password_recovery.uuid\nstage  = authentik_stage_email.recovery.id\norder  = 10\n}\n\nresource \"authentik_flow_stage_binding\" \"recovery_password_change\" {\ntarget = authentik_flow.password_recovery.uuid\nstage  = data.authentik_stage.default_password_change_prompt.id\norder  = 20\n}\n\nresource \"authentik_flow_stage_binding\" \"recovery_password_write\" {\ntarget = authentik_flow.password_recovery.uuid\nstage  = data.authentik_stage.default_password_change_write.id\norder  = 30\n}\n</code></pre> <p>Finally we need to enable it in the site's authentication flow. To be able to do change the default flow we'd need to do two manual steps, so to have all the code in terraform we will create a new tenancy for our site and a new authentication flow.</p> <p>Starting with the authentication flow we need to create the Flow, stages and stage bindings.</p> <pre><code># -----------\n# -- Flows --\n# -----------\n\nresource \"authentik_flow\" \"authentication\" {\nname        = \"Welcome to Authentik!\"\ntitle        = \"Welcome to Authentik!\"\nslug        = \"custom-authentication-flow\"\ndesignation = \"authentication\"\nauthentication = \"require_unauthenticated\"\ncompatibility_mode = false\n}\n\n# ------------\n# -- Stages --\n# ------------\n\nresource \"authentik_stage_identification\" \"authentication\" {\nname           = \"custom-authentication-identification\"\nuser_fields    = [\"username\", \"email\"]\npassword_stage = data.authentik_stage.default_authentication_password.id\ncase_insensitive_matching = true\nrecovery_flow = authentik_flow.password_recovery.uuid\n}\n\ndata \"authentik_stage\" \"default_authentication_mfa_validation\" {\nname = \"default-authentication-mfa-validation\"\n}\n\ndata \"authentik_stage\" \"default_authentication_login\" {\nname = \"default-authentication-login\"\n}\n\ndata \"authentik_stage\" \"default_authentication_password\" {\nname = \"default-authentication-password\"\n}\n\n# -------------------\n# -- Stage binding --\n# -------------------\n\nresource \"authentik_flow_stage_binding\" \"login_identification\" {\ntarget = authentik_flow.authentication.uuid\nstage  = authentik_stage_identification.authentication.id\norder  = 10\n}\n\nresource \"authentik_flow_stage_binding\" \"login_mfa\" {\ntarget = authentik_flow.authentication.uuid\nstage  = data.authentik_stage.default_authentication_mfa_validation.id\norder  = 20\n}\n\nresource \"authentik_flow_stage_binding\" \"login_login\" {\ntarget = authentik_flow.authentication.uuid\nstage  = data.authentik_stage.default_authentication_login.id\norder  = 30\n}\n</code></pre> <p>Now we can bind it to the new tenant for our site:</p> <pre><code># ------------\n# -- Tenant --\n# ------------\n\nresource \"authentik_tenant\" \"default\" {\ndomain         = \"your-domain.org\"\ndefault        = false\nbranding_title = \"Authentik\"\nbranding_logo = \"/static/dist/assets/icons/icon_left_brand.svg\"\nbranding_favicon = \"/static/dist/assets/icons/icon.png\"\nflow_authentication = authentik_flow.authentication.uuid\n  # We need to define id instead of uuid until \n  # https://github.com/goauthentik/terraform-provider-authentik/issues/305\n  # is fixed.\nflow_invalidation = data.authentik_flow.default_invalidation_flow.id\nflow_user_settings = data.authentik_flow.default_user_settings_flow.id\nflow_recovery = authentik_flow.password_recovery.uuid\n}\n\n# -----------\n# -- Flows --\n# -----------\n\ndata \"authentik_flow\" \"default_invalidation_flow\" {\nslug = \"default-invalidation-flow\"\n}\n\ndata \"authentik_flow\" \"default_user_settings_flow\" {\nslug = \"default-user-settings-flow\"\n}\n</code></pre>"}, {"location": "authentik/#use-blueprints", "title": "Use blueprints", "text": "<p>WARNING: Use the <code>terraform</code> provider instead!!!</p> <p>Blueprints offer a new way to template, automate and distribute authentik configuration. Blueprints can be used to automatically configure instances, manage config as code without any external tools, and to distribute application configs.</p> <p>Blueprints are yaml files, whose format is described further in File structure and uses YAML tags to configure the objects. It can be complicated when you first look at it, reading this example may help.</p> <p>Blueprints can be applied in one of two ways:</p> <ul> <li>As a Blueprint instance, which is a YAML file mounted into the authentik (worker) container. This file is read and applied every time it changes. Multiple instances can be created for a single blueprint file, and instances can be given context key:value attributes to configure the blueprint.</li> <li>As a Flow import, which is a YAML file uploaded via the Browser/API. This file is validated and applied directly after being uploaded, but is not further monitored/applied.</li> </ul> <p>The authentik container by default looks for blueprints in <code>/blueprints</code>. Underneath this directory, there are a couple default subdirectories:</p> <ul> <li><code>/blueprints/default</code>: Default blueprints for default flows, tenants, etc</li> <li><code>/blueprints/example</code>: Example blueprints for common configurations and flows</li> <li><code>/blueprints/system</code>: System blueprints for authentik managed Property mappings, etc</li> </ul> <p>Any additional <code>.yaml</code> file in /blueprints will be discovered and automatically instantiated, depending on their labels.</p> <p>To disable existing blueprints, an empty file can be mounted over the existing blueprint.</p> <p>File-based blueprints are automatically removed once they become unavailable, however none of the objects created by those blueprints are affected by this.</p>"}, {"location": "authentik/#export-blueprints", "title": "Export blueprints", "text": "<p>Exports from either method will contain a (potentially) long list of objects, all with hardcoded primary keys and no ability for templating/instantiation. This is because currently, authentik does not check which primary keys are used where. It is assumed that for most exports, there'll be some manual changes done regardless, to filter out unwanted objects, adjust properties, etc. That's why it may be better to use the flow export for the resources you've created rather than the global export.</p>"}, {"location": "authentik/#global-export", "title": "Global export", "text": "<p>To migrate existing configurations to blueprints, run <code>ak export_blueprint</code> within any authentik Worker container. This will output a blueprint for most currently created objects. Some objects will not be exported as they might have dependencies on other things.</p> <p>Exported blueprints don't use any of the YAML Tags, they just contain a list of entries as they are in the database.</p> <p>Note that fields which are write-only (for example, OAuth Provider's Secret Key) will not be added to the blueprint, as the serialisation logic from the API is used for blueprints.</p> <p>Additionally, default values will be skipped and not added to the blueprint.</p>"}, {"location": "authentik/#flow-export", "title": "Flow export", "text": "<p>Instead of exporting everything from a single instance, there's also the option to export a single flow with it's attached stages, policies and other objects.</p> <p>This export can be triggered via the API or the Web UI by clicking the download button in the flow list.</p>"}, {"location": "authentik/#hide-and-application-from-a-user", "title": "Hide and application from a user", "text": "<p>Application access can be configured using (Policy) Bindings. Click on an application in the applications list, and select the Policy / Group / User Bindings tab. There you can bind users/groups/policies to grant them access. When nothing is bound, everyone has access. You can use this to grant access to one or multiple users/groups, or dynamically give access using policies.</p> <p>With terraform you can use <code>authentik_policy_binding</code>, for example:</p> <pre><code>resource \"authentik_policy_binding\" \"admin\" {\ntarget = authentik_application.gitea.uuid\ngroup  = authentik_group.admins.id\norder  = 0\n}\n</code></pre>"}, {"location": "authentik/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Docs</li> <li> <p>Home</p> </li> <li> <p>Terraform provider docs</p> </li> <li>Terraform provider source code</li> </ul>"}, {"location": "aws_savings_plan/", "title": "AWS Savings plan", "text": "<p>Saving plans offer a flexible pricing model that provides savings on AWS usage. You can save up to 72 percent on your AWS compute workloads.</p> <p>!!! note \"Please don't make Jeff Bezos even richer, try to pay as less money to AWS as you can.\"</p> <p>Savings Plans provide savings beyond On-Demand rates in exchange for a commitment of using a specified amount of compute power (measured per hour) for a one or three year period.</p> <p>When you sign up for Savings Plans, the prices you'll pay for usage stays the same through the plan term. You can pay for your commitment using All Upfront, Partial upfront, or No upfront payment options.</p> <p>Plan types:</p> <ul> <li> <p>Compute Savings Plans provide the most flexibility and prices that are up     to 66 percent off of On-Demand rates. These plans automatically apply to     your EC2 instance usage, regardless of instance family (for example, m5, c5,     etc.), instance sizes (for example, c5.large, c5.xlarge, etc.), Region (for     example, us-east-1, us-east-2, etc.), operating system (for example,     Windows, Linux, etc.), or tenancy (for example, Dedicated, default,     Dedicated Host). With Compute Savings Plans, you can move a workload from C5     to M5, shift your usage from EU (Ireland) to EU (London). You can continue     to benefit from the low prices provided by Compute Savings Plans as you make     these changes.</p> </li> <li> <p>EC2 Instance Savings Plans provide savings up to 72 percent off On-Demand,     in exchange for a commitment to a specific instance family in a chosen AWS     Region (for example, M5 in Virginia). These plans automatically apply to     usage regardless of size (for example, m5.xlarge, m5.2xlarge, etc.), OS (for     example, Windows, Linux, etc.), and tenancy (Host, Dedicated, Default)     within the specified family in a Region.</p> <p>With an EC2 Instance Savings Plan, you can change your instance size within the instance family (for example, from c5.xlarge to c5.2xlarge) or the operating system (for example, from Windows to Linux), or move from Dedicated tenancy to Default and continue to receive the discounted rate provided by your EC2 Instance Savings Plan.</p> </li> <li> <p>Standard Reserved Instances: The old reservation system, you reserve an     instance type and you can get up to 72 percent of discount. The lack of     flexibility makes them inferior to the new EC2 instance plans.</p> </li> <li> <p>Convertible Reserved Instances: Same as the Standard Reserved Instances but     with more flexibility. Discounts range up to 66%, similar to the new Compute     Savings Plan, which again gives more less the same discounts with more     flexibility, so I wouldn't use this plan either.</p> </li> </ul>"}, {"location": "aws_savings_plan/#understanding-how-savings-plans-apply-to-your-aws-usage", "title": "Understanding how Savings Plans apply to your AWS usage", "text": "<p>If you have active Savings Plans, they apply automatically to your eligible AWS usage to reduce your bill.</p> <p>Savings Plans apply to your usage after the Amazon EC2 Reserved Instances (RI) are applied. Then EC2 Instance Savings Plans are applied before Compute Savings Plans because Compute Savings Plans have broader applicability.</p> <p>They calculate your potential savings percentages of each combination of eligible usage. This percentage compares the Savings Plans rates with your current On-Demand rates. Your Savings Plans are applied to your highest savings percentage first. If there are multiple usages with equal savings percentages, Savings Plans are applied to the first usage with the lowest Savings Plans rate. Savings Plans continue to apply until there are no more remaining usages, or your commitment is exhausted. Any remaining usage is charged at the On-Demand rates.</p>"}, {"location": "aws_savings_plan/#savings-plan-example", "title": "Savings plan example", "text": "<p>In this example, you have the following usage in a single hour:</p> <ul> <li>4x r5.4xlarge Linux, shared tenancy instances in us-east-1, running for the     duration of a full hour.</li> <li>1x m5.24xlarge Linux, dedicated tenancy instance in us-east-1, running for the     duration of a full hour.</li> </ul> <p>Pricing example:</p> Type On-Demand rate Compute Savings Plans rate CSP Savings percentage EC2 Instance Savings Plans rate EC2IS percentage r5.4xlarge $1.00 $0.70 30% $0.60 40% m5.24xlarge $10.00 $8.20 18% $7.80 22% <p>They've included other products in the example but I've removed them for the sake of simplicity</p>"}, {"location": "aws_savings_plan/#scenario-1-savings-plan-apply-to-all-usage", "title": "Scenario 1: Savings Plan apply to all usage", "text": "<p>You purchase a one-year, partial upfront Compute Savings Plan with a $50.00/hour commitment.</p> <p>Your Savings Plan covers all of your usage because multiplying each of your usages by the equivalent Compute Savings Plans is $47.13. This is still less than the $50.00/hour commitment.</p> <p>Without Savings Plans, you would be charged at On-Demand rates in the amount of $59.10.</p>"}, {"location": "aws_savings_plan/#scenario-2-savings-plans-apply-to-some-usage", "title": "Scenario 2: Savings Plans apply to some usage", "text": "<p>You purchase a one-year, partial upfront Compute Savings Plan with a $2.00/hour commitment.</p> <p>In any hour, your Savings Plans apply to your usage starting with the highest discount percentage (30 percent).</p> <p>Your $2.00/hour commitment is used to cover approximately 2.9 units of this usage. The remaining 1.1 units are charged at On-Demand rates, resulting in $1.14 of On-Demand charges for r5.</p> <p>The rest of your usage are also charged at On-Demand rates, resulting in $55.10 of On-Demand charges. The total On-Demand charges for this usage are $56.24.</p>"}, {"location": "aws_savings_plan/#scenario-3-savings-plans-and-ec2-reserved-instances-apply-to-the-usage", "title": "Scenario 3: Savings Plans and EC2 reserved instances apply to the usage", "text": "<p>You purchase a one-year, partial upfront Compute Savings Plan with an $18.20/hour commitment. You have two EC2 Reserved Instances (RI) for r5.4xlarge Linux shared tenancy in us-east-1.</p> <p>First, the Reserve Instances covers two of the r5.4xlarge instances. Then, the Savings Plans rate is applied to the remaining r5.4xlarge and the rest of the usage, which exhausts the hourly commitment of $18.20.</p>"}, {"location": "aws_savings_plan/#scenario-4-multiple-savings-plans-apply-to-the-usage", "title": "Scenario 4: Multiple Savings Plans apply to the usage", "text": "<p>You purchase a one-year, partial upfront EC2 Instance Family Savings Plan for the r5 family in us-east-1 with a $3.00/hour commitment. You also have a one-year, partial upfront Compute Savings Plan with a $16.80/hour commitment.</p> <p>Your EC2 Instance Family Savings Plan (r5, us-east-1) covers all of the r5.4xlarge usage because multiplying the usage by the EC2 Instance Family Savings Plan rate is $2.40. This is less than the $3.00/hour commitment.</p> <p>Next, the Compute Savings Plan is applied to rest of the resource usage, if it doesn't cover the whole expense, then On demand rates will apply.</p>"}, {"location": "aws_savings_plan/#monitoring-the-savings-plan", "title": "Monitoring the savings plan", "text": "<p>Monitoring is an important part of your Savings Plans usage. Understanding the Savings Plan that you own, how they are applying to your usage, and what usage is being covered are important parts of optimizing your costs with Savings Plans. You can monitor your usage in multiple forms.</p> <ul> <li> <p>Using the     inventory:     The Savings Plans Inventory page shows a detailed overview of the Savings     Plans that you own, or have queued for future purchase.</p> <p>To view your Inventory page:</p> <ul> <li>Open the AWS Cost Management     console.</li> <li>In the navigation pane, under Savings Plans, choose Inventory.</li> </ul> </li> <li> <p>Using the utilization     report:     Savings Plans utilization shows you the percentage of your Savings Plans     commitment that you're using across your On-Demand usage. You can use your     Savings Plans utilization report to visually understand how your Savings     Plans apply to your usage over the configured time period. Along with     a visualized graph, the report shows high-level metrics based on your     selected Savings Plan, filters, and lookback periods. Utilization is     calculated based on how your Savings Plans applied to your usage over the     lookback period.</p> <p>For example, if you have a 10 $/hour commitment, and your usage billed with Savings Plans rates totals to $9.80 for the hour, your utilization for that hour is 98 percent.</p> <p>You can find high-level metrics in the Utilization report section:</p> <ul> <li>On-Demand Spend Equivalent: The amount you would have spent on the same     usage if you didn\u2019t commit to Savings Plans. This amount is the     equivalent On-Demand cost based on current On-Demand rates.</li> <li>Savings Plans spend: Your Savings Plans commitment spend over the     lookback period.</li> <li>Total Net Savings: The amount you saved using Savings Plans commitments     over the selected time period, compared to the On-Demand cost estimate.</li> </ul> <p>To access your utilization report:</p> <ul> <li>Open the AWS Cost Management     console.</li> <li>On the navigation pane, choose Savings Plans.</li> <li>In the left pane, choose Utilization report.</li> </ul> </li> <li> <p>Using the coverage     report:     The Savings Plans coverage report shows how much of your eligible spend was     covered by your Savings Plans and how much is not covered by either Savings plan or Reserved instances based on the selected time period. </p> <p>You can find the following high-level metrics in the Coverage report section:</p> <ul> <li>Average Coverage: The aggregated Savings Plans coverage percentage based     on the selected filters and look-back period.</li> <li>Additional potential savings: Your potential savings amount based on     your Savings Plans recommendations. This is shown as a monthly amount.</li> <li>On-Demand spend not covered: The amount of eligible savings spend that     was not covered by Savings Plans or Reserved Instances over the lookback     period.</li> </ul> <p>To access your utilization report:</p> <ul> <li>Open the AWS Cost Management     console.</li> <li>On the navigation pane, choose Savings Plans.</li> <li>In the left pane, choose Coverage report.</li> </ul> <p>The columns are a bit tricky:</p> <ul> <li>\"Spend covered by Savings Plan\": Refers to the on demand usage amount that you would have paid on demand that is being covered by the Savings Plans. Not the Savings Plan amount that is applied to on demand usage.</li> </ul> <p>The coverage report of the reserved instances has the same trick on the columns:</p> <ul> <li>\"Reservation covered hours\": the column does not refer to your RI hours. This column refers to your on demand hours that was covered by Reserved Instances.</li> </ul> </li> </ul>"}, {"location": "aws_savings_plan/#doing-your-savings-plan", "title": "Doing your savings plan", "text": "<p>Go to the AWS savings plan simulator and check the different instances you were evaluating.</p>"}, {"location": "aws_snippets/", "title": "AWS Snippets", "text": ""}, {"location": "aws_snippets/#find-if-external-ip-belongs-to-you", "title": "Find if external IP belongs to you", "text": "<p>You can list the network interfaces that match the IP you're searching for</p> <pre><code>aws ec2 describe-network-interfaces --filters Name=association.public-ip,Values=\"{{ your_ip_address}}\"\n</code></pre>"}, {"location": "aws_waf/", "title": "AWS WAF", "text": "<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting. You can also customize rules that filter out specific traffic patterns.</p>"}, {"location": "aws_waf/#extracting-information", "title": "Extracting information", "text": "<p>You can configure the WAF to write it's logs into S3, Kinesis or a Cloudwatch log group. S3 saves the data in small compressed files which are difficult to analyze, Kinesis makes sense if you post-process the data on a log system such as graylog, the last one allows you to use the WAF's builtin cloudwatch log insights which has the next interesting reports:             . * Top 100 Ip addresses * Top 100 countries * Top 100 hosts * Top 100 terminating rules             . Nevertheless, it still lacks some needed reports to analyze the traffic. But it's quite easy to build them yourself in Cloudwatch Log Insights. If you have time I'd always suggest to avoid using proprietary AWS tools, but sadly it's the quickest way to get results.</p>"}, {"location": "aws_waf/#creating-log-insights-queries", "title": "Creating Log Insights queries", "text": "<p>Inside the Cloudwatch site, on the left menu you'll see the <code>Logs</code> tab, and under it <code>Log Insights</code>. There you can write the query you want to run. Once it returns the expected result, you can save it. Saved queries can be seen on the right menu, under <code>Queries</code>.</p> <p>If you later change the query, you'll see a blue dot beside the query you last run. The query will remain changed until you click on <code>Actions</code> and then <code>Reset</code>.</p>"}, {"location": "aws_waf/#useful-queries", "title": "Useful Queries", "text": ""}, {"location": "aws_waf/#top-ips", "title": "Top IPs", "text": "<p>Is a directory to save the queries to analyze a count of requests aggregated by ips.</p>"}, {"location": "aws_waf/#top-ips-query", "title": "Top IPs query", "text": "<pre><code>fields httpRequest.clientIp\n| stats count(*) as requestCount by httpRequest.clientIp\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-ips-by-uri", "title": "Top IPs by uri", "text": "<pre><code>fields httpRequest.clientIp\n| filter httpRequest.uri like \"/\"\n| stats count(*) as requestCount by httpRequest.clientIp\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-uris", "title": "Top URIs", "text": "<p>Is a directory to save the queries to analyze a count of requests aggregated by uris.</p>"}, {"location": "aws_waf/#top-uris-query", "title": "Top URIs query", "text": "<p>This report shows all the uris that are allowed to pass the WAF.</p> <pre><code>fields httpRequest.uri\n| filter action like \"ALLOW\"\n| stats count(*) as requestCount by httpRequest.uri\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-uris-of-a-termination-rule", "title": "Top URIs of a termination rule", "text": "<pre><code>fields httpRequest.uri\n| filter terminatingRuleId like \"AWS-AWSManagedRulesUnixRuleSet\"\n| stats count(*) as requestCount by httpRequest.uri\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-uris-of-an-ip", "title": "Top URIs of an IP", "text": "<pre><code>fields httpRequest.uri\n| filter @message like \"6.132.241.132\"\n| stats count(*) as requestCount by httpRequest.uri\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-uris-of-a-cloudfront-id", "title": "Top URIs of a Cloudfront ID", "text": "<pre><code>fields httpRequest.uri\n| filter httpSourceId like \"CLOUDFRONT_ID\"\n| stats count(*) as requestCount by httpRequest.uri\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#waf-top-terminating-rules", "title": "WAF Top terminating rules", "text": "<p>Report that shows the top rules that are blocking the content.</p> <pre><code>fields terminatingRuleId\n| filter terminatingRuleId not like \"Default_Action\"\n| stats count(*) as requestCount by terminatingRuleId\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-blocks-by-cloudfront-id", "title": "Top blocks by Cloudfront ID", "text": "<pre><code>fields httpSourceId\n| filter terminatingRuleId not like \"Default_Action\"\n| stats count(*) as requestCount by httpSourceId\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-allows-by-cloudfront-id", "title": "Top allows by Cloudfront ID", "text": "<pre><code>fields httpSourceId\n| filter terminatingRuleId like \"Default_Action\"\n| stats count(*) as requestCount by httpSourceId\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#waf-top-countries", "title": "WAF Top countries", "text": "<pre><code>fields httpRequest.country\n| stats count(*) as requestCount by httpRequest.country\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#requests-by", "title": "Requests by", "text": "<p>Is a directory to save the queries to show the requests filtered by a criteria.</p>"}, {"location": "aws_waf/#requests-by-ip", "title": "Requests by IP", "text": "<pre><code>fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId\n| sort @timestamp desc\n| filter @message like \"6.132.241.132\"\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#requests-by-termination-rule", "title": "Requests by termination rule", "text": "<pre><code>fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId\n| sort @timestamp desc\n| filter terminatingRuleId like \"AWS-AWSManagedRulesUnixRuleSet\"\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#requests-by-uri", "title": "Requests by URI", "text": "<pre><code>fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId\n| sort @timestamp desc\n| filter httpRequest.uri like \"wp-json\"\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#waf-top-args-of-an-uri", "title": "WAF Top Args of an URI", "text": "<pre><code>fields httpRequest.args\n| filter httpRequest.uri like \"/\"\n| stats count(*) as requestCount by httpRequest.args\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#analysis-workflow", "title": "Analysis workflow", "text": "<p>To analyze the WAF insights you can:</p> <ul> <li>Analyze the traffic of the top IPs</li> <li>Analyze the top URIs</li> <li>Analyze the terminating rules</li> </ul>"}, {"location": "aws_waf/#analyze-the-traffic-of-the-top-ips", "title": "Analyze the traffic of the top IPS", "text": "<p>For IP in the WAF Top IPs report, do:</p> <ul> <li> <p>Analyze the top uris of that IP to see if they are     legit requests or if it contains malicious requests. If you want to get the     details of a particular request, you can use the requests by     uri report.</p> <p>For request in malicious requests:</p> <ul> <li>If it's not being filtered by the WAF update your WAF rules.</li> </ul> </li> <li> <p>If the IP is malicious mark it as problematic.</p> </li> </ul>"}, {"location": "aws_waf/#analyze-the-top-uris", "title": "Analyze the top uris", "text": "<p>For uri in the WAF Top URIs report, do:</p> <ul> <li> <p>For argument in the top arguments of that uri     report, see if they are legit requests or if it's malicious. If you want to     get the details of a particular request, you can use the requests by     uri report.</p> <p>For request in malicious requests:</p> <ul> <li>If it's not being filtered by the WAF update your WAF rules.</li> <li>For IP in top uris by IP report:<ul> <li>Mark IP as problematic.</li> </ul> </li> </ul> </li> </ul>"}, {"location": "aws_waf/#analyze-the-terminating-rules", "title": "Analyze the terminating rules", "text": "<p>For terminating rule in the WAF Top terminating rules report, do:</p> <ul> <li>For IP in the top ips by termination rule mark it as problematic.</li> </ul> <p>After some time you can see which rules are not being triggered and remove them. With the requests by termination rule you can see which requests are being blocked and try to block it in another rule set and merge both.</p>"}, {"location": "aws_waf/#mark-ip-as-problematic", "title": "Mark IP as problematic", "text": "<p>To process an problematic IP:</p> <ul> <li>Add it to the captcha list.</li> <li>If it is already in the captcha list and is still triggering problematic     requests, add it to the block list.</li> <li>Add a task to remove the IP from the block or captcha list X minutes or     hours in the future.</li> </ul>"}, {"location": "aws_waf/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "calendar_versioning/", "title": "Calendar Versioning", "text": "<p>Calendar Versioning is a versioning convention based on your project's release calendar, instead of arbitrary numbers.</p> <p>CalVer suggests version number to be in format of: <code>YEAR.MONTH.sequence</code>. For example, <code>20.1</code> indicates a release in 2020 January, while <code>20.5.2</code> indicates a release that occurred in 2020 May, while the <code>2</code> indicates this is the third release of the month.</p> <p>You can see it looks similar to semantic versioning and has the benefit that a later release qualifies as bigger than an earlier one within the semantic versioning world (which mandates that a version number must grow monotonically). This makes it easy to use in all places where semantic versioning can be used.</p> <p>The idea here is that if the only maintained version is the latest, then we might as well use the version number to indicate the release date to signify just how old of a version you\u2019re using. You also have the added benefit that you can make calendar-based promises. For example, Ubuntu offers five years of support, therefore given version <code>20.04</code> you can quickly determine that it will be supported up to April 2025.</p>"}, {"location": "calendar_versioning/#when-to-use-calver", "title": "When to use CalVer", "text": "<p>Check the Deciding what version system to use for your programs article section.</p>"}, {"location": "calendar_versioning/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "changelog/", "title": "Changelog", "text": "<p>A changelog is a file which contains a curated, chronologically ordered list of notable changes for each version of a project.</p> <p>It's purpose is to make it easier for users and contributors to see precisely what notable changes have been made between each release (or version) of the project.</p>"}, {"location": "changelog/#types-of-changes", "title": "Types of changes", "text": "<ul> <li>Added for new features.</li> <li>Changed for changes in existing functionality.</li> <li>Deprecated for soon-to-be removed features.</li> <li>Removed for now removed features.</li> <li>Fixed for any bug fixes.</li> <li>Security in case of vulnerabilities.</li> </ul>"}, {"location": "changelog/#changelog-guidelines", "title": "Changelog Guidelines", "text": "<p>Good changelogs follow the next principles:</p> <ul> <li>Changelogs are for humans, not machines.</li> <li>There should be an entry for every single version.</li> <li>The same types of changes should be grouped.</li> <li>Versions and sections should be linkable.</li> <li>The latest version comes first.</li> <li>The release date of each version is displayed.</li> <li>Mention your versioning strategy.</li> <li>Call it     <code>CHANGELOG.md</code>.</li> </ul> <p>Some examples of bad changelogs are:</p> <ul> <li> <p>Commit log diffs: The purpose of a changelog entry is to document the     noteworthy difference, often across multiple commits, to communicate them     clearly to end users. If someone wants to see the commit log diffs they can     access it through the <code>git</code> command.</p> </li> <li> <p>Ignoring Deprecations: When people upgrade from one version to another, it     should be painfully clear when something will break. It should be possible     to upgrade to a version that lists deprecations, remove what's deprecated,     then upgrade to the version where the deprecations become removals.</p> </li> <li> <p>Confusing Dates: Regional date formats vary throughout the world and it's     often difficult to find a human-friendly date format that feels intuitive to     everyone. The advantage of dates formatted like 2017-07-17 is that they     follow the order of largest to smallest units: year, month, and day. This     format also doesn't overlap in ambiguous ways with other date formats,     unlike some regional formats that switch the position of month and day     numbers. These reasons, and the fact this date format is an ISO standard,     are why it is the recommended date format for changelog entries.</p> </li> </ul>"}, {"location": "changelog/#how-to-reduce-the-effort-required-to-maintain-a-changelog", "title": "How to reduce the effort required to maintain a changelog", "text": "<p>There are two ways to ease the burden of maintaining a changelog:</p>"}, {"location": "changelog/#build-it-automatically", "title": "Build it automatically", "text": "<p>If you use Semantic Versioning you can use the commitizen tool to automatically generate the changelog each time you cut a new release by running <code>cz bump --changelog --no-verify</code>.</p> <p>The <code>--no-verify</code> part is required if you use pre-commit hooks.</p>"}, {"location": "changelog/#use-the-unreleased-section", "title": "Use the <code>Unreleased</code> section", "text": "<p>Keep an Unreleased section at the top to track upcoming changes.</p> <p>This serves two purposes:</p> <ul> <li>People can see what changes they might expect in upcoming releases.</li> <li>At release time, you can move the Unreleased section changes into a new     release version section.</li> </ul>"}, {"location": "changelog/#references", "title": "References", "text": "<ul> <li>Keep a Changelog</li> </ul>"}, {"location": "chezmoi/", "title": "Chezmoi", "text": "<p>Chezmoi stores the desired state of your dotfiles in the directory <code>~/.local/share/chezmoi</code>. When you run <code>chezmoi apply</code>, <code>chezmoi</code> calculates the desired contents for each of your dotfiles and then makes the minimum changes required to make your dotfiles match your desired state.</p> <p>What I like:</p> <ul> <li>Supports <code>pass</code> to retrieve credentials.</li> <li>Popular</li> <li>Can remove directories on <code>apply</code></li> <li>It has a <code>diff</code></li> <li>It can include dotfiles from an URL</li> <li>Encrypt files with gpg</li> <li>There's a vim plugin</li> <li>Actively maintained</li> <li>Good documentation</li> </ul> <p>What I don't like:</p> <ul> <li>Go templates, although   it supports autotemplating   and it's   well explained</li> <li>Written in Go</li> </ul>"}, {"location": "chezmoi/#installation", "title": "Installation", "text": "<p>I've added some useful aliases:</p> <pre><code>alias ce='chezmoi edit'\nalias ca='chezmoi add'\nalias cdiff='chezmoi diff'\nalias cdata='chezmoi edit-config'\nalias capply='chezmoi apply'\nalias cexternal='nvim ~/.local/share/chezmoi/.chezmoiexternal.yaml'\n</code></pre>"}, {"location": "chezmoi/#basic-usage", "title": "Basic Usage", "text": "<p>Assuming that you have already installed <code>chezmoi</code>, initialize <code>chezmoi</code> with:</p> <pre><code>$ chezmoi init\n</code></pre> <p>This will create a new git local repository in <code>~/.local/share/chezmoi</code> where <code>chezmoi</code> will store its source state. By default, <code>chezmoi</code> only modifies files in the working copy.</p> <p>Manage your first file with chezmoi:</p> <pre><code>$ chezmoi add ~/.bashrc\n</code></pre> <p>This will copy <code>~/.bashrc</code> to <code>~/.local/share/chezmoi/dot_bashrc</code>.</p> <p>Edit the source state:</p> <pre><code>$ chezmoi edit ~/.bashrc\n</code></pre> <p>This will open <code>~/.local/share/chezmoi/dot_bashrc</code> in your <code>$EDITOR</code>. Make some changes and save the file.</p> <p>See what changes chezmoi would make:</p> <pre><code>$ chezmoi diff\n</code></pre> <p>Apply the changes:</p> <pre><code>$ chezmoi -v apply\n</code></pre> <p>All <code>chezmoi</code> commands accept the <code>-v</code> (verbose) flag to print out exactly what changes they will make to the file system, and the <code>-n</code> (dry run) flag to not make any actual changes. The combination <code>-n -v</code> is very useful if you want to see exactly what changes would be made.</p> <p>Next, open a shell in the source directory, to commit your changes:</p> <pre><code>$ chezmoi cd\n$ git add .\n$ git commit -m \"Initial commit\"\n</code></pre> <p>Create a new repository on your desired git server called <code>dotfiles</code> and then push your repo:</p> <pre><code>$ git remote add origin https://your_git_server.com/$GIT_USERNAME/dotfiles.git\n$ git branch -M main\n$ git push -u origin main\n</code></pre> <p>Hint: <code>chezmoi</code> can be configured to automatically add, commit, and push changes to your repo.</p> <p>Finally, exit the shell in the source directory to return to where you were:</p> <pre><code>$ exit\n</code></pre>"}, {"location": "chezmoi/#install-a-binary-from-an-external-url", "title": "Install a binary from an external url", "text": "<p>Sometimes you may want to install some binaries from external urls, for example <code>velero</code> a backup tool for kubernetes. And you may want to be able to define what version you want to have and be able to update it at will. </p> <p>To do that we can define the version in the configuration with <code>chezmoi edit-config</code></p> <pre><code>data:\nvelero_version: 1.9.5\n</code></pre> <p>All the variables you define under the <code>data</code> field are globally available on all your templates.</p> <p>Then we can set the external configuration of chezmoi by editing the file <code>~/.config/chezmoi/.chezmoiexternal.yaml</code> and add the next snippet:</p> <pre><code>.local/bin/velero:\ntype: \"file\"\nurl: https://github.com/vmware-tanzu/velero/releases/download/v{{ .velero_version }}/velero-v{{ .velero_version }}-{{ .chezmoi.os }}-{{ .chezmoi.arch }}.tar.gz\nexecutable: true\nrefreshPeriod: 168h\nfilter:\ncommand: tar\nargs:\n- --extract\n- --file\n- /dev/stdin\n- --gzip\n- --to-stdout\n- velero-v{{ .velero_version }}-{{ .chezmoi.os }}-{{ .chezmoi.arch }}/velero\n</code></pre> <p>This will download the binary of version <code>1.9.5</code> from the source, unpack it and extract the <code>velero</code> binary and save it to <code>~/.local/bin/velero</code>.</p>"}, {"location": "chezmoi/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "contact/", "title": "Contact", "text": "<p>I'm available through my blog where I ocasionally write.</p>"}, {"location": "docker/", "title": "Docker", "text": "<p>Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. Because all of the containers share the services of a single operating system kernel, they use fewer resources than virtual machines.</p>"}, {"location": "docker/#how-to-keep-containers-updated", "title": "How to keep containers updated", "text": ""}, {"location": "docker/#with-renovate", "title": "With Renovate", "text": "<p>Renovate is a program that does automated dependency updates. Multi-platform and multi-language.</p>"}, {"location": "docker/#with-watchtower", "title": "With Watchtower", "text": "<p>With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially.</p> <p>Run the watchtower container with the next command:</p> <pre><code>docker run -d \\\n--name watchtower \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /etc/localtime:/etc/localtime:ro \\\n-e WATCHTOWER_NOTIFICATIONS=email \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_FROM={{ email.from }} \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_TO={{ email.to }} \\\\\n-e WATCHTOWER_NOTIFICATION_EMAIL_SERVER=mail.riseup.net \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PORT=587 \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_USER={{ email.user }} \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PASSWORD={{ email.password }} \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_DELAY=2 \\\ncontainrrr/watchtower:latest --no-restart --no-startup-message\n</code></pre> <p>Use the <code>--no-restart</code> flag if you use systemd to manage the dockers, and <code>--no-startup-message</code> if you don't want watchtower to send you an email each time it starts the update process.</p> <p>Keep in mind that if the containers don't have good migration scripts, upgrading may break the service. To enable this feature, make sure you have frequent backups and a tested rollback process. If you're not sure one of the containers is going to behave well, you can only monitor it or disable it by using docker labels.</p> <p>The first check will be done by default in the next 24 hours, to check that everything works use the <code>--run-once</code> flag.</p> <p>Another alternative is Diun, which is a CLI application written in Go and delivered as a single executable (and a Docker image) to receive notifications when a Docker image is updated on a Docker registry.</p> <p>They don't yet support Prometheus metrics but it surely looks promising.</p>"}, {"location": "docker/#logging-in-automatically", "title": "Logging in automatically", "text": "<p>To log in automatically without entering the password, you need to have the password stored in your personal password store (not in root's!), imagine it's in the <code>dockerhub</code> entry. Then you can use:</p> <pre><code>pass show dockerhub | docker login --username foo --password-stdin\n</code></pre>"}, {"location": "docker/#override-entrypoint", "title": "Override entrypoint", "text": "<pre><code>sudo docker run -it --entrypoint /bin/bash [docker_image]\n</code></pre>"}, {"location": "docker/#snippets", "title": "Snippets", "text": ""}, {"location": "docker/#attach-a-docker-to-many-networks", "title": "Attach a docker to many networks", "text": "<p>You can't do it through the <code>docker run</code> command, there you can only specify one network. However, you can attach a docker to a network with the command:</p> <pre><code>docker network attach network-name docker-name\n</code></pre>"}, {"location": "docker/#get-the-output-of-docker-ps-as-a-json", "title": "Get the output of <code>docker ps</code> as a json", "text": "<p>To get the complete json for reference.</p> <pre><code>docker ps -a --format \"{{json .}}\" | jq -s\n</code></pre> <p>To get only the required columns in the output with tab separated version</p> <pre><code>docker ps -a --format \"{{json .}}\" | jq -r -c '[.ID, .State, .Names, .Image]'\n</code></pre> <p>To get also the image's ID you can use:</p> <pre><code>docker inspect --format='{{json .}}' $(docker ps -aq) | jq -r -c '[.Id, .Name, .Config.Image, .Image]'\n</code></pre>"}, {"location": "docker/#connect-multiple-docker-compose-files", "title": "Connect multiple docker compose files", "text": "<p>You can connect services defined across multiple docker-compose.yml files.</p> <p>In order to do this you\u2019ll need to:</p> <ul> <li>Create an external network with <code>docker network create &lt;network name&gt;</code></li> <li>In each of your <code>docker-compose.yml</code> configure the default network to use your     externally created network with the networks top-level key.</li> <li>You can use either the service name or container name to connect between containers.</li> </ul> <p>Let's do it with an example:</p> <ul> <li> <p>Creating the network</p> <pre><code>$ docker network create external-example\n2af4d92c2054e9deb86edaea8bb55ecb74f84a62aec7614c9f09fee386f248a6\n</code></pre> </li> <li> <p>Create the first docker-compose file</p> <pre><code>version: '3'\nservices:\nservice1:\nimage: busybox\ncommand: sleep infinity\n\nnetworks:\ndefault:\nexternal:\nname: external-example\n</code></pre> </li> <li> <p>Bring the service up</p> <pre><code>$ docker-compose up -d\nCreating compose1_service1_1 ... done\n</code></pre> </li> <li> <p>Create the second docker-compose file with network configured</p> <pre><code>version: '3'\nservices:\nservice2:\nimage: busybox\ncommand: sleep infinity\n\nnetworks:\ndefault:\nexternal:\nname: external-example\n</code></pre> </li> <li> <p>Bring the service up</p> <pre><code>$ docker-compose up -d\nCreating compose2_service2_1 ... done\n</code></pre> </li> </ul> <p>After running <code>docker-compose up -d</code> on both docker-compose.yml files, we see that no new networks were created.</p> <pre><code>$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\n25e0c599d5e5        bridge              bridge              local\n2af4d92c2054        external-example    bridge              local\n7df4631e9cff        host                host                local\n194d4156d7ab        none                null                local\n</code></pre> <p>With the containers using the external-example network, they are able to ping one another.</p> <pre><code># By service name\n$ docker exec -it compose1_service1_1 ping service2\nPING service2 (172.24.0.3): 56 data bytes\n64 bytes from 172.24.0.3: seq=0 ttl=64 time=0.054 ms\n^C\n--- service2 ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.054/0.054/0.054 ms\n\n# By container name\n$ docker exec -it compose1_service1_1 ping compose2_service2_1\nPING compose2_service2_1 (172.24.0.2): 56 data bytes\n64 bytes from 172.24.0.2: seq=0 ttl=64 time=0.042 ms\n^C\n--- compose2_service2_1 ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.042/0.042/0.042 ms\n</code></pre> <p>The other way around works too.</p>"}, {"location": "docker/#troubleshooting", "title": "Troubleshooting", "text": "<p>If you are using a VPN and docker, you're going to have a hard time.</p> <p>The <code>docker</code> systemd service logs <code>systemctl status docker.service</code> usually doesn't give much information. Try to start the daemon directly with <code>sudo /usr/bin/dockerd</code>.</p>"}, {"location": "docker/#dont-store-credentials-in-plaintext", "title": "Don't store credentials in plaintext", "text": "<p>It doesn't work, don't go this painful road and assume that docker is broken.</p> <p>The official steps are horrible, and once you've spent two hours debugging it, you won't be able to push or pull images with your user.</p> <p>When you use <code>docker login</code> and introduce the user and password you get the next warning:</p> <pre><code>WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n</code></pre> <p>I got a nice surprise when I saw that <code>pass</code> was suggested in the link of the warning, to be used as a backend to store the password. But that feeling soon faded.</p> <p>To make docker understand that you want to use <code>pass</code> you need to use the <code>docker-credential-pass</code> script. A Go script \"maintained\" by docker, whose last commit was two years ago , has the CI broken and many old unanswered issues. Setting it up it's not easy either and it's ill documented.</p> <p>Furthermore, the script doesn't do what I expected, which is to store the password of your registry user in a pass entry. Instead, you need to create an empty pass entry in <code>docker-credential-helpers/docker-pass-initialized-check</code>, and when you use <code>docker login</code>, manually introducing your data, it creates another entry, as you can see in the next <code>pass</code> output:</p> <pre><code>Password Store\n\u2514\u2500\u2500 docker-credential-helpers\n    \u251c\u2500\u2500 aHR0cHM6Ly9pbmRleC5kb2NrZXIuaW8vdjEv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 lyz\n    \u2514\u2500\u2500 docker-pass-initialized-check\n</code></pre> <p>That entry is removed when you use <code>docker logout</code> so the next time you log in you need to introduce the user and password <code>(\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b</code>.</p>"}, {"location": "docker/#installing-docker-credential-pass", "title": "Installing docker-credential-pass", "text": "<p>You first need to install the script:</p> <pre><code># Check for later releases at https://github.com/docker/docker-credential-helpers/releases\nversion=\"v0.6.3\"\narchive=\"docker-credential-pass-$version-amd64.tar.gz\"\nurl=\"https://github.com/docker/docker-credential-helpers/releases/download/$version/$archive\"\n\n# Download cred helper, unpack, make executable, and move it where Docker will find it.\nwget $url \\\n&amp;&amp; tar -xf $archive \\\n&amp;&amp; chmod +x docker-credential-pass \\\n&amp;&amp; mv -f docker-credential-pass /usr/local/bin/\n</code></pre> <p>Another tricky issue is that even if you use a non-root user who's part of the <code>docker</code> group, the script is not aware of that, so it will look in the password store of root instead of the user's. This means that additionally to your own, you need to create a new password store for root. Follow the next steps with the root user:</p> <ul> <li>Create the password with <code>gpg --full-gen</code>, and copy the key id. Use a non     empty password, otherwise you are getting the same security as with the     password in cleartext.</li> <li>Initialize the password store <code>pass init gpg_id</code>, changing <code>gpg_id</code> for the     one of the last step.</li> <li> <p>Create the empty <code>docker-credential-helpers/docker-pass-initialized-check</code>     entry:</p> <pre><code>pass insert docker-credential-helpers/docker-pass-initialized-check\n</code></pre> <p>And press enter twice.</p> </li> </ul> <p>Finally we need to specify in the root's docker configuration that we want to use the <code>pass</code> credential storage.</p> <p>File: /root/.docker/config.json</p> <pre><code>{\n\"credsStore\": \"pass\"\n}\n</code></pre>"}, {"location": "docker/#testing-it-works", "title": "Testing it works", "text": "<p>To test that docker is able to use pass as backend to store the credentials, run <code>docker login</code> and introduce the user and password. You should see the <code>Login Succeeded</code> message without any warning.</p> <pre><code>Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: lyz\nPassword:\nLogin Succeeded\n</code></pre> <p>Awful experience, wasn't it? Don't worry it gets worse.</p> <p>Now that you're logged in, whenever you try to push an image you're probably going to get an <code>denied: requested access to the resource is denied</code> error. That's because docker is not able to use the password it has stored in the root's password store. If you're using <code>root</code> to push the image (bad idea anyway), you will need to <code>export GPG_TTY=$(tty)</code> so that docker can ask you for your password to unlock root's <code>pass</code> entry. If you're like me that uses a non-root user belonging to the <code>docker</code> group, not even that works, so you've spent all this time reading and trying to fix everything for nothing... Thank you Docker <code>-.-</code>.</p>"}, {"location": "docker/#start-request-repeated-too-quickly", "title": "Start request repeated too quickly", "text": "<p>Shutdown the VPN and it will work. If it doesn't inspect the output of <code>journalctl -eu docker</code>.</p>"}, {"location": "dotdrop/", "title": "Dotdrop", "text": "<p>The main idea of Dotdropis to have the ability to store each dotfile only once and deploy them with a different content on different hosts/setups. To achieve this, it uses a templating engine that allows to specify, during the dotfile installation with dotdrop, based on a selected profile, how (with what content) each dotfile will be installed.</p> <p>What I like:</p> <ul> <li>Popular</li> <li>Actively maintained</li> <li>Written in Python</li> <li>Uses jinja2</li> <li>Has a nice to read config file</li> </ul> <p>What I don't like:</p> <ul> <li>Updating dotfiles doesn't look as smooth as with chezmoi</li> <li>Uses <code>{{@@ @@}}</code> instead of <code>{{ }}</code> :S</li> <li>Doesn't support <code>pass</code>.</li> <li>Not easy way to edit the files.</li> </ul>"}, {"location": "dotdrop/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Docs</li> <li>Blog post</li> </ul>"}, {"location": "dotfiles/", "title": "Dotfiles", "text": "<p>User-specific application configuration is traditionally stored in so called dotfiles (files whose filename starts with a dot). It is common practice to track dotfiles with a version control system such as Git to keep track of changes and synchronize dotfiles across various hosts. There are various approaches to managing your dotfiles (e.g. directly tracking dotfiles in the home directory v.s. storing them in a subdirectory and symlinking/copying/generating files with a shell script or a dedicated tool).</p> <p>Note: this is not meant to configure files that are outside your home directory, use Ansible for that use case.</p>"}, {"location": "dotfiles/#tracking-dotfiles-directly-with-git", "title": "Tracking dotfiles directly with Git", "text": "<p>The benefit of tracking dotfiles directly with Git is that it only requires Git and does not involve symlinks. The disadvantage is that host-specific configuration generally requires merging changes into multiple branches.</p> <pre><code>$ git init --bare ~/.dotfiles\n$ alias config='/usr/bin/git --git-dir=$HOME/.dotfiles/ --work-tree=$HOME'\n$ config config status.showUntrackedFiles no\n</code></pre>"}, {"location": "dotfiles/#host-specific-configuration", "title": "Host-specific configuration", "text": "<p>A common problem with synchronizing dotfiles across various machines is host-specific configuration.</p> <p>With Git this can be solved by maintaining a main branch for all shared configuration, while each individual machine has a machine-specific branch checked out. Host-specific configuration can be committed to the machine-specific branch; when shared configuration is modified in the master branch, the per-machine branches need to be rebased on top of the updated master.</p> <p>If you find rebasing Git branches too cumbersome, you may want to use a tool that supports file grouping, or if even greater flexibility is desired, a tool that does processing.</p>"}, {"location": "dotfiles/#using-ansible-to-manage-the-dotfiles", "title": "Using Ansible to manage the dotfiles", "text": "<p>Ansible gives you a lot of freedom on how to configure complex devices, and I've use it for a while, creating my own roles for each application with tests, it was beautiful to see.</p> <p>It wasn't so pleasant to use or maintain because:</p> <ul> <li> <p>Every time you update something you need to:</p> </li> <li> <p>Change the files manually until you get the new state of the files</p> </li> <li>Copy the files to the ansible-playbook repo</li> <li>Apply the changes</li> </ul> <p>Alternatively you can do the changes directly on the playbook repo, but then   you'd need to run the <code>apply</code> many more times, and it's slow, so in the end   you don't do it.</p> <ul> <li> <p>If you want to try a new tool but you're not sure you want it either you add   it to the playbook and then remove it (waste of time), or play with the tool   and then once your finished add it to the playbook. This last approach didn't   work for me. It's like writing the docs after you've finished coding, you just   don't do it, you don't have energy left and go to the next thing.</p> </li> <li> <p>Most of the applications that use dotfiles are similarly configured, so   ansible is an overkill for them. dotfiles tools are much better because you'd   spend less time configuring it and the result is the same.</p> </li> </ul>"}, {"location": "dotfiles/#tools", "title": "Tools", "text": "Name Written in File grouping Processing Stars chezmoi Go directory-based Go templates 8.2k dot-templater Rust directory-based custom syntax dotdrop Python configuration file Jinja2 1.5k dotfiles Python No No 555 Dots Python directory-based custom append points 264 Mackup Python automatic  per application No 12.8k dotter Rust configuration file Handlebars dt-cli Rust configuration file Handlebars mir.qualia Python No custom blocks <p>Where:</p> <ul> <li>File grouping: How configuration files can be grouped to configuration groups   (also called profiles or packages).</li> <li>Processing: Some tools process configuration files to allow them to be   customized depending on the host.</li> </ul> <p>A quick look up shows that:</p> <ul> <li>chezmoi looks like the best option.</li> <li>dotdrop looks like the second best option.</li> <li>dotfiles is unmaintained.</li> <li>dots: is maintained but migrated to go</li> <li>mackup: Looks like it's built for the cloud and it needs to support your   application?</li> </ul> <p>I think I'll give <code>chezmoi</code> a try.</p>"}, {"location": "drone/", "title": "Drone", "text": "<p>Drone is a modern Continuous Integration platform that empowers busy teams to automate their build, test and release workflows using a powerful, cloud native pipeline engine.</p>"}, {"location": "drone/#installation", "title": "Installation", "text": "<p>This section explains how to install the Drone server for Gitea.</p> <p>Note</p> <p>They explicitly recommend not to use Gitea and Drone in the same instance, and even less using <code>docker-compose</code> due to network complications. But if you have only a small instance as I do, you'll have to try :P.</p> <ul> <li>Log in with an admin Gitea user. </li> <li>Go to the administration configuration, then Applications.</li> <li> <p>Create a Gitea OAuth application. The Consumer Key and Consumer Secret are     used to authorize access to Gitea resources. Use the next data:</p> <ul> <li>Name: Drone</li> <li>Redirect URI: https://drone.your-domain.com/login</li> </ul> <p>Even though it looks like you could use terraform to create the resource, you can't as it creates an application but at user level, not at gitea level. So the next snippet won't work</p> <pre><code>resource \"gitea_oauth2_app\" \"drone\" {\nname = \"drone\"\nredirect_uris = [var.drone_redirect_uri]\n}\n\noutput \"drone_oauth2_id\" {\nvalue = gitea_oauth2_app.drone.client_id\n}\n\noutput \"drone_oauth2_secret\" {\nvalue = gitea_oauth2_app.drone.client_secret\n}\n</code></pre> </li> <li> <p>Create a shared secret to authenticate communication between runners and your     central Drone server.</p> <pre><code>openssl rand -hex 16\n</code></pre> </li> <li> <p>Create the required docker networks:     <pre><code>docker network create continuous-delivery\ndocker network create drone\ndocker network create swag\n</code></pre></p> </li> <li> <p>Create the docker-compose file for the server</p> <pre><code>---\nversion: '3'\n\nservices:\nserver:\nimage: drone/drone:2\nenvironment:\n- DRONE_GITEA_SERVER=https://try.gitea.io\n- DRONE_GITEA_CLIENT_ID=05136e57d80189bef462\n- DRONE_GITEA_CLIENT_SECRET=7c229228a77d2cbddaa61ddc78d45e\n- DRONE_RPC_SECRET=super-duper-secret\n- DRONE_SERVER_HOST=drone.company.com\n- DRONE_SERVER_PROTO=https\ncontainer_name: drone\nrestart: always\nnetworks:\n- swag\n- drone\n- continuous-delivery\nvolumes:\n- drone-data:/data\n\nnetworks:\ncontinuous-delivery:\nexternal:\nname: continuous-delivery\ndrone:\nexternal:\nname: drone\nswag:\nexternal:\nname: swag\n\nvolumes:\ndrone-data:\ndriver: local\ndriver_opts:\ntype: none\no: bind\ndevice: /data/drone\n</code></pre> <p>Where we specify where we want the data to be stored at, and the networks to use. We're assuming that you're going to use the linuxserver swag proxy to end the ssl connection (which is accessible through the <code>swag</code> network), and that <code>gitea</code> is in the <code>continuous-delivery</code> network.</p> </li> <li> <p>Add the runners you want to install.</p> </li> <li>Configure your proxy to forward the requests to the correct dockers.</li> <li>Run <code>docker-compose up</code> from the file where your <code>docker-compose.yaml</code> file is     to test everything works. If it does, run <code>docker-compose down</code>.</li> <li>Create a systemd service to start and stop the whole service. For example     create the <code>/etc/systemd/system/drone.service</code> file with the content:     <pre><code>Description=drone\n[Unit]\nDescription=drone\nRequires=gitea.service\nAfter=gitea.service\n\n[Service]\nRestart=always\nUser=root\nGroup=docker\nWorkingDirectory=/data/config/continuous-delivery/drone\n# Shutdown container (if running) when unit is started\nTimeoutStartSec=100\nRestartSec=2s\n# Start container when unit is started\nExecStart=/usr/bin/docker-compose -f docker-compose.yml up\n# Stop container when unit is stopped\nExecStop=/usr/bin/docker-compose -f docker-compose.yml down\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></li> </ul>"}, {"location": "drone/#drone-runners", "title": "Drone Runners", "text": ""}, {"location": "drone/#docker-runner", "title": "Docker Runner", "text": "<p>Merge the next docker-compose with the one of the server above:</p> <pre><code>---\nversion: '3'\n\nservices:\ndocker_runner:\nimage: drone/drone-runner-docker:1\nenvironment:\n- DRONE_RPC_PROTO=https\n- DRONE_RPC_HOST=drone.company.com\n- DRONE_RPC_SECRET=super-duper-secret\n- DRONE_RUNNER_CAPACITY=2\n- DRONE_RUNNER_NAME=docker-runner\ncontainer_name: drone-docker-runner\nrestart: always\nnetworks:\n- drone\nvolumes:\n- /var/run/docker.sock:/var/run/docker.sock\nexpose:\n- \"3000\"\n\nnetworks:\ndrone:\nexternal:\nname: drone\n</code></pre> <p>Use the <code>docker logs</code> command to view the logs and verify the runner successfully established a connection with the Drone server.</p> <pre><code>$ docker logs runner\n\nINFO[0000] starting the server\nINFO[0000] successfully pinged the remote server\n</code></pre>"}, {"location": "drone/#ssh-runner", "title": "SSH Runner", "text": "<p>Merge the next docker-compose with the one of the server above:</p> <pre><code>---\nversion: '3'\n\nservices:\nssh_runner:\nimage: drone/drone-runner-ssh:latest\nenvironment:\n- DRONE_RPC_PROTO=https\n- DRONE_RPC_HOST=drone.company.com\n- DRONE_RPC_SECRET=super-duper-secret\ncontainer_name: drone-ssh-runner\nrestart: always\nnetworks:\n- drone\nexpose:\n- \"3000\"\n\nnetworks:\ndrone:\nexternal:\nname: drone\n</code></pre> <p>Use the <code>docker logs</code> command to view the logs and verify the runner successfully established a connection with the Drone server.</p> <pre><code>$ docker logs runner\n\nINFO[0000] starting the server\nINFO[0000] successfully pinged the remote server\n</code></pre>"}, {"location": "drone/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Home</li> </ul>"}, {"location": "elasticsearch_exporter/", "title": "Blackbox Exporter", "text": "<p>The elasticsearch exporter allows monitoring Elasticsearch clusters with Prometheus.</p>"}, {"location": "elasticsearch_exporter/#installation", "title": "Installation", "text": "<p>To install the exporter we'll use helmfile to install the prometheus-elasticsearch-exporter chart.</p> <p>Add the following lines to your <code>helmfile.yaml</code>.</p> <pre><code>- name: prometheus-elasticsearch-exporter\nnamespace: monitoring\nchart: prometheus-community/prometheus-elasticsearch-exporter\nvalues:\n- prometheus-elasticsearch-exporter/values.yaml\n</code></pre> <p>Edit the chart values. <pre><code>mkdir prometheus-elasticsearch-exporter\nhelm inspect values prometheus-community/prometheus-elasticsearch-exporter &gt; prometheus-elasticsearch-exporter/values.yaml\nvi prometheus-elasticsearch-exporter/values.yaml\n</code></pre></p> <p>Comment out all the values you don't edit, so that the chart doesn't break when you upgrade it.</p> <p>Make sure that the <code>serviceMonitor</code> labels match your Prometheus <code>serviceMonitorSelector</code> otherwise they won't be added to the configuration.</p> <pre><code>es:\n## Address (host and port) of the Elasticsearch node we should connect to.\n## This could be a local node (localhost:9200, for instance), or the address\n## of a remote Elasticsearch server. When basic auth is needed,\n## specify as: &lt;proto&gt;://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;. e.g., http://admin:pass@localhost:9200.\n##\nuri: http://localhost:9200\n\nserviceMonitor:\n## If true, a ServiceMonitor CRD is created for a prometheus operator\n## https://github.com/coreos/prometheus-operator\n##\nenabled: true\n#  namespace: monitoring\nlabels:\nrelease: prometheus-operator\ninterval: 30s\n# scrapeTimeout: 10s\n# scheme: http\n# relabelings: []\n# targetLabels: []\nmetricRelabelings:\n- sourceLabels: [cluster]\ntargetLabel: cluster_name\nregex: '.*:(.*)'\n# sampleLimit: 0\n</code></pre> <p>You can build the <code>cluster</code> label following this instructions, I didn't find the required meta tags, so I've built the <code>cluster_name</code> label for alerting purposes.</p> <p>The grafana dashboard I chose is <code>2322</code>. Taking as reference the grafana helm chart values, add the next yaml under the <code>grafana</code> key in the <code>prometheus-operator</code> <code>values.yaml</code>.</p> <pre><code>grafana:\nenabled: true\ndefaultDashboardsEnabled: true\ndashboardProviders:\ndashboardproviders.yaml:\napiVersion: 1\nproviders:\n- name: 'default'\norgId: 1\nfolder: ''\ntype: file\ndisableDeletion: false\neditable: true\noptions:\npath: /var/lib/grafana/dashboards/default\ndashboards:\ndefault:\nelasticsearch:\n# Ref: https://grafana.com/dashboards/2322\ngnetId: 2322\nrevision: 4\ndatasource: Prometheus\n</code></pre> <p>And install.</p> <pre><code>helmfile diff\nhelmfile apply\n</code></pre>"}, {"location": "elasticsearch_exporter/#elasticsearch-exporter-alerts", "title": "Elasticsearch exporter alerts", "text": "<p>Now that we've got the metrics, we can define the alert rules. Most have been tweaked from the Awesome prometheus alert rules collection.</p>"}, {"location": "elasticsearch_exporter/#availability-alerts", "title": "Availability alerts", "text": "<p>The most basic probes, test if the service is healthy</p> <pre><code>- alert: ElasticsearchClusterRed\nexpr: elasticsearch_cluster_health_status{color=\"red\"} == 1\nfor: 0m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch Cluster Red\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElastic Cluster Red status\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchClusterYellow\nexpr: elasticsearch_cluster_health_status{color=\"yellow\"} == 1\nfor: 0m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch Cluster Yellow\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElastic Cluster Yellow status\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchHealthyNodes\nexpr: elasticsearch_cluster_health_number_of_nodes &lt; 3\nfor: 0m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch Healthy Nodes\n(cluster {{ $labels.cluster_name }})\ndescription: |\nMissing node in Elasticsearch cluster\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchHealthyMasterNodes\nexpr: &gt;\nelasticsearch_cluster_health_number_of_nodes\n- elasticsearch_cluster_health_number_of_data_nodes &gt; 0 &lt; 3\nfor: 0m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch Healthy Master Nodes &lt; 3\n(cluster {{ $labels.cluster_name }})\ndescription: |\nMissing master node in Elasticsearch cluster\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchHealthyDataNodes\nexpr: elasticsearch_cluster_health_number_of_data_nodes &lt; 3\nfor: 0m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch Healthy Data Nodes\n(cluster {{ $labels.cluster_name }})\ndescription: |\nMissing data node in Elasticsearch cluster\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n</code></pre>"}, {"location": "elasticsearch_exporter/#performance-alerts", "title": "Performance alerts", "text": "<pre><code>- alert: ElasticsearchCPUUsageTooHigh\nexpr: elasticsearch_os_cpu_percent &gt; 90\nfor: 2m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch Node CPU Usage Too High\n(cluster {{ $labels.cluster_name }} node {{ $labels.name }})\ndescription: |\nThe CPU usage of node {{ $labels.name }} is over 90%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchCPUUsageWarning\nexpr: elasticsearch_os_cpu_percent &gt; 80\nfor: 2m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch Node CPU Usage Too High\n(cluster {{ $labels.cluster_name }} node {{ $labels.name }})\ndescription: |\nThe CPU usage of node {{ $labels.name }} is over 90%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchHeapUsageTooHigh\nexpr: &gt;\n(\nelasticsearch_jvm_memory_used_bytes{area=\"heap\"}\n/ elasticsearch_jvm_memory_max_bytes{area=\"heap\"}\n) * 100 &gt; 90\nfor: 2m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch Node Heap Usage Critical\n(cluster {{ $labels.cluster_name }} node {{ $labels.name }})\ndescription: |\nThe heap usage of node {{ $labels.name }} is over 90%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchHeapUsageWarning\nexpr: &gt;\n(\nelasticsearch_jvm_memory_used_bytes{area=\"heap\"}\n/ elasticsearch_jvm_memory_max_bytes{area=\"heap\"}\n) * 100 &gt; 80\nfor: 2m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch Node Heap Usage Warning\n(cluster {{ $labels.cluster_name }} node {{ $labels.name }})\ndescription: |\nThe heap usage of node {{ $labels.name }} is over 80%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchDiskOutOfSpace\nexpr: &gt;\nelasticsearch_filesystem_data_available_bytes\n/ elasticsearch_filesystem_data_size_bytes * 100 &lt; 10\nfor: 0m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch disk out of space\n(cluster {{ $labels.cluster_name }})\ndescription: |\nThe disk usage is over 90%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchDiskSpaceLow\nexpr: &gt;\nelasticsearch_filesystem_data_available_bytes\n/ elasticsearch_filesystem_data_size_bytes * 100 &lt; 20\nfor: 2m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch disk space low\n(cluster {{ $labels.cluster_name }})\ndescription: |\nThe disk usage is over 80%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchRelocatingShardsTooLong\nexpr: elasticsearch_cluster_health_relocating_shards &gt; 0\nfor: 15m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch relocating shards too long\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElasticsearch has been relocating shards for 15min\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchInitializingShardsTooLong\nexpr: elasticsearch_cluster_health_initializing_shards &gt; 0\nfor: 15m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch initializing shards too long\n(cluster_name {{ $labels.cluster }})\ndescription: |\nElasticsearch has been initializing shards for 15 min\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchUnassignedShards\nexpr: elasticsearch_cluster_health_unassigned_shards &gt; 0\nfor: 0m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch unassigned shards\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElasticsearch has unassigned shards\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchPendingTasks\nexpr: elasticsearch_cluster_health_number_of_pending_tasks &gt; 0\nfor: 15m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch pending tasks\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElasticsearch has pending tasks. Cluster works slowly.\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n\n- alert: ElasticsearchCountOfJVMGarbageCollectorRuns\nexpr: rate(elasticsearch_jvm_gc_collection_seconds_count{}[5m]) &gt; 5\nfor: 1m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch JVM Garbage Collector runs &gt; 5\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElastic Cluster JVM Garbage Collector runs &gt; 5\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n\n- alert: ElasticsearchCountOfJVMGarbageCollectorTime\nexpr: rate(elasticsearch_jvm_gc_collection_seconds_sum[5m]) &gt; 0.3\nfor: 1m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch JVM Garbage Collector time &gt; 0.3\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElastic Cluster JVM Garbage Collector runs &gt; 0.3\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchJSONParseErrors\nexpr: elasticsearch_cluster_health_json_parse_failures &gt; 0\nfor: 1m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch json parse error\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElasticsearch json parse error\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchCircuitBreakerTripped\nexpr: rate(elasticsearch_breakers_tripped{}[5m])&gt;0\nfor: 1m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch breaker {{ $labels.breaker }} tripped\n(cluster {{ $labels.cluster_name }}, node {{ $labels.name }})\ndescription: |\nElasticsearch breaker {{ $labels.breaker }} tripped\n(cluster {{ $labels.cluster_name }}, node {{ $labels.name }})\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n</code></pre>"}, {"location": "elasticsearch_exporter/#snapshot-alerts", "title": "Snapshot alerts", "text": "<pre><code>- alert: ElasticsearchMonthlySnapshot\nexpr: &gt;\ntime() -\nelasticsearch_snapshot_stats_snapshot_end_time_timestamp{state=\"SUCCESS\"}\n&gt; (3600 * 24 * 32)\nfor: 15m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch monthly snapshot failed\n(cluster {{ $labels.cluster_name }},\nsnapshot {{ $labels.repository }})\ndescription: |\nLast successful elasticsearch snapshot\nof repository {{ $labels.repository}} is older than 32 days.\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n\n- record: elasticsearch_indices_search_latency:rate1m\nexpr: |\nincrease(elasticsearch_indices_search_query_time_seconds[1m])/\nincrease(elasticsearch_indices_search_query_total[1m])\n- record: elasticsearch_indices_search_rate:rate1m\nexpr: increase(elasticsearch_indices_search_query_total[1m])/60\n- alert: ElasticsearchSlowSearchLatency\nexpr: elasticsearch_indices_search_latency:rate1m &gt; 1\nfor: 2m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch search latency is greater than 1 s\n(cluster {{ $labels.cluster_name }}, node {{ $labels.name }})\ndescription: |\nElasticsearch search latency is greater than 1 s\n(cluster {{ $labels.cluster_name }}, node {{ $labels.name }})\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n</code></pre>"}, {"location": "elasticsearch_exporter/#links", "title": "Links", "text": "<ul> <li>Git</li> </ul>"}, {"location": "flakeheaven/", "title": "Flakeheaven", "text": "<p>Flakeheaven is a Flake8 wrapper to make it cool.</p> <p>Some of it's features are:</p> <ul> <li>Lint md, rst, ipynb, and     more.</li> <li>Shareable and remote     configs.</li> <li>Legacy-friendly:     ability to get report only about new errors.</li> <li>Caching for much better performance.</li> <li>Use only specified     plugins, not     everything installed.</li> <li>Make output beautiful.</li> <li>pyproject.toml support.</li> <li>Check that all required plugins are     installed.</li> <li>Syntax highlighting in messages and code     snippets.</li> <li>PyLint integration.</li> <li>Remove unused noqa.</li> <li>Powerful GitLab support.</li> <li>Codes management:<ul> <li>Manage codes per plugin.</li> <li>Enable and disable plugins and codes by wildcard.</li> <li>Show codes for installed plugins.</li> <li>Show all messages and codes for a plugin.</li> <li>Allow codes intersection for different plugins.</li> </ul> </li> </ul> <p>You can use this cookiecutter template to create a python project with <code>flakeheaven</code> already configured.</p>"}, {"location": "flakeheaven/#installation", "title": "Installation", "text": "<pre><code>pip install flakeheaven\n</code></pre>"}, {"location": "flakeheaven/#configuration", "title": "Configuration", "text": "<p>Flakeheaven can be configured in pyproject.toml. You can specify any Flake8 options and Flakeheaven-specific parameters.</p>"}, {"location": "flakeheaven/#plugins", "title": "Plugins", "text": "<p>In <code>pyproject.toml</code> you can specify <code>[tool.flakeheaven.plugins]</code> table. It's a list of flake8 plugins and associated to them rules.</p> <p>Key can be exact plugin name or wildcard template. For example <code>\"flake8-commas\"</code> or <code>\"flake8-*\"</code>. Flakeheaven will choose the longest match for every plugin if possible. In the previous example, <code>flake8-commas</code> will match to the first pattern, <code>flake8-bandit</code> and <code>flake8-bugbear</code> to the second, and <code>pycodestyle</code> will not match to any pattern.</p> <p>Value is a list of templates for error codes for this plugin. First symbol in every template must be <code>+</code> (include) or <code>-</code> (exclude). The latest matched pattern wins. For example, <code>[\"+*\", \"-F*\", \"-E30?\", \"-E401\"]</code> means \"Include everything except all checks that starts with <code>F</code>, check from <code>E301</code> to <code>E310</code>, and <code>E401</code>\".</p> <p>Example: pyproject.toml</p> <pre><code>[tool.flakeheaven]\n# specify any flake8 options. For example, exclude \"example.py\":\nexclude = [\"example.py\"]\n# make output nice\nformat = \"grouped\"\n# don't limit yourself\nmax_line_length = 120\n# show line of source code in output\nshow_source = true\n\n# list of plugins and rules for them\n[tool.flakeheaven.plugins]\n# include everything in pyflakes except F401\npyflakes = [\"+*\", \"-F401\"]\n# enable only codes from S100 to S199\nflake8-bandit = [\"-*\", \"+S1??\"]\n# enable everything that starts from `flake8-`\n\"flake8-*\" = [\"+*\"]\n# explicitly disable plugin\nflake8-docstrings = [\"-*\"]\n\n# disable some checks for tests\n[tool.flakeheaven.exceptions.\"tests/\"]\npycodestyle = [\"-F401\"]     # disable a check\npyflakes = [\"-*\"]           # disable a plugin\n\n# do not disable `pyflakes` for one file in tests\n[tool.flakeheaven.exceptions.\"tests/test_example.py\"]\npyflakes = [\"+*\"]           # enable a plugin\n</code></pre> <p>Check a complete list of flake8 extensions.</p> <ul> <li>flake8-bugbear: Finding likely bugs     and design problems in your program. Contains warnings that don't belong in     pyflakes and pycodestyle.</li> <li>flake8-fixme: Check for FIXME,     TODO and other temporary developer notes.</li> <li>flake8-debugger: Check for     <code>pdb</code> or <code>idbp</code> imports and set traces.</li> <li>flake8-mutable: Checks for     mutable default     arguments anti-pattern.</li> <li>flake8-pytest: Check for uses of     Django-style assert-statements in tests. So no more <code>self.assertEqual(a, b)</code>,     but instead <code>assert a == b</code>.</li> <li>flake8-pytest-style: Checks     common style issues or inconsistencies with pytest-based tests.</li> <li>flake8-simplify: Helps you     to simplify code.</li> <li>flake8-variables-names:     Helps to make more readable variables names.</li> <li>pep8-naming: Check your code against     PEP 8 naming conventions.</li> <li>flake8-expression-complexity:     Check expression complexity.</li> <li>flake8-use-fstring:     Checks you're using f-strings.</li> <li>flake8-docstrings: adds an     extension for the fantastic     pydocstyle tool to Flake8.</li> <li>flake8-markdown: lints     GitHub-style Python code blocks in Markdown files using flake8.</li> <li>pylint is a Python static code analysis     tool which looks for programming errors, helps enforcing a coding standard,     sniffs for code smells and offers simple refactoring suggestions.</li> <li>dlint: Encourage best coding practices     and helping ensure Python code is secure.</li> <li>flake8-aaa: Checks Python tests     follow the Arrange-Act-Assert     pattern.</li> <li>flake8-annotations-complexity:     Report on too complex type annotations.</li> <li>flake8-annotations: Detects the     absence of PEP 3107-style function annotations and PEP 484-style type     comments.</li> <li>flake8-typing-imports:     Checks that typing imports are properly guarded.</li> <li>flake8-comprehensions:     Help you write better list/set/dict comprehensions.</li> <li>flake8-eradicate:  find     commented out (or so called \"dead\") code.</li> </ul>"}, {"location": "flakeheaven/#usage", "title": "Usage", "text": "<p>When using Flakeheaven, I frequently use the following commands:</p> <code>flakeheaven lint</code> Runs the linter, similar to the flake8 command. <code>flakeheaven plugins</code> Lists all the plugins used, and their configuration status. <code>flakeheaven missed</code> Shows any plugins that are in the configuration but not installed properly. <code>flakeheaven code S322</code> (or any other code) Shows the explanation for that specific warning code. <code>flakeheaven yesqa</code> Removes unused codes from <code># noqa</code> and removes bare noqa that says \u201cignore everything on this line\u201d as is a bad practice."}, {"location": "flakeheaven/#integrations", "title": "Integrations", "text": "<p>Flakeheaven checks can be run in:</p> <ul> <li> <p>In Vim though the ALE plugin.</p> </li> <li> <p>Through a pre-commit:</p> <pre><code>  - repo: https://github.com/flakeheaven/flakeheaven\nrev: master\nhooks:\n- name: Run flakeheaven static analysis tool\nid: flakeheaven\n</code></pre> </li> <li> <p>In the CI:     <pre><code>  - name: Test linters\nrun: make lint\n</code></pre></p> <p>Assuming you're using a Makefile like the one in my cookiecutter-python-project.</p> </li> </ul>"}, {"location": "flakeheaven/#issues", "title": "Issues", "text": "<ul> <li>ImportError: cannot import name 'MergedConfigParser' from     'flake8.options.config':     remove the dependency pin in cookiecutter template and propagate to all     projects.</li> <li> <p>'Namespace' object has no attribute 'extended_default_ignore'     error:     Until it's fixed either use a version below or equal to 3.9.0, or add to     your <code>pyproject.toml</code>:</p> <pre><code>[tool.flakeheaven]\nextended_default_ignore=[]  # add this\n</code></pre> <p>Once it's fixed, remove the patch from the maintained projects.</p> </li> </ul>"}, {"location": "flakeheaven/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "flakeheaven/#namespace-object-has-no-attribute", "title": "['Namespace' object has no attribute", "text": "<p>'extended_default_ignore'](https://githubmemory.com/repo/flakeheaven/flakeheaven/issues/10)</p> <p>Add to your <code>pyproject.toml</code>:</p> <pre><code>[tool.flakeheaven]\nextended_default_ignore=[]\n</code></pre>"}, {"location": "flakeheaven/#references", "title": "References", "text": "<ul> <li>Git</li> <li> <p>Docs</p> </li> <li> <p>Using Flake8 and pyproject.toml with Flakeheaven article by Jonathan     Bowman</p> </li> </ul>"}, {"location": "git/", "title": "Git", "text": "<p>Git is a software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development. Its goals include speed, data integrity, and support for distributed, non-linear workflows (thousands of parallel branches running on different systems).</p>"}, {"location": "git/#learning-git", "title": "Learning git", "text": "<p>Git is a tough nut to crack, no matter how experience you are you'll frequently get surprised. Sadly it's one of the main tools to develop your code, so you must master it as soon as possible.</p> <p>Depending on how you like to learn I've found these options:</p> <ul> <li>Written courses: W3 git course</li> <li>Interactive tutorials:   Learngitbranching interactive tutorial</li> <li>Written article:   Freecode camp article</li> <li>Video courses: Code academy and   Udemy</li> </ul>"}, {"location": "git/#pull-request-process", "title": "Pull Request Process", "text": "<p>This part of the doc is shamefully edited from the source. It was for the k8s project but they are good practices that work for all the projects. It explains the process and best practices for submitting a PR It should serve as a reference for all contributors, and be useful especially to new and infrequent submitters.</p>"}, {"location": "git/#before-you-submit-a-pr", "title": "Before You Submit a PR", "text": "<p>This guide is for contributors who already have a PR to submit. If you're looking for information on setting up your developer environment and creating code to contribute to the project, search the development guide.</p> <p>Make sure your PR adheres to the projects best practices. These include following project conventions, making small PRs, and commenting thoroughly.</p>"}, {"location": "git/#run-local-verifications", "title": "Run Local Verifications", "text": "<p>You can run the tests in local before you submit your PR to predict the pass or fail of continuous integration.</p>"}, {"location": "git/#why-is-my-pr-not-getting-reviewed", "title": "Why is my PR not getting reviewed?", "text": "<p>A few factors affect how long your PR might wait for review.</p> <p>If it's the last few weeks of a milestone, we need to reduce churn and stabilize.</p> <p>Or, it could be related to best practices. One common issue is that the PR is too big to review. Let's say you've touched 39 files and have 8657 insertions. When your would-be reviewers pull up the diffs, they run away - this PR is going to take 4 hours to review and they don't have 4 hours right now. They'll get to it later, just as soon as they have more free time (ha!).</p> <p>There is a detailed rundown of best practices, including how to avoid too-lengthy PRs, in the next section.</p> <p>But, if you've already followed the best practices and you still aren't getting any PR love, here are some things you can do to move the process along:</p> <ul> <li> <p>Make sure that your PR has an assigned reviewer (assignee in GitHub). If not,   reply to the PR comment stream asking for a reviewer to be assigned.</p> </li> <li> <p>Ping the assignee (@username) on the PR comment stream, and ask for an   estimate of when they can get to the review.</p> </li> <li> <p>Ping the assignee by email (many of us have publicly available email   addresses).</p> </li> <li> <p>If you're a member of the organization ping the team (via @team-name) that   works in the area you're submitting code.</p> </li> <li> <p>If you have fixed all the issues from a review, and you haven't heard back,   you should ping the assignee on the comment stream with a \"please take another   look\" (<code>PTAL</code>) or similar comment indicating that you are ready for another   review.</p> </li> </ul> <p>Read on to learn more about how to get faster reviews by following best practices.</p>"}, {"location": "git/#best-practices-for-faster-reviews", "title": "Best Practices for Faster Reviews", "text": "<p>You've just had a brilliant idea on how to make a project better. Let's call that idea Feature-X. Feature-X is not even that complicated. You have a pretty good idea of how to implement it. You jump in and implement it, fixing a bunch of stuff along the way. You send your PR - this is awesome! And it sits. And sits. A week goes by and nobody reviews it. Finally, someone offers a few comments, which you fix up and wait for more review. And you wait. Another week or two go by. This is horrible.</p> <p>Let's talk about best practices so your PR gets reviewed quickly.</p>"}, {"location": "git/#familiarize-yourself-with-project-conventions", "title": "Familiarize yourself with project conventions", "text": "<ul> <li>Search for the Development guide</li> <li>Search for the Coding conventions</li> <li>Search for the API conventions</li> </ul>"}, {"location": "git/#is-the-feature-wanted-make-a-design-doc-or-sketch-pr", "title": "Is the feature wanted? Make a Design Doc or Sketch PR", "text": "<p>Are you sure Feature-X is something the project team wants or will accept? Is it implemented to fit with other changes in flight? Are you willing to bet a few days or weeks of work on it?</p> <p>It's better to get confirmation beforehand. There are two ways to do this:</p> <ul> <li>Make a proposal doc (in docs/proposals; for example   the QoS proposal), or reach out to the affected   special interest group (SIG). Some projects have that</li> <li>Coordinate your effort with SIG Docs ahead of time</li> <li>Make a sketch PR (e.g., just the API or Go interface). Write or code up just   enough to express the idea and the design and why you made those choices</li> </ul> <p>Or, do all of the above.</p> <p>Be clear about what type of feedback you are asking for when you submit a proposal doc or sketch PR.</p> <p>Now, if we ask you to change the design, you won't have to re-write it all.</p>"}, {"location": "git/#smaller-is-better-small-commits-small-prs", "title": "Smaller Is Better: Small Commits, Small PRs", "text": "<p>Small commits and small PRs get reviewed faster and are more likely to be correct than big ones.</p> <p>Attention is a scarce resource. If your PR takes 60 minutes to review, the reviewer's eye for detail is not as keen in the last 30 minutes as it was in the first. It might not get reviewed at all if it requires a large continuous block of time from the reviewer.</p> <p>Breaking up commits</p> <p>Break up your PR into multiple commits, at logical break points.</p> <p>Making a series of discrete commits is a powerful way to express the evolution of an idea or the different ideas that make up a single feature. Strive to group logically distinct ideas into separate commits.</p> <p>For example, if you found that Feature-X needed some prefactoring to fit in, make a commit that JUST does that prefactoring. Then make a new commit for Feature-X.</p> <p>Strike a balance with the number of commits. A PR with 25 commits is still very cumbersome to review, so use judgment.</p> <p>Breaking up PRs</p> <p>Or, going back to our prefactoring example, you could also fork a new branch, do the prefactoring there and send a PR for that. If you can extract whole ideas from your PR and send those as PRs of their own, you can avoid the painful problem of continually rebasing.</p> <p>Multiple small PRs are often better than multiple commits. Don't worry about flooding us with PRs. We'd rather have 100 small, obvious PRs than 10 unreviewable monoliths.</p> <p>We want every PR to be useful on its own, so use your best judgment on what should be a PR vs. a commit.</p> <p>As a rule of thumb, if your PR is directly related to Feature-X and nothing else, it should probably be part of the Feature-X PR. If you can explain why you are doing seemingly no-op work (\"it makes the Feature-X change easier, I promise\") we'll probably be OK with it. If you can imagine someone finding value independently of Feature-X, try it as a PR. (Do not link pull requests by <code>#</code> in a commit description, because GitHub creates lots of spam. Instead, reference other PRs via the PR your commit is in.)</p>"}, {"location": "git/#open-a-different-pr-for-fixes-and-generic-features", "title": "Open a Different PR for Fixes and Generic Features", "text": "<p>Put changes that are unrelated to your feature into a different PR.</p> <p>Often, as you are implementing Feature-X, you will find bad comments, poorly named functions, bad structure, weak type-safety, etc.</p> <p>You absolutely should fix those things (or at least file issues, please) - but not in the same PR as your feature. Otherwise, your diff will have way too many changes, and your reviewer won't see the forest for the trees.</p> <p>Look for opportunities to pull out generic features.</p> <p>For example, if you find yourself touching a lot of modules, think about the dependencies you are introducing between packages. Can some of what you're doing be made more generic and moved up and out of the Feature-X package? Do you need to use a function or type from an otherwise unrelated package? If so, promote! We have places for hosting more generic code.</p> <p>Likewise, if Feature-X is similar in form to Feature-W which was checked in last month, and you're duplicating some tricky stuff from Feature-W, consider prefactoring the core logic out and using it in both Feature-W and Feature-X. (Do that in its own commit or PR, please.)</p>"}, {"location": "git/#comments-matter", "title": "Comments Matter", "text": "<p>In your code, if someone might not understand why you did something (or you won't remember why later), comment it. Many code-review comments are about this exact issue.</p> <p>If you think there's something pretty obvious that we could follow up on, add a TODO.</p>"}, {"location": "git/#test", "title": "Test", "text": "<p>Nothing is more frustrating than starting a review, only to find that the tests are inadequate or absent. Very few PRs can touch code and NOT touch tests.</p> <p>If you don't know how to test Feature-X, please ask! We'll be happy to help you design things for easy testing or to suggest appropriate test cases.</p>"}, {"location": "git/#squashing-and-commit-titles", "title": "Squashing and Commit Titles", "text": "<p>Your reviewer has finally sent you feedback on Feature-X.</p> <p>Make the fixups, and don't squash yet. Put them in a new commit, and re-push. That way your reviewer can look at the new commit on its own, which is much faster than starting over.</p> <p>We might still ask you to clean up your commits at the very end for the sake of a more readable history, but don't do this until asked: typically at the point where the PR would otherwise be tagged <code>LGTM</code>.</p> <p>Each commit should have a good title line (<code>&lt;70</code> characters) and include an additional description paragraph describing in more detail the change intended.</p> <p>General squashing guidelines:</p> <ul> <li>Sausage =&gt; squash</li> </ul> <p>Do squash when there are several commits to fix bugs in the original commit(s), address reviewer feedback, etc. Really we only want to see the end state and commit message for the whole PR.</p> <ul> <li>Layers =&gt; don't squash</li> </ul> <p>Don't squash when there are independent changes layered to achieve a single goal. For instance, writing a code munger could be one commit, applying it could be another, and adding a precommit check could be a third. One could argue they should be separate PRs, but there's really no way to test/review the munger without seeing it applied, and there needs to be a precommit check to ensure the munged output doesn't immediately get out of date.</p> <p>A commit, as much as possible, should be a single logical change.</p>"}, {"location": "git/#kiss-yagni-mvp-etc", "title": "KISS, YAGNI, MVP, etc.", "text": "<p>Sometimes we need to remind each other of core tenets of software design - Keep It Simple, You Aren't Gonna Need It, Minimum Viable Product, and so on. Adding a feature \"because we might need it later\" is antithetical to software that ships. Add the things you need NOW and (ideally) leave room for things you might need later - but don't implement them now.</p>"}, {"location": "git/#its-ok-to-push-back", "title": "It's OK to Push Back", "text": "<p>Sometimes reviewers make mistakes. It's OK to push back on changes your reviewer requested. If you have a good reason for doing something a certain way, you are absolutely allowed to debate the merits of a requested change. Both the reviewer and reviewee should strive to discuss these issues in a polite and respectful manner.</p> <p>You might be overruled, but you might also prevail. We're pretty reasonable people. Mostly.</p> <p>Another phenomenon of open-source projects (where anyone can comment on any issue) is the dog-pile - your PR gets so many comments from so many people it becomes hard to follow. In this situation, you can ask the primary reviewer (assignee) whether they want you to fork a new PR to clear out all the comments. You don't HAVE to fix every issue raised by every person who feels like commenting, but you should answer reasonable comments with an explanation.</p>"}, {"location": "git/#common-sense-and-courtesy", "title": "Common Sense and Courtesy", "text": "<p>No document can take the place of common sense and good taste. Use your best judgment, while you put a bit of thought into how your work can be made easier to review. If you do these things your PRs will get merged with less friction.</p>"}, {"location": "git/#split-long-pr-into-smaller-ones", "title": "Split long PR into smaller ones", "text": "<ul> <li> <p>Start a new branch from where you want to merge.</p> </li> <li> <p>Start an interactive rebase on HEAD:</p> </li> </ul> <pre><code>git rebase -i HEAD\n</code></pre> <ul> <li>Get the commits you want: Now comes the clever part, we are going to pick out   all the commits we care about from 112-new-feature-branch using the following   command:</li> </ul> <pre><code>git log --oneline --reverse HEAD..112-new-feature-branch -- app/models/ spec/models\n</code></pre> <p>Woah thats quite the line! Let\u2019s dissect it first:</p> <ul> <li><code>git log</code> shows a log of what you have done in your project.</li> <li><code>--online</code> formats the output from a few lines (including author and time of     commit), to just \u201c[sha-hash-of-commit] [description-of-commit]\u201d</li> <li><code>--reverse</code> reverses the log output chronologically (so oldest commit first,     newest last).</li> <li><code>112-new-feature-branch..HEAD</code> shows the difference in commits from your     current branch (HEAD) and the branch you are interested in     112-new-feature-branch.</li> <li><code>-- app/models/ spec/models</code> Only show commits that changed files in     app/models/ or spec/models So that we confine the changes to our model and     its tests.</li> </ul> <p>Now if you are using vim (or vi or neovim) you can put the results of this   command directly into your rebase-todo (which was opened when starting the   rebase) using the :r command like so:</p> <pre><code>:r !git log --oneline --reverse HEAD..112-new-feature-branch -- app/models/\n</code></pre> <ul> <li>Review the commits you want: Now you have a chance to go though your todo once   again. First you should remove the noop from above, since you actually do   something now. Second you should check the diffs of the sha-hashes.</li> </ul> <p>Note: If you are using vim, you might already have the fugitive plug-in. If   you haven\u2019t changed the standard configuration, you can just move your cursor   over the sha-hashes and press K (note that its capitalized) to see the diff of   that commit.</p> <p>If you don\u2019t have fugitive or don\u2019t use vim, you can check the diff using git   show SHA-HASH (for example <code>git show c4f74d0</code>), which shows the commits data.</p> <p>Now you can prepend and even rearrange the commits (Be careful rearranging or   leaving out commits, you might have to fix conflicts later).</p> <ul> <li>Execute the rebase: Now you can save and exit the editor and git will try to   execute the rebase. If you have conflicts you can fix them just like you do   with merges and then continue using git rebase --continue.</li> </ul> <p>If you feel like something is going terribly wrong (for example you have a   bunch of conflicts in just a few commits), you can abort the rebase using git   rebase --abort and it will be like nothing ever happened.</p>"}, {"location": "git/#git-workflow", "title": "Git workflow", "text": "<p>There are many ways of using git, one of the most popular is git flow, please read this article to understand it before going on.</p> <p>Unless you are part of a big team that delivers software that needs to maintain many versions, it's not worth using git flow as it's too complex and cumbersome. Instead I suggest a variation of the Github workflow.</p> <p>To carry out a reliable continuous delivery we must work to comply with the following list of best practices:</p> <ul> <li>Everything must be in the git server: source code, tests, pipelines, scripts,   templates and documentation.</li> <li>There is only a main branch (main) whose key is that everything is in this   branch must be always stable and deployable into production at any time.</li> <li>New branches are created from main in order to develop new features that   should be merged into main branch in short development cycles.</li> <li>It is highly recommended to do small commits to have more control over what is   being done and to avoid discarding many lines of code if a rollback has to be   done.</li> <li>A commit message policy should be set so that they are clear and conform the   same pattern, for example semantic versioning.</li> <li><code>main</code> is blocked to reject direct pushes as well as to protect it of   catastrophic deletion. Only pre-validated merge requests are accepted.</li> <li>When a feature is ready, we will open a merge request to merge changes into   <code>main</code> branch.</li> <li>Use webhooks to automate the execution of tests and validation tasks in the CI   server before/after adding changes in main.</li> <li>It is not needed to discard a merge request if any of the validation tasks   failed. We check the code and when the changes are pushed, the CI server will   relaunch the validation tasks.</li> <li>If all validation tasks pass, we will assign the merge request to two team   developers to review the feature code.</li> <li>After both reviewers validate the code, the merge request can be accepted and   the feature branch may be deleted.</li> <li>A clear versioning policy must be adopted for all generated artifacts.</li> <li>Each artifact must be generated once and be promoted to the different   environments in different stages.</li> </ul> <p>When a developer wants to add code to main should proceed as follows:</p> <ul> <li>Wait until the pipeline execution ends if it exists. If that process fails,   then the developer must help to other team members to fix the issue before   requesting a new merge request.</li> <li>Pull the changes from main and resolve the conflicts locally before pushing   the code to the new feature branch.</li> <li>Run a local script that compiles and executes the tests before committing   changes. This task can be done executing it manually by the developer or using   a git precommit.</li> <li>Open a new merge request setting the feature branch as source branch and main   as target branch.</li> <li>The CI server is notified of the new merge request and executes the pipeline   which compiles the source code, executes the tests, deploys the artifact, etc.</li> <li>If there are errors in the previous step, the developer must fix the code and   push it to the git server as soon as possible so that the CI server validate   once again the merge request.</li> <li>If no errors, the CI server will mark the merge request as OK and the   developer can assign it to two other team members to review the feature code.</li> <li>At this point, the developer can start with other task.</li> </ul> <p>Considerations</p> <p>The build process and the execution of the tests have to be pretty fast. It should not exceed about 10 minutes.</p> <p>Unit tests must be guarantee that they are completely unitary; they must be executed without starting the context of the application, they must not access to the DDBB, external systems, file system, etc.</p>"}, {"location": "git/#naming-conventions", "title": "Naming conventions", "text": "<p>The best idea is to use Semantic Versioning to define the names of the branches, for example: <code>feat/add-command-line-support</code> or <code>fix/correct-security-issue</code>, and also for the commit messages.</p>"}, {"location": "git/#tag-versioning-policy", "title": "Tag versioning policy", "text": "<p>We will also adopt semantic versioning policy on version management.</p>"}, {"location": "git/#versioning-control", "title": "Versioning control", "text": "<p>When a branch is merged into main, the CI server launches a job which generates a new artifact release as follow:</p> <ul> <li>The new version number is calculated taken into account the above   considerations.</li> <li>Generates a new artifact named as appname-major.minor.patch.build</li> <li>Upload the previous artifact to the artifact repository manager.</li> <li>Create a git tag on the repository with the same version identifier,   major.minor.patch.build</li> <li>Automatically deploy the artifact on the desired environment (dev, pre, etc)</li> </ul>"}, {"location": "git/#hotfixing", "title": "Hotfixing", "text": "<p>Hotfix should be developed and fixed using one of the next cases, which has been defined by preference order:</p>"}, {"location": "git/#case-1", "title": "Case 1", "text": "<p>In this case, we have pushed new code to \"main\" branch since the last deploy on production and we want to deploy the new code with the fix code.</p> <p>We have to follow the next steps:</p> <ul> <li>Create a branch \"Hotfix\" from commit/tag of the last deploy</li> <li>Fix the bug in \"hotfix\" branch</li> <li>Merge the new branch to \"main\"</li> <li>Deploy main branch</li> </ul>"}, {"location": "git/#case-2", "title": "Case 2", "text": "<p>In this case, we have pushed new code to \"main\" branch since the last deploy on production but we don't want to deploy the new code with the fix code.</p> <p>We have to follow the next steps:</p> <ul> <li>Create a branch \"Hotfix\" from commit/tag of the last deploy</li> <li>Fix the bug in \"hotfix\" branch</li> <li>Deploy main branch</li> <li>Merge the new branch to \"main.</li> </ul>"}, {"location": "git/#case-3", "title": "Case 3", "text": "<p>In this case, we have pushed new code to \"main\" branch since the last deploy on production but we don't want to deploy the new code with the fix code.</p> <p>We have to follow the next steps:</p> <ul> <li>Create a branch \"Hotfix\" from commit/tag of the last deploy</li> <li>Fix the bug in \"hotfix\" branch</li> <li>Deploy main branch</li> <li>Merge the new branch to \"main.</li> </ul>"}, {"location": "git/#git-housekeeping", "title": "Git housekeeping", "text": "<p>The best option is to:</p> <pre><code>git fetch --prune\ngit-sweep cleanup\n</code></pre> <p>To remove the local branches you can:</p> <pre><code>cd myrepo\ngit remote add local $(pwd)\ngit-sweep cleanup --origin=local\n</code></pre> <ul> <li>git-sweep: For local branches</li> <li>archaeologit: Tool to search   strings in the history of a github user</li> <li>jessfraz made a tool ghb0t: For github</li> </ul>"}, {"location": "git/#submodules", "title": "Submodules", "text": "<p>Shamefully edited from the docs</p> <p>It often happens that while working on one project, you need to use another project from within it. Perhaps it\u2019s a library that a third party developed or that you\u2019re developing separately and using in multiple parent projects. A common issue arises in these scenarios: you want to be able to treat the two projects as separate yet still be able to use one from within the other.</p> <p>Here\u2019s an example. Suppose you\u2019re developing a website and creating Atom feeds. Instead of writing your own Atom-generating code, you decide to use a library. You\u2019re likely to have to either include this code from a shared library like a CPAN install or Ruby gem, or copy the source code into your own project tree. The issue with including the library is that it\u2019s difficult to customize the library in any way and often more difficult to deploy it, because you need to make sure every client has that library available. The issue with copying the code into your own project is that any custom changes you make are difficult to merge when upstream changes become available.</p> <p>Git addresses this issue using submodules. Submodules allow you to keep a Git repository as a subdirectory of another Git repository. This lets you clone another repository into your project and keep your commits separate.</p> <p>It often happens that while working on one project, you need to use another project from within it. Perhaps it\u2019s a library that a third party developed or that you\u2019re developing separately and using in multiple parent projects. A common issue arises in these scenarios: you want to be able to treat the two projects as separate yet still be able to use one from within the other.</p> <p>Here\u2019s an example. Suppose you\u2019re developing a website and creating Atom feeds. Instead of writing your own Atom-generating code, you decide to use a library. You\u2019re likely to have to either include this code from a shared library like a CPAN install or Ruby gem, or copy the source code into your own project tree. The issue with including the library is that it\u2019s difficult to customize the library in any way and often more difficult to deploy it, because you need to make sure every client has that library available. The issue with copying the code into your own project is that any custom changes you make are difficult to merge when upstream changes become available.</p> <p>Git addresses this issue using submodules. Submodules allow you to keep a Git repository as a subdirectory of another Git repository. This lets you clone another repository into your project and keep your commits separate.</p>"}, {"location": "git/#submodule-tips", "title": "Submodule tips", "text": ""}, {"location": "git/#submodule-foreach", "title": "Submodule Foreach", "text": "<p>There is a foreach submodule command to run some arbitrary command in each submodule. This can be really helpful if you have a number of submodules in the same project.</p> <p>For example, let\u2019s say we want to start a new feature or do a bugfix and we have work going on in several submodules. We can easily stash all the work in all our submodules.</p> <pre><code>git submodule foreach 'git stash'\n</code></pre> <p>Then we can create a new branch and switch to it in all our submodules.</p> <pre><code>git submodule foreach 'git checkout -b featureA'\n</code></pre> <p>You get the idea. One really useful thing you can do is produce a nice unified diff of what is changed in your main project and all your subprojects as well.</p> <pre><code>git diff; git submodule foreach 'git diff'\n</code></pre>"}, {"location": "git/#useful-aliases", "title": "Useful Aliases", "text": "<p>You may want to set up some aliases for some of these commands as they can be quite long and you can\u2019t set configuration options for most of them to make them defaults. We covered setting up Git aliases in Git Aliases, but here is an example of what you may want to set up if you plan on working with submodules in Git a lot.</p> <pre><code>git config alias.sdiff '!'\"git diff &amp;&amp; git submodule foreach 'git diff'\"\ngit config alias.spush 'push --recurse-submodules=on-demand'\ngit config alias.supdate 'submodule update --remote --merge'\n</code></pre> <p>This way you can simply run git supdate when you want to update your submodules, or git spush to push with submodule dependency checking.</p>"}, {"location": "git/#encrypt-sensitive-information", "title": "Encrypt sensitive information", "text": "<p>Use git-crypt.</p>"}, {"location": "git/#use-different-git-configs", "title": "Use different git configs", "text": "<p>Include in your <code>~/.gitconfig</code></p> <pre><code>[includeIf \"gitdir:~/company_A/\"]\n  path = ~/.config/git/company_A.config\n</code></pre> <p>Every repository you create under that directory it will append the other configuration</p>"}, {"location": "git/#renaming-from-master-to-main", "title": "Renaming from master to main", "text": "<p>There's been a movement to migrate from <code>master</code> to <code>main</code>, the reason behind it is that the initial branch name, <code>master</code>, is offensive to some people and we empathize with those hurt by the use of that term.</p> <p>Existing versions of Git are capable of working with any branch name; there's nothing special about <code>master</code> except that it has historically been the name used for the first branch when creating a new repository from scratch (with the <code>git init</code> command). Thus many projects use it to represent the primary line of development. We support and encourage projects to switch to branch names that are meaningful and inclusive.</p> <p>To configure <code>git</code> to use <code>main</code> by default run:</p> <pre><code>git config --global init.defaultBranch main\n</code></pre> <p>It only works on since git version 2.28.0, so you're stuck with manually changing it if you have an earlier version.</p>"}, {"location": "git/#changes-controversy", "title": "Change's Controversy", "text": "<p>The change is not free of controversy, for example in the PDM project some people are not sure that it's needed for many reasons. Let's see each of them:</p> <ul> <li>The reason people are implementing the change is because other people are   doing it: After a quick search I found that the first one to do the change   was   the software freedom conservancy with the Git project.   You can also see Python,   Django,   Redis,   Drupal,   CouchDB and   Github's   statements.</li> </ul> <p>As we're not part of the deciding organisms of the collectives doing the   changes, all we can use are their statements and discussions to guess what are   the reasons behind their support of the change. Despite that some of them do   use the argument that other communities do support the change to emphasize the   need of the change, all of them mention that the main reason is that the term   is offensive to some people.</p> <ul> <li>I don't see an issue using the term master: If you relate to this statement   it can be because you're not part of the communities that suffer the   oppression tied to the term, and that makes you blind to the issue. It's a   lesson I learned on my own skin throughout the years. There are thousand of   situations, gestures, double meaning words and sentences that went unnoticed   by me until I started discussing it with the people that are suffering them   (women, racialized people, LGTBQI+, ...). Throughout my experience I've seen   that the more privileged you are, the blinder you become. You can read more on   privileged blindness   here,   here or   here   (I've skimmed through the articles, and are the first articles I've found,   there are probably better references).</li> </ul> <p>I'm not saying that privileged people are not aware of the issues or that they   can even raise them. We can do so and more we read, discuss and train   ourselves, the better we'll detect them. All I'm saying is that a non   privileged person will always detect more because they suffer them daily.</p> <p>I understand that for you there is no issue using the word master, there   wasn't an issue for me either until I saw these projects doing the change,   again I was blinded to the issue as I'm not suffering it. That's because   change is not meant for us, as we're not triggered by it. The change is   targeted to the people that do perceive that <code>master</code> is an offensive term.   What we can do is empathize with them and follow this tiny tiny tiny gesture.   It's the least we can do.</p> <p>Think of a term that triggers you, such as heil hitler, imagine that those   words were being used to define the main branch of your code, and that   everyday you sit in front of your computer you see them. You'll probably be   reminded of the historic events, concepts, feelings that are tied to that term   each time you see it, and being them quite negative it can slowly mine you.   Therefore it's legit that you wouldn't want to be exposed to that negative   effects.</p> <ul> <li>I don't see who will benefit from this change: Probably the people that   belongs to communities that are and have been under constant oppression for a   very long time, in this case, specially the racialized ones which have   suffered slavery.</li> </ul> <p>Sadly you will probably won't see many the affected people speak in these   discussions, first because there are not that many, sadly the IT world is   dominated by middle aged, economically comfortable, white, cis, hetero, males.   Small changes like this are meant to foster diversity in the community by   allowing them being more comfortable. Secondly because when they see these   debates they move on as they are so fed up on teaching privileged people of   their privileges. They not only have to suffer the oppression, we also put the   burden on their shoulders to teach us.</p> <p>As and ending thought, if you see yourself being specially troubled by the change, having a discomfort feeling and strong reactions. In my experience these signs are characteristic of privileged people that feel that their privileges are being threatened, I've felt them myself countless times. When I feel it, I usually do two things, fight them as strong as I can, or embrace them, analyze them, and go to the root of them. Depending on how much energy I have I go with the easy or the hard one. I'm not saying that it's you're case, but it could be.</p>"}, {"location": "git/#configuration", "title": "Configuration", "text": ""}, {"location": "git/#set-the-upstream-remote-by-default", "title": "Set the upstream remote by default", "text": "<pre><code>git config --global --add push.default current\ngit config --global --add push.autoSetupRemote true\n</code></pre>"}, {"location": "git/#snippets", "title": "Snippets", "text": ""}, {"location": "git/#revert-a-commit", "title": "Revert a commit", "text": "<pre><code>git revert commit_id\n</code></pre>"}, {"location": "git/#get-interesting-stats-of-the-repo", "title": "Get interesting stats of the repo", "text": "<p>Number of commits of the last year per user:</p> <pre><code>git shortlog -sne --since=\"31 Dec 2020\" --before=\"31 Dec 2021\"\n</code></pre> <p>You can also use <code>git-fame</code> to extract a more detailed report:</p> <pre><code>$: git-fame --since 1.year --cost hour --loc ins -w -M -C\n\n| Author          |   hrs |   loc |   coms |   fils |  distribution   |\n|:----------------|------:|------:|-------:|-------:|:----------------|\n| Lyz             |    10 | 28933 |    112 |    238 | 64.1/33.3/75.8  |\n| GitHub Action   |     2 | 16194 |    220 |     73 | 35.9/65.5/23.2  |\n| Alexander Gil   |     2 |     9 |      1 |      1 | 0.0/ 0.3/ 0.3   |\n| n0rt3y5ur       |     2 |     1 |      1 |      1 | 0.0/ 0.3/ 0.3   |\n| Guilherme Danno |     2 |     1 |      1 |      1 | 0.0/ 0.3/ 0.3   |\n| lyz-code        |     2 |     0 |      1 |      0 | 0.0/ 0.3/ 0.0   |\n</code></pre> <p>You can use <code>pipx install git-fame</code> to install it.</p>"}, {"location": "git/#references", "title": "References", "text": "<ul> <li>FAQ</li> <li>Funny FAQ</li> <li>Nvie post on branching model</li> </ul>"}, {"location": "git/#courses", "title": "Courses", "text": "<ul> <li>W3 git course</li> <li>Learngitbranching interactive tutorial</li> <li>katakoda</li> <li>Code academy</li> <li>Udemy</li> <li>Freecode camp article</li> </ul>"}, {"location": "git/#tools", "title": "Tools", "text": "<ul> <li>git-extras</li> </ul>"}, {"location": "gitea/", "title": "Gitea", "text": "<p>Gitea is a community managed lightweight code hosting solution written in Go. It's the best self hosted Github alternative in my opinion.</p>"}, {"location": "gitea/#installation", "title": "Installation", "text": "<p>Gitea provides automatically updated Docker images within its Docker Hub organisation.</p>"}, {"location": "gitea/#disable-the-regular-login-use-only-oauth", "title": "Disable the regular login, use only Oauth", "text": "<p>Inside your <code>custom</code> directory which may be <code>/var/lib/gitea/custom</code>:</p> <ul> <li>Create the directories <code>templates/user/auth</code>, </li> <li>Create the <code>signin_inner.tmpl</code> file with the next contents:   <pre><code>                {{if or (not .LinkAccountMode) (and .LinkAccountMode .LinkAccountModeSignIn)}}\n              {{template \"base/alert\" .}}\n              {{end}}\n              &lt;h4 class=\"ui top attached header center\"&gt;\n                      {{if .LinkAccountMode}}\n                              {{.locale.Tr \"auth.oauth_signin_title\"}}\n                      {{else}}\n                              {{.locale.Tr \"auth.login_userpass\"}}\n                      {{end}}\n              &lt;/h4&gt;\n              &lt;div class=\"ui attached segment\"&gt;\n                      &lt;form class=\"ui form\" action=\"{{.SignInLink}}\" method=\"post\"&gt;\n                      {{.CsrfTokenHtml}}\n                      {{if and .OrderedOAuth2Names .OAuth2Providers}}\n                      &lt;div class=\"ui attached segment\"&gt;\n                              &lt;div class=\"oauth2 center\"&gt;\n                                      &lt;div id=\"oauth2-login-loader\" class=\"ui disabled centered loader\"&gt;&lt;/div&gt;\n                                      &lt;div&gt;\n                                              &lt;div id=\"oauth2-login-navigator\"&gt;\n                                                      &lt;p&gt;Sign in with &lt;/p&gt;\n                                                      {{range $key := .OrderedOAuth2Names}}\n                                                              {{$provider := index $.OAuth2Providers $key}}\n                                                              &lt;a href=\"{{AppSubUrl}}/user/oauth2/{{$key}}\"&gt;\n                                                                      &lt;img\n                                                                              alt=\"{{$provider.DisplayName}}{{if eq $provider.Name \"openidConnect\"}} ({{$key}}){{end}}\"\n                                                                              title=\"{{$provider.DisplayName}}{{if eq $provider.Name \"openidConnect\"}} ({{$key}}){{end}}\"\n                                                                              class=\"{{$provider.Name}} oauth-login-image\"\n                                                                              src=\"{{AppSubUrl}}{{$provider.Image}}\"\n                                                                      &gt;&lt;/a&gt;\n                                                      {{end}}\n                                              &lt;/div&gt;\n                                      &lt;/div&gt;\n                              &lt;/div&gt;\n                      &lt;/div&gt;\n                      {{end}}\n                      &lt;/form&gt;\n              &lt;/div&gt;\n</code></pre></li> <li>Download the <code>signin_inner.tmpl</code></li> </ul>"}, {"location": "gitea/#configure-it-with-terraform", "title": "Configure it with terraform", "text": "<p>Gitea can be configured through terraform too. There is an official provider that doesn't work, there's a fork that does though. Sadly it doesn't yet support configuring Oauth Authentication sources. Be careful <code>gitea_oauth2_app</code> looks to be the right resource to do that, but instead it configures Gitea to be the Oauth provider, not a consumer.</p> <p>To configure the provider you need to specify the url and a Gitea API token, keeping in mind that whoever gets access to this information will have access and full permissions on your Gitea instance it's critical that you store this information well. We'll use <code>sops</code> to encrypt the token with GPG..</p> <p>First create a Gitea user under <code>Site Administration/User Accounts/</code> with the <code>terraform</code> name (use your Oauth2 provider if you have one!).</p> <p>Then log in with that user and create a token with name <code>Terraform</code> under <code>Settings/Applications</code>, copy it to your clipboard.</p> <p>Configure <code>sops</code> by defining the gpg keys in a <code>.sops.yaml</code> file at the top of your repository:</p> <pre><code>---\ncreation_rules:\n- pgp: &gt;-\n2829BASDFHWEGWG23WDSLKGL323534J35LKWERQS,\n2GEFDBW349YHEDOH2T0GE9RH0NEORIG342RFSLHH\n</code></pre> <p>Then create the secrets file with the command <code>sops secrets.enc.json</code> somewhere in your terraform repository. For example:</p> <pre><code>{\n\"gitea_token\": \"paste the token here\"\n}\n</code></pre> <pre><code>terraform {\n  required_providers {\n    gitea = {\n      source  = \"Lerentis/gitea\"\n      version = \"~&gt; 0.12.1\"\n    }\n    sops = {\n      source = \"carlpett/sops\"\n      version = \"~&gt; 0.5\"\n    }\n  }\n}\n\nprovider \"gitea\" {\n  base_url   = \"https://gitea.your-domain.org\"\n  token = data.sops_file.secrets.data[\"gitea_token\"]\n}\n</code></pre>"}, {"location": "gitea/#create-an-organization", "title": "Create an organization", "text": "<p>If you manage your users externally for example with an Oauth2 provider like Authentik you don't need to create a resource for the users, use a <code>data</code> instead:</p> <pre><code>resource \"gitea_org\" \"docker_compose\" {\nname = \"docker-compose\"\n}\n\nresource \"gitea_team\" \"docker_compose\" {\nname         = \"Developers\"\norganisation = gitea_org.docker_compose.name\npermission   = \"owner\"\nmembers      = [\ndata.gitea_user.lyz.username,\n]\n}\n</code></pre> <p>If you have many organizations that share the same users you can use variables.</p> <pre><code>resource \"gitea_org\" \"docker_compose\" {\nname = \"docker-compose\"\n}\n\nresource \"gitea_team\" \"docker_compose\" {\nname         = \"Developers\"\norganisation = gitea_org.docker_compose.name\npermission   = \"owner\"\nmembers      = [\ndata.gitea_user.lyz.username,\n]\n}\n</code></pre>"}, {"location": "gitea/#create-an-admin-user-through-the-command-line", "title": "Create an admin user through the command line", "text": "<pre><code>gitea --config /etc/gitea/app.ini admin user create --admin --email email --username user_name --password password\n</code></pre> <p>Or you can change the admin's password:</p> <pre><code>gitea --config /etc/gitea/app.ini admin user change-password -u username -p password\n</code></pre>"}, {"location": "gitea/#references", "title": "References", "text": "<ul> <li>Home</li> <li> <p>Docs</p> </li> <li> <p>Terraform provider docs</p> </li> <li>Terraform provider source code</li> </ul>"}, {"location": "graylog/", "title": "Graylog", "text": "<p>Graylog is a log management tool</p>"}, {"location": "graylog/#tips", "title": "Tips", "text": ""}, {"location": "graylog/#send-a-test-message-to-check-an-input", "title": "Send a test message to check an input", "text": "<p>The next line will send a test message to the TCP 12201 port of the graylog server, if you use UDP, add the <code>-u</code> flag to the <code>nc</code> command.</p> <pre><code>echo -e '{\"version\": \"1.1\",\"host\":\"example.org\",\"short_message\":\"Short message\",\"full_message\":\"Backtrace here\\n\\nmore stuff\",\"level\":1,\"_user_id\":9001,\"_some_info\":\"foo\",\"_some_env_var\":\"bar\"}\\0' | nc -w 1 my.graylog.server 12201\n</code></pre> <p>To see if it arrives, you can check the <code>Input</code> you're trying to access, or at a lower level, you can <code>ngrep</code> with:</p> <pre><code>ngrep -d any port 12201\n</code></pre> <p>Or if you're using UDP:</p> <pre><code>ngrep -d any '' udp port 12201\n</code></pre>"}, {"location": "graylog/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "helm_git/", "title": "helm-git", "text": "<p>helm-git is a helm downloader plugin that provides GIT protocol support.</p> <p>This fits the following use cases:</p> <ul> <li>Need to keep charts private.</li> <li>Doesn't want to package charts before installing.</li> <li>Charts in a sub-path, or with another ref than master.</li> <li>Pull values files directly from (private) Git repository.</li> </ul>"}, {"location": "helm_git/#installation", "title": "Installation", "text": "<pre><code>helm plugin install https://github.com/aslafy-z/helm-git --version 0.11.1\n</code></pre>"}, {"location": "helm_git/#usage", "title": "Usage", "text": "<p><code>helm-git</code> will package any chart that is not so you can directly reference paths to original charts.</p> <p>Here's the Git urls format, followed by examples:</p> <pre><code>git+https://[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&amp;sparse=0][&amp;depupdate=0]]\ngit+ssh://git@[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&amp;sparse=0][&amp;depupdate=0]]\ngit+file://[path/to/repo]@[path/to/charts][?[ref=git-ref][&amp;sparse=0][&amp;depupdate=0]]\n\ngit+https://github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&amp;sparse=0\ngit+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&amp;sparse=1\ngit+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2\ngit+https://github.com/istio/istio@install/kubernetes/helm?ref=1.5.4&amp;sparse=0&amp;depupdate=0\n</code></pre> <p>Add your repository:</p> <pre><code>helm repo add cert-manager git+https://github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2\n</code></pre> <p>You can use it as any other Helm chart repository. Try:</p> <pre><code>$ helm search cert-manager\nNAME                                    CHART VERSION   APP VERSION     DESCRIPTION\ncert-manager/cert-manager               v0.6.6          v0.6.2          A Helm chart for cert-manager\n\n$ helm install cert-manager/cert-manager --version \"0.6.6\"\n</code></pre> <p>Fetching also works:</p> <pre><code>helm fetch cert-manager/cert-manager --version \"0.6.6\"\nhelm fetch git+https://github.com/jetstack/cert-manager@deploy/charts/cert-manager-v0.6.2.tgz?ref=v0.6.2\n</code></pre>"}, {"location": "helm_git/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "krew/", "title": "Krew", "text": "<p>Krew is a tool that makes it easy to use kubectl plugins. Krew helps you discover plugins, install and manage them on your machine. It is similar to tools like apt, dnf or brew.</p>"}, {"location": "krew/#installation", "title": "Installation", "text": "<ol> <li> <p>Run this command to download and install krew:</p> <pre><code>(\nset -x; cd \"$(mktemp -d)\" &amp;&amp;\nOS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\nARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\nKREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\ncurl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\ntar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n./\"${KREW}\" install krew\n)\n</code></pre> </li> <li> <p>Add the <code>$HOME/.krew/bin</code> directory to your PATH environment variable. To do    this, update your <code>.bashrc</code> or <code>.zshrc</code> file and append the following line:</p> </li> </ol> <pre><code>export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n</code></pre> <ol> <li> <p>Restart your shell.</p> </li> <li> <p>Run <code>kubectl krew</code> to check the installation.</p> </li> </ol>"}, {"location": "krew/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "ksniff/", "title": "Ksniff", "text": "<p>Ksniff is a Kubectl plugin to ease sniffing on kubernetes pods using tcpdump and wireshark.</p>"}, {"location": "ksniff/#installation", "title": "Installation", "text": "<p>Recommended installation is done via krew</p> <pre><code>kubectl krew install sniff\n</code></pre> <p>For manual installation, download the latest release package, unzip it and use the attached makefile:</p> <pre><code>unzip ksniff.zip\nmake install\n</code></pre> <p>(I tried doing it manually and it failed for me).</p>"}, {"location": "ksniff/#usage", "title": "Usage", "text": "<pre><code>kubectl sniff &lt;POD_NAME&gt; [-n &lt;NAMESPACE_NAME&gt;] [-c &lt;CONTAINER_NAME&gt;] [-i &lt;INTERFACE_NAME&gt;] [-f &lt;CAPTURE_FILTER&gt;] [-o OUTPUT_FILE] [-l LOCAL_TCPDUMP_FILE] [-r REMOTE_TCPDUMP_FILE]\n\nPOD_NAME: Required. the name of the kubernetes pod to start capture it's traffic.\nNAMESPACE_NAME: Optional. Namespace name. used to specify the target namespace to operate on.\nCONTAINER_NAME: Optional. If omitted, the first container in the pod will be chosen.\nINTERFACE_NAME: Optional. Pod Interface to capture from. If omitted, all Pod interfaces will be captured.\nCAPTURE_FILTER: Optional. specify a specific tcpdump capture filter. If omitted no filter will be used.\nOUTPUT_FILE: Optional. if specified, ksniff will redirect tcpdump output to local file instead of wireshark. Use '-' for stdout.\nLOCAL_TCPDUMP_FILE: Optional. if specified, ksniff will use this path as the local path of the static tcpdump binary.\nREMOTE_TCPDUMP_FILE: Optional. if specified, ksniff will use the specified path as the remote path to upload static tcpdump to.\n</code></pre> <p>You'll need to remove the pods manually once you've finished analyzing the traffic.</p>"}, {"location": "ksniff/#issues", "title": "Issues", "text": ""}, {"location": "ksniff/#wtap_encap-0", "title": "<code>WTAP_ENCAP = 0</code>", "text": "<p>Upgrade your wireshark to a version greater or equal to <code>3.3.0</code>.</p>"}, {"location": "ksniff/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "kubernetes_debugging/", "title": "Kubernetes Debugging", "text": ""}, {"location": "kubernetes_debugging/#network-debugging", "title": "Network debugging", "text": "<p>Sometimes you need to monitor the network traffic that goes between pods to solve an issue. There are different ways to see it:</p> <ul> <li>Using Mizu</li> <li>Running tcpdump against a running container</li> <li>Using ksniff</li> <li>Using ephemeral debug containers</li> </ul> <p>Of all the solutions, the cleaner and easier is to use Mizu.</p>"}, {"location": "kubernetes_debugging/#running-tcpdump-against-a-running-container", "title": "Running tcpdump against a running container", "text": "<p>If the pod you want to analyze has root permissions (bad idea) you'll be able to install <code>tcpdump</code> (<code>apt-get install tcpdump</code>) and pipe it into <code>wireshark</code> on your local machine.</p> <pre><code>kubectl exec my-app-pod -- tcpdump -i eth0 -w - | wireshark -k -i -\n</code></pre> <p>There's some issues with this, though:</p> <ul> <li>You have to <code>kubectl exec</code> and install arbitrary software from the internet on     a running Pod. This is fine for internet-connected dev environments, but     probably not something you'd want to do (or be able to do) in production.</li> <li>If this app had been using a minimal <code>distroless</code> base image or was built with     a <code>buildpack</code> you won't be able to install <code>tcpdump</code>.</li> </ul>"}, {"location": "kubernetes_debugging/#using-ephemeral-debug-containers", "title": "Using ephemeral debug containers", "text": "<p>Kubernetes 1.16 has a new Ephemeral Containers feature that is perfect for our use case. With Ephemeral Containers, we can ask for a new temporary container with the image of our choosing to run inside an existing Pod. This means we can keep the main images for our applications lightweight and then bolt on a heavy image with all of our favorite debug tools when necessary.</p> <pre><code>kubectl debug -it pod-to-debug-id --image=nicolaka/netshoot --target=pod-to-debug -- tcpdump -i eth0 -w - | wireshark -k -i\n</code></pre> <p>Where <code>nicolaka/netshoot</code> is an optimized network troubleshooting docker.</p> <p>There's some issues with this too, for example, as of Kubernetes 1.21 Ephemeral containers are not enabled by default, so chances are you won't have access to them yet in your environment.</p>"}, {"location": "letsencrypt/", "title": "letsencrypt", "text": "<p>Letsencrypt is a free, automated, and open certificate authority brought to you by the nonprofit Internet Security Research Group (ISRG). Basically it gives away SSL certificates, which are required to configure webservers to use HTTPS instead of HTTP for example.</p>"}, {"location": "letsencrypt/#configure-a-wildcard-dns-when-the-provider-is-not-supported", "title": "Configure a wildcard dns when the provider is not supported", "text": "<p>If you\u2019d like to obtain a wildcard certificate from Let\u2019s Encrypt or run certbot on a machine other than your target webserver, you can use one of Certbot\u2019s DNS plugins.</p> <p>They support some DNS providers and a generic protocol if your DNS provider is not in the first list and it doesn't either support RFC 2136 you need to set the manual renewal of certificates. Keep in mind though that Let's Encrypt doesn't support HTTP validation for wildcard domains.</p> <p>To do so you first need to install certbot. Of the different ways to do it, the cleanest for this case is to use docker (given that you're already using it and don't mind shutting down your web application service so that let's encrypt docker can bind to ports 80 or 443). I'd prefer not to bring down the service for this purpose. Even if it is just once each 2 months, because I feel that the automation of this process will be more difficult in the end. The option we have left is to install it with <code>pip</code> but as we want a clean environment, it's better to use <code>pipx</code>.</p> <pre><code>pipx install certbot\n</code></pre>"}, {"location": "linux_snippets/", "title": "Linux snippets", "text": ""}, {"location": "linux_snippets/#get-class-of-a-window", "title": "Get class of a window", "text": "<p>Use <code>xprop</code> and click the window.</p>"}, {"location": "linux_snippets/#change-the-brightness-of-the-screen", "title": "Change the brightness of the screen", "text": "<p>Get the current brightness level with <code>cat /sys/class/backlight/intel_backlight/brightness</code>. Imagine it's <code>1550</code>, then if you want to lower the brightness use:</p> <pre><code>sudo echo 500 &gt; /sys/class/backlight/intel_backlight/brightness\n</code></pre>"}, {"location": "linux_snippets/#force-umount-nfs-mounted-directory", "title": "Force umount nfs mounted directory", "text": "<pre><code>umount -l path/to/mounted/dir\n</code></pre>"}, {"location": "linux_snippets/#configure-fstab-to-mount-nfs", "title": "Configure fstab to mount nfs", "text": "<p>NFS stands for \u2018Network File System\u2019. This mechanism allows Unix machines to share files and directories over the network. Using this feature, a Linux machine can mount a remote directory (residing in a NFS server machine) just like a local directory and can access files from it.</p> <p>An NFS share can be mounted on a machine by adding a line to the <code>/etc/fstab</code> file.</p> <p>The default syntax for <code>fstab</code> entry of NFS mounts is as follows.</p> <pre><code>Server:/path/to/export /local_mountpoint nfs &lt;options&gt; 0 0\n</code></pre> <p>Where:</p> <ul> <li><code>Server</code>: The hostname or IP address of the NFS server where the exported directory resides.</li> <li><code>/path/to/export</code>: The shared directory (exported folder) path.</li> <li><code>/local_mountpoint</code>: Existing directory in the host where you want to mount the NFS share.</li> </ul> <p>You can specify a number of options that you want to set on the NFS mount:</p> <ul> <li><code>soft/hard</code>: When the mount option <code>hard</code> is set, if the NFS server crashes or becomes unresponsive, the NFS requests will be retried indefinitely. You can set the mount option <code>intr</code>, so that the process can be interrupted. When the NFS server comes back online, the process can be continued from where it was while the server became unresponsive.</li> </ul> <p>When the option <code>soft</code> is set, the process will be reported an error when the NFS server is unresponsive after waiting for a period of time (defined by the <code>timeo</code> option). In certain cases <code>soft</code> option can cause data corruption and loss of data. So, it is recommended to use <code>hard</code> and <code>intr</code> options.</p> <ul> <li><code>noexec</code>: Prevents execution of binaries on mounted file systems. This is useful if the system is mounting a non-Linux file system via NFS containing incompatible binaries.</li> <li><code>nosuid</code>: Disables set-user-identifier or set-group-identifier bits. This prevents remote users from gaining higher privileges by running a setuid program.</li> <li><code>tcp</code>: Specifies the NFS mount to use the TCP protocol.</li> <li><code>udp</code>: Specifies the NFS mount to use the UDP protocol.</li> <li><code>nofail</code>: Prevent issues when rebooting the host. The downside is that if you have services that depend on the volume to be mounted they won't behave as expected.</li> </ul>"}, {"location": "linux_snippets/#fix-limit-on-the-number-of-inotify-watches", "title": "Fix limit on the number of inotify watches", "text": "<p>Programs that sync files such as dropbox, git etc use inotify to notice changes to the file system. The limit can be see by -</p> <pre><code>cat /proc/sys/fs/inotify/max_user_watches\n</code></pre> <p>For me, it shows <code>65536</code>. When this limit is not enough to monitor all files inside a directory it throws this error.</p> <p>If you want to increase the amount of inotify watchers, run the following in a terminal:</p> <pre><code>echo fs.inotify.max_user_watches=100000 | sudo tee -a /etc/sysctl.conf &amp;&amp; sudo sysctl -p\n</code></pre> <p>Where <code>100000</code> is the desired number of inotify watches.</p>"}, {"location": "linux_snippets/#what-is-varlogtallylog", "title": "What is <code>/var/log/tallylog</code>", "text": "<p><code>/var/log/tallylog</code> is the file where the <code>PAM</code> linux module (used for authentication of the machine) keeps track of the failed ssh logins in order to temporarily block users.</p>"}, {"location": "linux_snippets/#manage-users", "title": "Manage users", "text": "<ul> <li>Change main group of user</li> </ul> <pre><code>usermod -g {{ group_name }} {{ user_name }}\n</code></pre> <ul> <li>Add user to group</li> </ul> <pre><code>usermod -a -G {{ group_name }} {{ user_name }}\n</code></pre> <ul> <li>Remove user from group.</li> </ul> <pre><code>usermod -G {{ remaining_group_names }} {{ user_name }}\n</code></pre> <p>You have to execute <code>groups {{ user }}</code> get the list and pass the remaining to the above command</p> <ul> <li>Change uid and gid of the user</li> </ul> <pre><code>usermod -u {{ newuid }} {{ login }}\ngroupmod -g {{ newgid }} {{ group }}\nfind / -user {{ olduid }} -exec chown -h {{ newuid }} {} \\;\nfind / -group {{ oldgid }} -exec chgrp -h {{ newgid }} {} \\;\nusermod -g {{ newgid }} {{ login }}\n</code></pre>"}, {"location": "linux_snippets/#manage-ssh-keys", "title": "Manage ssh keys", "text": "<ul> <li>Generate ed25519 key</li> </ul> <pre><code>ssh-keygen -t ed25519 -f {{ path_to_keyfile }}\n</code></pre> <ul> <li>Generate RSA key</li> </ul> <pre><code>ssh-keygen -t rsa -b 4096 -o -a 100 -f {{ path_to_keyfile }}\n</code></pre> <ul> <li>Generate different comment</li> </ul> <pre><code>ssh-keygen -t ed25519 -f {{ path_to_keyfile }} -C {{ email }}\n</code></pre> <ul> <li>Generate key headless, batch</li> </ul> <pre><code>ssh-keygen -t ed25519 -f {{ path_to_keyfile }} -q -N \"\"\n</code></pre> <ul> <li>Generate public key from private key</li> </ul> <pre><code>ssh-keygen -y -f {{ path_to_keyfile }} &gt; {{ path_to_public_key_file }}\n</code></pre> <ul> <li>Get fingerprint of key   <pre><code>ssh-keygen -lf {{ path_to_key }}\n</code></pre></li> </ul>"}, {"location": "linux_snippets/#measure-the-network-performance-between-two-machines", "title": "Measure the network performance between two machines", "text": "<p>Install <code>iperf3</code> with <code>apt-get install iperf3</code> on both server and client.</p> <p>On the server system run:</p> <pre><code>server#: iperf3 -i 10 -s\n</code></pre> <p>Where:</p> <ul> <li><code>-i</code>: the interval to provide periodic bandwidth updates</li> <li><code>-s</code>: listen as a server</li> </ul> <p>On the client system:</p> <pre><code>client#: iperf3 -i 10 -w 1M -t 60 -c [server hostname or ip address]\n</code></pre> <p>Where:</p> <ul> <li><code>-i</code>: the interval to provide periodic bandwidth updates</li> <li><code>-w</code>: the socket buffer size (which affects the TCP Window). The buffer size is also set on the server by this client command.</li> <li><code>-t</code>: the time to run the test in seconds</li> <li><code>-c</code>: connect to a listening server at\u2026</li> </ul> <p>Sometimes is interesting to test both ways as they may return different outcomes</p> <p>I've got the next results at home:</p> <ul> <li>From new NAS to laptop through wifi 67.5 MB/s</li> <li>From laptop to new NAS 59.25 MB/s</li> <li>From intel Nuc to new NAS 116.75 MB/s (934Mbit/s)</li> <li>From old NAS to new NAS 11 MB/s</li> </ul>"}, {"location": "linux_snippets/#measure-the-performance-iops-of-a-disk", "title": "Measure the performance, IOPS of a disk", "text": "<p>To measure disk IOPS performance in Linux, you can use the <code>fio</code> tool. Install it with</p> <pre><code>apt-get install fio\n</code></pre> <p>Then you need to go to the directory where your disk is mounted. The test is done by performing read/write operations in this directory.</p> <p>To do a random read/write operation test an 8 GB file will be created. Then <code>fio</code> will read/write a 4KB block (a standard block size) with the 75/25% by the number of reads and writes operations and measure the performance. </p> <pre><code>fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randrw --rwmixread=75\n</code></pre> <p>I've run this test in different environments with awesome results:</p> <ul> <li> <p>New NAS server NVME:    <pre><code>read: IOPS=297k, BW=1159MiB/s (1215MB/s)(3070MiB/2649msec)\n bw (  MiB/s): min= 1096, max= 1197, per=99.80%, avg=1156.61, stdev=45.31, samples=5\n iops        : min=280708, max=306542, avg=296092.00, stdev=11598.11, samples=5\nwrite: IOPS=99.2k, BW=387MiB/s (406MB/s)(1026MiB/2649msec); 0 zone resets\n bw (  KiB/s): min=373600, max=408136, per=99.91%, avg=396248.00, stdev=15836.85, samples=5\n iops        : min=93400, max=102034, avg=99062.00, stdev=3959.21, samples=5\ncpu          : usr=15.67%, sys=67.18%, ctx=233314, majf=0, minf=8\n</code></pre></p> </li> <li> <p>New NAS server ZFS pool with RAIDZ:</p> </li> </ul> <pre><code>read: IOPS=271k, BW=1059MiB/s (1111MB/s)(3070MiB/2898msec)\n bw (  MiB/s): min=  490, max= 1205, per=98.05%, avg=1038.65, stdev=306.74, samples=5\n iops        : min=125672, max=308484, avg=265893.20, stdev=78526.52, samples=5\nwrite: IOPS=90.6k, BW=354MiB/s (371MB/s)(1026MiB/2898msec); 0 zone resets\n bw (  KiB/s): min=167168, max=411776, per=98.26%, avg=356236.80, stdev=105826.16, samples=5\n iops        : min=41792, max=102944, avg=89059.20, stdev=26456.54, samples=5\ncpu          : usr=12.84%, sys=63.20%, ctx=234345, majf=0, minf=6\n</code></pre> <ul> <li>Laptop NVME:</li> </ul> <pre><code>read: IOPS=36.8k, BW=144MiB/s (151MB/s)(3070MiB/21357msec)\n bw (  KiB/s): min=129368, max=160304, per=100.00%, avg=147315.43, stdev=6640.40, samples=42\n iops        : min=32342, max=40076, avg=36828.86, stdev=1660.10, samples=42\nwrite: IOPS=12.3k, BW=48.0MiB/s (50.4MB/s)(1026MiB/21357msec); 0 zone resets\n bw (  KiB/s): min=42952, max=53376, per=100.00%, avg=49241.33, stdev=2151.40, samples=42\n iops        : min=10738, max=13344, avg=12310.33, stdev=537.85, samples=42\ncpu          : usr=14.32%, sys=32.17%, ctx=356674, majf=0, minf=7\n</code></pre> <ul> <li>Laptop ZFS pool through NFS (running in parallel with other network processes):</li> </ul> <pre><code>read: IOPS=4917, BW=19.2MiB/s (20.1MB/s)(3070MiB/159812msec)\n bw (  KiB/s): min=16304, max=22368, per=100.00%, avg=19681.46, stdev=951.52, samples=319\n iops        : min= 4076, max= 5592, avg=4920.34, stdev=237.87, samples=319\nwrite: IOPS=1643, BW=6574KiB/s (6732kB/s)(1026MiB/159812msec); 0 zone resets\n bw (  KiB/s): min= 5288, max= 7560, per=100.00%, avg=6577.35, stdev=363.32, samples=319\n iops        : min= 1322, max= 1890, avg=1644.32, stdev=90.82, samples=319\ncpu          : usr=5.21%, sys=10.59%, ctx=175825, majf=0, minf=8\n</code></pre> <ul> <li> <p>Intel Nuc server disk SSD:   <pre><code>  read: IOPS=11.0k, BW=46.9MiB/s (49.1MB/s)(3070MiB/65525msec)\n bw (  KiB/s): min=  280, max=73504, per=100.00%, avg=48332.30, stdev=25165.49, samples=130\n iops        : min=   70, max=18376, avg=12083.04, stdev=6291.41, samples=130\nwrite: IOPS=4008, BW=15.7MiB/s (16.4MB/s)(1026MiB/65525msec); 0 zone resets\n bw (  KiB/s): min=   55, max=24176, per=100.00%, avg=16153.84, stdev=8405.53, samples=130\n iops        : min=   13, max= 6044, avg=4038.40, stdev=2101.42, samples=130\ncpu          : usr=8.04%, sys=25.87%, ctx=268055, majf=0, minf=8\n</code></pre></p> </li> <li> <p>Intel Nuc server external HD usb disk :   <pre><code>\n</code></pre></p> </li> <li> <p>Intel Nuc ZFS pool through NFS (running in parallel with other network processes):</p> </li> </ul> <pre><code>  read: IOPS=18.7k, BW=73.2MiB/s (76.8MB/s)(3070MiB/41929msec)\n bw (  KiB/s): min=43008, max=103504, per=99.80%, avg=74822.75, stdev=16708.40, samples=83\n iops        : min=10752, max=25876, avg=18705.65, stdev=4177.11, samples=83\nwrite: IOPS=6264, BW=24.5MiB/s (25.7MB/s)(1026MiB/41929msec); 0 zone resets\n bw (  KiB/s): min=14312, max=35216, per=99.79%, avg=25003.55, stdev=5585.54, samples=83\n iops        : min= 3578, max= 8804, avg=6250.88, stdev=1396.40, samples=83\ncpu          : usr=6.29%, sys=13.21%, ctx=575927, majf=0, minf=10\n</code></pre> <ul> <li>Old NAS with RAID5:   <pre><code>read : io=785812KB, bw=405434B/s, iops=98, runt=1984714msec\nwrite: io=262764KB, bw=135571B/s, iops=33, runt=1984714msec\ncpu          : usr=0.16%, sys=0.59%, ctx=212447, majf=0, minf=8\n</code></pre></li> </ul> <p>Conclusions:</p> <ul> <li>New NVME are super fast (1215MB/s read, 406MB/s write)</li> <li>ZFS rocks, with a RAIDZ1, L2ARC and ZLOG it returned almost the same performance as the NVME ( 1111MB/s read, 371MB/s write)</li> <li>Old NAS with RAID is super slow (0.4KB/s read, 0.1KB/s write!)</li> <li>I should replace the laptop's NVME, the NAS one has 10x performace both on read and write.</li> </ul> <p>There is a huge difference between ZFS in local and through NFS. In local you get (1111MB/s read and 371MB/s write) while through NFS I got (20.1MB/s read and 6.7MB/s write). I've measured the network performance between both machines with <code>iperf3</code> and got:</p> <ul> <li>From NAS to laptop 67.5 MB/s</li> <li>From laptop to NAS 59.25 MB/s</li> </ul> <p>It was because I was running it over wifi.</p> <p>From the Intel nuc to the new server I get 76MB/s read and 25.7MB/s write. Still a huge difference though against the local transfer. The network speed measured with <code>iperf3</code> are 116 MB/s.</p>"}, {"location": "linux_snippets/#use-a-pass-password-in-a-makefile", "title": "Use a <code>pass</code> password in a Makefile", "text": "<pre><code>TOKEN ?= $(shell bash -c '/usr/bin/pass show path/to/token')\n\ndiff:\n@AUTHENTIK_TOKEN=$(TOKEN) terraform plan\n</code></pre>"}, {"location": "linux_snippets/#install-a-new-font", "title": "Install a new font", "text": "<p>Install a font manually by downloading the appropriate <code>.ttf</code> or <code>otf</code> files and placing them into <code>/usr/local/share/fonts</code> (system-wide), <code>~/.local/share/fonts</code> (user-specific) or <code>~/.fonts</code> (user-specific). These files should have the permission 644 (<code>-rw-r--r--</code>), otherwise they may not be usable.</p>"}, {"location": "linux_snippets/#get-vpn-password-from-pass", "title": "Get VPN password from <code>pass</code>", "text": "<p>To be able to retrieve the user and password from pass you need to run the openvpn command with the next flags:</p> <pre><code>sudo bash -c \"openvpn --config config.ovpn  --auth-user-pass &lt;(echo -e 'user_name\\n$(pass show vpn)')\"\n</code></pre> <p>Assuming that <code>vpn</code> is an entry of your <code>pass</code> password store.</p>"}, {"location": "linux_snippets/#download-ts-streams", "title": "Download TS streams", "text": "<p>Some sites give stream content with small <code>.ts</code> files that you can't download directly. Instead open the developer tools, reload the page and search for a request with extension <code>.m3u8</code>, that gives you the playlist of all the chunks of <code>.ts</code> files. Once you have that url you can use <code>yt-dlp</code> to download it.</p>"}, {"location": "linux_snippets/#df-and-du-showing-different-results", "title": "df and du showing different results", "text": "<p>Sometimes on a linux machine you will notice that both <code>df</code> command (display free disk space) and <code>du</code> command (display disk usage statistics) report different output. Usually, <code>df</code> will output a bigger disk usage than <code>du</code>.</p> <p>The <code>du</code> command estimates file space usage, and the <code>df</code> command shows file system disk space usage.</p> <p>There are many reasons why this could be happening:</p>"}, {"location": "linux_snippets/#disk-mounted-over-data", "title": "Disk mounted over data", "text": "<p>If you mount a disk on a directory that already holds data, then when you run <code>du</code> that data won't show, but <code>df</code> knows it's there.</p> <p>To troubleshoot this, umount one by one of your disks, and do an <code>ls</code> to see if there's any remaining data in the mount point.</p>"}, {"location": "linux_snippets/#used-deleted-files", "title": "Used deleted files", "text": "<p>When a file is deleted under Unix/Linux, the disk space occupied by the file will not be released immediately in some cases. The result of the command <code>du</code> doesn\u2019t include the size of the deleting file. But the impact of the command <code>df</code> for the deleting file\u2019s size due to its disk space is not released immediately. Hence, after deleting the file, the results of <code>df</code> and <code>du</code> are different until the disk space is freed.</p> <p>Open file descriptor is main causes of such wrong information. For example, if a file called <code>/tmp/application.log</code> is open by a third-party application OR by a user and the same file is deleted, both <code>df</code> and <code>du</code> report different outputs. You can use the <code>lsof</code> command to verify this:</p> <pre><code>lsof | grep tmp\n</code></pre> <p>To fix it:</p> <ul> <li>Use the <code>lsof</code> command as discussed above to find a deleted file opened by   other users and apps. See how to list all users in the system for more info.</li> <li>Then, close those apps and log out of those Linux and Unix users.</li> <li>As a sysadmin you restart any process or <code>kill</code> the process under Linux and   Unix that did not release the deleted file.</li> <li>Flush the filesystem using the <code>sync</code> command that synchronizes cached writes   to persistent disk storage.</li> <li>If everything else fails, try restarting the system using the <code>reboot</code> command   or <code>shutdown</code> command.</li> </ul>"}, {"location": "linux_snippets/#scan-a-physical-page-in-linux", "title": "Scan a physical page in Linux", "text": "<p>Install <code>xsane</code> and run it.</p>"}, {"location": "linux_snippets/#git-checkout-to-main-with-master-as-a-fallback", "title": "Git checkout to main with master as a fallback", "text": "<p>I usually use the alias <code>gcm</code> to change to the main branch of the repository, given the change from main to master now I have some repos that use one or the other, but I still want <code>gcm</code> to go to the correct one. The solution is to use:</p> <pre><code>alias gcm='git checkout \"$(git symbolic-ref refs/remotes/origin/HEAD | cut -d'/' -f4)\"'\n</code></pre>"}, {"location": "linux_snippets/#create-qr-code", "title": "Create QR code", "text": "<pre><code>qrencode -o qrcode.png 'Hello World!'\n</code></pre>"}, {"location": "linux_snippets/#trim-silences-of-sound-files", "title": "Trim silences of sound files", "text": "<p>To trim all silence longer than 2 seconds down to only 2 seconds long.</p> <pre><code>sox in.wav out6.wav silence -l 1 0.1 1% -1 2.0 1%\n</code></pre> <p>Note that SoX does nothing to bits of silence shorter than 2 seconds.</p> <p>If you encounter the <code>sox FAIL formats: no handler for file extension 'mp3'</code> error you'll need to install the <code>libsox-fmt-all</code> package.</p>"}, {"location": "linux_snippets/#adjust-the-replay-gain-of-many-sound-files", "title": "Adjust the replay gain of many sound files", "text": "<pre><code>sudo apt-get install python-rgain\nreplaygain -f *.mp3\n</code></pre>"}, {"location": "linux_snippets/#check-vulnerabilities-in-nodejs-applications", "title": "Check vulnerabilities in Node.js applications", "text": "<p>With <code>yarn audit</code> you'll see the vulnerabilities, with <code>yarn outdated</code> you can see the packages that you need to update.</p>"}, {"location": "linux_snippets/#check-vulnerabilities-in-rails-dependencies", "title": "Check vulnerabilities in rails dependencies", "text": "<pre><code>gem install bundler-audit\ncd project_with_gem_lock\nbundler-audit\n</code></pre>"}, {"location": "linux_snippets/#create-basic-auth-header", "title": "Create Basic Auth header", "text": "<pre><code>$ echo -n user:password | base64\ndXNlcjpwYXNzd29yZA==\n</code></pre> <p>Without the <code>-n</code> it won't work well.</p>"}, {"location": "linux_snippets/#install-one-package-from-debian-unstable", "title": "Install one package from Debian unstable", "text": "<ul> <li>Add the <code>unstable</code> repository to your <code>/etc/apt/sources.list</code></li> </ul> <pre><code># Unstable\ndeb http://deb.debian.org/debian/ unstable main contrib non-free\ndeb-src http://deb.debian.org/debian/ unstable main contrib non-free\n</code></pre> <ul> <li>Configure <code>apt</code> to only use <code>unstable</code> when specified</li> </ul> <p>!!! note \"File: <code>/etc/apt/preferences</code>\" ``` Package: * Pin: release a=stable Pin-Priority: 700</p> <pre><code>Package: *\nPin: release  a=testing\nPin-Priority: 600\n\nPackage: *\nPin: release a=unstable\nPin-Priority: 100\n```\n</code></pre> <ul> <li>Update the package data with <code>apt-get update</code>.</li> <li>See that the new versions are available with   <code>apt-cache policy   &lt;package_name&gt;</code></li> <li>To install a package from unstable you can run   <code>apt-get install -t unstable   &lt;package_name&gt;</code>.</li> </ul>"}, {"location": "linux_snippets/#fix-the-following-packages-have-been-kept-back", "title": "Fix the following packages have been kept back", "text": "<pre><code>sudo apt-get --with-new-pkgs upgrade\n</code></pre>"}, {"location": "linux_snippets/#monitor-outgoing-traffic", "title": "Monitor outgoing traffic", "text": ""}, {"location": "linux_snippets/#easy-and-quick-way-watch-lsof", "title": "Easy and quick way watch &amp; lsof", "text": "<p>You can simply use a combination of <code>watch</code> &amp; <code>lsof</code> command in Linux to get an idea of outgoing traffic on specific ports. Here is an example of outgoing traffic on ports <code>80</code> and <code>443</code>.</p> <pre><code>$ watch -n1 lsof -i TCP:80,443\n</code></pre> <p>Here is a sample output.</p> <pre><code>dropbox    2280 saml   23u  IPv4 56015285      0t0  TCP www.example.local:56003-&gt;snt-re3-6c.sjc.dropbox.com:http (ESTABLISHED)\nthunderbi  2306 saml   60u  IPv4 56093767      0t0  TCP www.example.local:34788-&gt;ord08s09-in-f20.1e100.net:https (ESTABLISHED)\nmono       2322 saml   15u  IPv4 56012349      0t0  TCP www.example.local:54018-&gt;204-62-14-135.static.6sync.net:https (ESTABLISHED)\nchrome    4068 saml  175u  IPv4 56021419      0t0  TCP www.example.local:42182-&gt;stackoverflow.com:http (ESTABLISHED)\n</code></pre> <p>You'll miss the short lived connections though.</p>"}, {"location": "linux_snippets/#fine-grained-with-tcpdump", "title": "Fine grained with tcpdump", "text": "<p>You can also use <code>tcpdump</code> command to capture all raw packets, on all interfaces, on all ports, and write them to file.</p> <pre><code>sudo tcpdump -tttt -i any -w /tmp/http.log\n</code></pre> <p>Or you can limit it to a specific port adding the arguments <code>port 443 or 80</code>. The <code>-tttt</code> flag is used to capture the packets with a human readable timestamp.</p> <p>To read the recorded information, run the <code>tcpdump</code> command with <code>-A</code> option. It will print ASCII text in recorded packets, that you can browse using page up/down keys.</p> <pre><code>tcpdump -A -r /tmp/http.log | less\n</code></pre> <p>However, <code>tcpdump</code> cannot decrypt information, so you cannot view information about HTTPS requests in it.</p>"}, {"location": "linux_snippets/#clean-up-system-space", "title": "Clean up system space", "text": ""}, {"location": "linux_snippets/#clean-package-data", "title": "Clean package data", "text": "<p>There is a couple of things to do when we want to free space in a no-brainer way. First, we want to remove those deb packages that get cached every time we do <code>apt-get install</code>.</p> <pre><code>apt-get clean\n</code></pre> <p>Also, the system might keep packages that were downloaded as dependencies but are not needed anymore. We can get rid of them with</p> <pre><code>apt-get autoremove\n</code></pre> <p>Remove data of unpurged packages.</p> <pre><code>sudo apt-get purge $(dpkg -l | grep '^rc' | awk '{print $2}')\n</code></pre> <p>If we want things tidy, we must know that whenever we <code>apt-get remove</code> a package, the configuration will be kept in case we want to install it again. In most cases we want to use <code>apt-get purge</code>. To clean those configurations from removed packages, we can use</p> <pre><code>dpkg --list | grep \"^rc\" | cut -d \" \" -f 3 | xargs --no-run-if-empty sudo dpkg --purge\n</code></pre> <p>So far we have not uninstalled anything. If now we want to inspect what packages are consuming the most space, we can type</p> <pre><code>dpkg-query -Wf '${Installed-Size}\\t${Package}\\n' | sort -n\n</code></pre>"}, {"location": "linux_snippets/#clean-snap-data", "title": "Clean snap data", "text": "<p>If you're using <code>snap</code> you can clean space by:</p> <ul> <li> <p>Reduce the number of versions kept of a package with   <code>snap set system refresh.retain=2</code></p> </li> <li> <p>Remove the old versions with <code>clean_snap.sh</code></p> </li> </ul> <pre><code>#!/bin/bash\n#Removes old revisions of snaps\n#CLOSE ALL SNAPS BEFORE RUNNING THIS\nset -eu\nLANG=en_US.UTF-8 snap list --all | awk '/disabled/{print $1, $3}' |\nwhile read snapname revision; do\nsnap remove \"$snapname\" --revision=\"$revision\"\ndone\n</code></pre>"}, {"location": "linux_snippets/#clean-journalctl-data", "title": "Clean journalctl data", "text": "<ul> <li>Check how much space it's using: <code>journalctl --disk-usage</code></li> <li>Rotate the logs: <code>journalctl --rotate</code></li> </ul> <p>Then you have three ways to reduce the data:</p> <ol> <li>Clear journal log older than X days: <code>journalctl --vacuum-time=2d</code></li> <li>Restrict logs to a certain size: <code>journalctl --vacuum-size=100M</code></li> <li>Restrict number of log files: <code>journactl --vacuum-files=5</code>.</li> </ol> <p>The operations above will affect the logs you have right now, but it won't solve the problem in the future. To let <code>journalctl</code> know the space you want to use open the <code>/etc/systemd/journald.conf</code> file and set the <code>SystemMaxUse</code> to the amount you want (for example <code>1000M</code> for a gigabyte). Once edited restart the service with <code>sudo systemctl restart systemd-journald</code>.</p>"}, {"location": "linux_snippets/#clean-up-docker-data", "title": "Clean up docker data", "text": "<p>To remove unused <code>docker</code> data you can run <code>docker system prune -a</code>. This will remove:</p> <ul> <li>All stopped containers</li> <li>All networks not used by at least one container</li> <li>All images without at least one container associated to them</li> <li>All build cache</li> </ul> <p>Sometimes that's not enough, and your <code>/var/lib/docker</code> directory still weights more than it should. In those cases:</p> <ul> <li>Stop the <code>docker</code> service.</li> <li>Remove or move the data to another directory</li> <li>Start the <code>docker</code> service.</li> </ul> <p>In order not to loose your persisted data, you need to configure your dockers to mount the data from a directory that's not within <code>/var/lib/docker</code>.</p>"}, {"location": "linux_snippets/#set-up-docker-logs-rotation", "title": "Set up docker logs rotation", "text": "<p>By default, the stdout and stderr of the container are written in a JSON file located in <code>/var/lib/docker/containers/[container-id]/[container-id]-json.log</code>. If you leave it unattended, it can take up a large amount of disk space.</p> <p>If this JSON log file takes up a significant amount of the disk, we can purge it using the next command.</p> <pre><code>truncate -s 0 &lt;logfile&gt;\n</code></pre> <p>We could setup a cronjob to purge these JSON log files regularly. But for the long term, it would be better to setup log rotation. This can be done by adding the following values in <code>/etc/docker/daemon.json</code>.</p> <pre><code>{\n\"log-driver\": \"json-file\",\n\"log-opts\": {\n\"max-size\": \"10m\",\n\"max-file\": \"10\"\n}\n}\n</code></pre>"}, {"location": "linux_snippets/#clean-old-kernels", "title": "Clean old kernels", "text": "<p>!!! warning \"I don't recommend using this step, rely on <code>apt-get autoremove</code>, it' safer\"</p> <p>The full command is</p> <pre><code>dpkg -l linux-* | awk '/^ii/{ print $2}' | grep -v -e `uname -r | cut -f1,2 -d\"-\"` | grep -e [0-9] | grep -E \"(image|headers)\" | xargs sudo apt-get -y purge\n</code></pre> <p>To test what packages will it remove use:</p> <pre><code>dpkg -l linux-* | awk '/^ii/{ print $2}' | grep -v -e `uname -r | cut -f1,2 -d\"-\"` | grep -e [0-9] | grep -e \"(image|headers)\" | xargs sudo apt-get --dry-run remove\n</code></pre> <p>Remember that your running kernel can be obtained by <code>uname -r</code>.</p>"}, {"location": "linux_snippets/#replace-a-string-with-sed-recursively", "title": "Replace a string with sed recursively", "text": "<pre><code>find . -type f -exec sed -i 's/foo/bar/g' {} +\n</code></pre>"}, {"location": "linux_snippets/#bypass-client-ssl-certificate-with-cli-tool", "title": "Bypass client SSL certificate with cli tool", "text": "<p>Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature.</p> <p>To solve it, we can use a transparent proxy that does the exchange for us.</p> <ul> <li>Export your certificate: If you have a <code>p12</code> certificate, you first need to   extract the key, crt and the ca from the certificate into the <code>site.pem</code>.</li> </ul> <pre><code>openssl pkcs12 -in certificate.p12 -out site.key.pem -nocerts -nodes # It asks for the p12 password\nopenssl pkcs12 -in certificate.p12 -out site.crt.pem -clcerts -nokeys\nopenssl pkcs12 -cacerts -nokeys -in certificate.p12 -out site-ca-cert.ca\n\ncat site.key.pem site.crt.pem site-ca-cert.ca &gt; site.pem\n</code></pre> <ul> <li>Build the proxy ca: Then we merge the site and the client ca's into the   <code>site-ca-file.cert</code> file:</li> </ul> <pre><code>openssl s_client -connect www.site.org:443 2&gt;/dev/null  | openssl x509 -text &gt; site-ca-file.cert\ncat site-ca-cert.ca &gt;&gt; web-ca-file.cert\n</code></pre> <ul> <li>Change your hosts file to redirect all requests to the proxy.</li> </ul> <pre><code># vim /etc/hosts\n[...]\n0.0.0.0 www.site.org\n</code></pre> <ul> <li>Run the proxy</li> </ul> <pre><code>docker run --rm \\\n-v $(pwd):/certs/ \\\n-p 3001:3001 \\\n-it ghostunnel/ghostunnel \\\nclient \\\n--listen 0.0.0.0:3001 \\\n--target www.site.org:443 \\\n--keystore /certs/site.pem \\\n--cacert /certs/site-ca-file.cert \\\n--unsafe-listen\n</code></pre> <ul> <li>Run the command line tool using the http protocol on the port 3001:</li> </ul> <pre><code>wpscan  --url http://www.site.org:3001/ --disable-tls-checks\n</code></pre> <p>Remember to clean up your env afterwards.</p>"}, {"location": "linux_snippets/#allocate-space-for-a-virtual-filesystem", "title": "Allocate space for a virtual filesystem", "text": "<p>Also useful to simulate big files</p> <pre><code>fallocate -l 20G /path/to/file\n</code></pre>"}, {"location": "linux_snippets/#identify-what-a-string-or-file-contains", "title": "Identify what a string or file contains", "text": "<p>Identify anything. <code>pyWhat</code> easily lets you identify emails, IP addresses, and more. Feed it a .pcap file or some text and it'll tell you what it is.</p>"}, {"location": "linux_snippets/#split-a-file-into-many-with-equal-number-of-lines", "title": "Split a file into many with equal number of lines", "text": "<p>You could do something like this:</p> <pre><code>split -l 200000 filename\n</code></pre> <p>Which will create files each with 200000 lines named <code>xaa</code>, <code>xab</code>, <code>xac</code>, ...</p>"}, {"location": "linux_snippets/#check-if-an-rsync-command-has-gone-well", "title": "Check if an rsync command has gone well", "text": "<p>Sometimes after you do an <code>rsync</code> between two directories of different devices (an usb and your hard drive for example), the sizes of the directories don't match. I've seen a difference of a 30% less on the destination. <code>du</code>, <code>ncdu</code> and <code>and</code> have a long story of reporting wrong sizes with advanced filesystems (ZFS, VxFS or compressing filesystems), these do a lot of things to reduce the disk usage (deduplication, compression, extents, files with holes...) which may lead to the difference in space.</p> <p>To check if everything went alright run <code>diff -r --brief source/ dest/</code>, and check that there is no output.</p>"}, {"location": "linux_snippets/#list-all-process-swap-usage", "title": "List all process swap usage", "text": "<pre><code>for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file; done | sort -k 2 -n -r | less\n</code></pre>"}, {"location": "mdformat/", "title": "MDFormat", "text": "<p>MDFormat is an opinionated Markdown formatter that can be used to enforce a consistent style in Markdown files. Mdformat is a Unix-style command-line tool as well as a Python library.</p> <p>The features/opinions of the formatter include:</p> <ul> <li>Consistent indentation and whitespace across the board</li> <li>Always use ATX style headings</li> <li>Move all link references to the bottom of the document (sorted by label)</li> <li>Reformat indented code blocks as fenced code blocks</li> <li>Use 1. as the ordered list marker if possible, also for noninitial list items.</li> </ul> <p>It's based on the <code>markdown-it-py</code> Markdown parser, which is a Python implementation of <code>markdown-it</code>.</p>"}, {"location": "mdformat/#installation", "title": "Installation", "text": "<p>By default it uses CommonMark support:</p> <pre><code>pip install mdformat\n</code></pre> <p>This won't support task lists, if you want them use the github flavoured parser instead:</p> <pre><code>pip install mdformat-gfm\n</code></pre> <p>You may want to also install some interesting plugins:</p> <ul> <li><code>mdformat-beautysh</code>: format   <code>bash</code> and <code>sh</code> code blocks.</li> <li><code>mdformat-black</code>: format <code>python</code>   code blocks.</li> <li><code>mdformat-config</code>: format <code>json</code>,   <code>toml</code> and <code>yaml</code> code blocks.</li> <li><code>mdformat-web</code>: format <code>javascript</code>,   <code>css</code>, <code>html</code> and <code>xml</code> code blocks.</li> <li><code>mdformat-tables</code>: Adds   support for Github Flavored Markdown style tables.</li> <li><code>mdformat-frontmatter</code>:   Adds support for the yaml header with metadata of the file.</li> </ul> <p>To install them with <code>pipx</code> you can run:</p> <pre><code>pipx install --include-deps mdformat-gfm\npipx inject mdformat-gfm mdformat-beautysh mdformat-black mdformat-config \\\nmdformat-web mdformat-tables mdformat-frontmatter\n</code></pre>"}, {"location": "mdformat/#desires", "title": "Desires", "text": "<p>These are the functionalities I miss when writing markdown that can be currently fixed with <code>mdformat</code>:</p> <ul> <li>Long lines are wrapped.</li> <li>Long lines in lists are wrapped and the indentation is respected.</li> <li>Add correct blank lines between sections.</li> </ul> <p>I haven't found yet a way to achieve:</p> <ul> <li>Links are sent to the bottom of the document.</li> <li>Do   typographic replacements</li> <li>End paragraphs with a dot.</li> </ul>"}, {"location": "mdformat/#developing-mdformat-plugins", "title": "Developing mdformat plugins", "text": "<p>There are two kinds of plugins:</p> <ul> <li>Formatters: They change the output of the text. For example   <code>mdformatormat-black</code>.</li> <li>Parsers: They are extensions to the base CommonMark parser.</li> </ul> <p>You can see some plugin examples here.</p>"}, {"location": "mdformat/#issues", "title": "Issues", "text": "<ul> <li>It doesn't yet   support admonitions</li> <li>You can't   ignore some files,   nor   some part of the file</li> </ul>"}, {"location": "mdformat/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> </ul>"}, {"location": "mermaidjs/", "title": "MermaidJS", "text": "<p>MermaidJS is a Javascript library that lets you create diagrams using text and code.</p> <p>It can render the next diagram types:</p> <ul> <li>Flowchart</li> <li>Sequence.</li> <li>Gantt</li> <li>Class</li> <li>Git graph</li> <li>Entity Relationship</li> <li>User journey</li> </ul>"}, {"location": "mermaidjs/#installation", "title": "Installation", "text": "<p>Installing it requires node, I've only used it in mkdocs, which is easier to install and use.</p>"}, {"location": "mermaidjs/#usage", "title": "Usage", "text": ""}, {"location": "mermaidjs/#flowchart", "title": "Flowchart", "text": "<p>It can have two orientations top to bottom (<code>TB</code>) or left to right (<code>LR</code>).</p> <pre><code>graph TD\n    Start --&gt; Stop\n</code></pre> <p>By default the text shown is the same as the id, if you need a big text it's recommended to use the <code>id1[This is the text in the box]</code> syntax so it's easy to reference the node in the relationships.</p> <p>To link nodes, use <code>--&gt;</code> or <code>---</code>. If you cant to add text to the link use <code>A-- text --&gt;B</code></p>"}, {"location": "mermaidjs/#adding-links", "title": "Adding links", "text": "<p>You can add <code>click</code> events to the diagrams:</p> <pre><code>graph LR;\n    A--&gt;B;\n    B--&gt;C;\n    C--&gt;D;\n    click A callback \"Tooltip for a callback\"\n    click B \"http://www.github.com\" \"This is a tooltip for a link\"\n    click A call callback() \"Tooltip for a callback\"\n    click B href \"http://www.github.com\" \"This is a tooltip for a link\"\n</code></pre> <p>By default the links are opened in the same browser tab/window. It is possible to change this by adding a link target to the click definition (<code>_self</code>, <code>_blank</code>, <code>_parent</code>, or <code>_top</code>).</p> <pre><code>graph LR;\n    A--&gt;B;\n    B--&gt;C;\n    C--&gt;D;\n    D--&gt;E;\n    click A \"http://www.github.com\" _blank\n</code></pre>"}, {"location": "mermaidjs/#node-styling", "title": "Node styling", "text": "<p>You can define the style for each node with:</p> <pre><code>graph LR\n    id1(Start)--&gt;id2(Stop)\n    style id1 fill:#f9f,stroke:#333,stroke-width:4px\n    style id2 fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5\n</code></pre> <p>Or if you're going to use the same style for multiple nodes, you can define classes:</p> <pre><code>graph LR\n    A:::someclass --&gt; B\n    classDef someclass fill:#f96;\n</code></pre>"}, {"location": "mermaidjs/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "mizu/", "title": "Mizu", "text": "<p>Mizu is an API Traffic Viewer for Kubernetes, think <code>TCPDump</code> and Chrome Dev Tools combined.</p>"}, {"location": "mizu/#installation", "title": "Installation", "text": "<pre><code>curl -Lo mizu \\\nhttps://github.com/up9inc/mizu/releases/latest/download/mizu_linux_amd64 \\\n&amp;&amp; chmod 755 mizu\n</code></pre>"}, {"location": "mizu/#usage", "title": "Usage", "text": "<p>At the core of Mizu functionality is the pod tap</p> <pre><code>mizu tap &lt;podname&gt;\n</code></pre> <p>To view traffic of several pods, identified by a regular expression:</p> <pre><code>mizu tap \"(catalo*|front-end*)\"\n</code></pre> <p>After tapping your pods, Mizu will tell you that \"Web interface is now available at <code>https://localhost:8899/</code>. Visit the link from Mizu to view traffic in the Mizu UI.</p>"}, {"location": "mizu/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Docs</li> </ul>"}, {"location": "monitoring_comparison/", "title": "Monitoring Comparison", "text": "<p>As with any technology, when you want to adopt it, you first need to analyze your options. In this article we're going to compare the two most popular solutions at the moment, Nagios and Prometheus. Zabbix is similar in architecture and features to Nagios, so for the first iteration we're going to skip it.</p> <p>TL;DR: Prometheus is better, but it needs more effort.</p> <p>Nagios is suitable for basic monitoring of small and/or static systems where blackbox probing is sufficient.</p> <p>If you want to do whitebox monitoring, or have a dynamic or cloud based environment, then Prometheus is a good choice.</p>"}, {"location": "monitoring_comparison/#nagios", "title": "Nagios", "text": "<p>Nagios is an industry leader in IT infrastructure monitoring. It has four different products to choose from:</p> <ul> <li> <p>Nagios XI: Is an enterprise-ready server and network monitoring system that     supplies data to track app or network infrastructure health, performance,     availability, of the components, protocols, and services. It has     a user-friendly interface that allows UI configuration, customized     visualizations, and alert preferences.</p> </li> <li> <p>Nagios Log Server: It's used for log management and analysis of user     scenarios. It has the ability to correlate logged events across different     services and servers in real time, which helps with the investigation of     incidents and the performance of root cause analysis.</p> <p>Because Nagios Log Server\u2019s design is specifically for network security and audits, it lets users generate alerts for suspicious operations and commands. Log Server retains historical data from all events, supplying organizations with everything they need to pass a security audit.</p> </li> <li> <p>Nagios Network Analyzer: It's a tool for collecting and displaying either     metrics or extra information about an application network. It identifies     which IPs are communicating with the application servers and what requests     they\u2019re sending. The Network Analyzer maintains a record of all server     traffic, including who connected a specific server, to a specific port and     the specific request.</p> <p>This helps plan out server and network capacity, plus understand various kinds of security breaches likes unauthorized access, data leaks, DDoS, and viruses or malwares on servers.</p> </li> <li> <p>Nagios Fusion: is a compilation of the three tools Nagios offers. It provides     a complete solution that assists businesses in satisfying any and all of     their monitoring requirements. Its design is for scalability and for     visibility of the application and all of its dependencies.</p> </li> </ul>"}, {"location": "monitoring_comparison/#prometheus", "title": "Prometheus", "text": "<p>Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy.</p> <p>At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets.</p> <p>The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications.</p> <p>There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented.</p>"}, {"location": "monitoring_comparison/#comparison", "title": "Comparison", "text": "<p>For each dimension we'll check how each solution meets the criteria. An aggregation of all the results can be found in the summary.</p>"}, {"location": "monitoring_comparison/#open-source", "title": "Open source", "text": "<p>Only the Nagios Core is open sourced, it provides basic monitoring but it's enhanced by community contributions. It's also the base of the rest solutions, which are proprietary.</p> <p>Prometheus is completely open source under the Apache 2.0 license.</p>"}, {"location": "monitoring_comparison/#community", "title": "Community", "text": "<p>In Nagios, only the Nagios Core is an open-source tool. The rest are proprietary, so there is no community behind them.</p> <p>Community contributions to Nagios are gathered in the Nagios Exchange, it's hard to get other activity statistics than the overall number of contributions, but there are more than 850 addons, 4.5k plugins and 300 documentation contributions.</p> <p>Overall metrics (2021-02-22):</p> Metric Nagios Core Prometheus Stars 932 35.4k Forks 341 5.7k Watch 121 1.2k Commits 3.4k 8.5k Open Issues 195 290 Closed Issues 455 3.5k Open PR 9 116 Closed PR 155 4.5k <p>Last month metrics (2021-02-22):</p> Metric Nagios Core Prometheus Active PR 1 80 Active Issues 3 64 Commits 0 74 Authors 0 35 <p>We can see that Prometheus in comparison with Nagios Core is:</p> <ul> <li>More popular in terms of community contributions.</li> <li>More maintained.</li> <li>Growing more.</li> <li>Development is more distributed.</li> <li>Manages the issues collaboratively.</li> </ul> <p>This comparison is biased though, because Nagios comes from a time where GitHub and Git (and Youtube!) did not exist, and the communities formed around different sites.</p> <p>Also, given that Nagios has almost 20 years of existence, and that it forked from a previous monitoring project (NetSaint), the low number contributions indicate a stable and mature product, whereas the high numbers for Prometheus are indicators of a young, still in development product.</p> <p>Keep in mind that this comparison only analyzes the core, it doesn't take into account the metrics of the community contributions, as it is not easy to aggregate their statistics.</p> <p>Which makes Prometheus one of the biggest open-source projects in existence. It actually has hundreds of contributors maintaining it. The tool continues to be up-to-date to contemporary and popular apps, extending its list of exporters and responding to requests.</p> <p>On 16 January 2014, Nagios Enterprises redirected the nagios-plugins.org domain to a web server controlled by Nagios Enterprises without explicitly notifying the Nagios Plugins community team the consequences of their actions. Nagios Enterprises replaced the nagios-plugins team with a group of new, different members. The community team members who were replaced continued their work under the name Monitoring Plugins along with a new website with the new domain of monitoring-plugins.org. Which is a nasty move against the community.</p>"}, {"location": "monitoring_comparison/#configuration-and-usage", "title": "Configuration and usage", "text": "<p>Neither solution is easy to configure, you need to invest time in them.</p> <p>Nagios is easier to use for non technical users though.</p>"}, {"location": "monitoring_comparison/#visualizations", "title": "Visualizations", "text": "<p>The graphs and dashboards Prometheus provides don't meet today's needs. As a result, users resort to other visualization tools to display metrics collected by Prometheus, often Grafana.</p> <p>Nagios comes with a set of dashboards that fit the requirements of monitoring networks and infrastructure components. Yet, it still lacks graphs for more applicative-related issues.</p> <p>Personally I find Grafana dashboards more beautiful and easier to change. It also has a massive community behind providing customizable dashboards for free.</p>"}, {"location": "monitoring_comparison/#installation", "title": "Installation", "text": "<p>Nagios comes as a downloadable bundle with dedicated packages for every product with Windows or Linux distributions. After downloading and installing the tool, a set of first-time configurations is required. Once you\u2019ve installed the Nagios agents, data should start streaming into Nagios and its generic dashboards.</p> <p>Prometheus deployment is done through Docker containers that can spin up on every machine type, or through pre-compiled or self-compiled binaries.</p> <p>There are community maintained ansible roles for both solutions, doing a quick search I've found a Prometheus one that it's more maintained.</p> <p>For Kubernetes installation, I've only found helm charts for Prometheus.</p>"}, {"location": "monitoring_comparison/#kubernetes-integration", "title": "Kubernetes integration", "text": "<p>Prometheus, as Kubernetes are leading projects of the Cloud Native Computing Foundation, which is a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution.</p> <p>Prometheus has native support to be run in and to monitor Kubernetes clusters. Although Nagios can monitor Kubernetes, it's not meant to be run inside it.</p>"}, {"location": "monitoring_comparison/#documentation", "title": "Documentation", "text": "<p>I haven't used much the Nagios documentation, but I can tell you that even though it's improving Prometheus' is not very complete, and you find yourself often looking at issues and stackoverflow.</p>"}, {"location": "monitoring_comparison/#integrations", "title": "Integrations", "text": "<p>Official Prometheus\u2019 integrations are practically boundless. The long list of existing exporters combined with the user\u2019s ability to write new exporters allows integration with any tool, and PromQL allows users to query Prometheus data from any visualization tool that supports it.</p> <p>Nagios has a very limited list of official integrations. Most of them are operating systems which use the agents to monitor other network components. Others include MongoDB, Oracle, Selenium, and VMware. Once again, the community comes to rescue us with their contributions, keep in mind that you'll need to dive into the exchange for special monitoring needs.</p>"}, {"location": "monitoring_comparison/#alerts", "title": "Alerts", "text": "<p>Prometheus offers Alertmanager, a simple service that allows users to set thresholds and push alerts when breaches occur.</p> <p>Nagios uses a variety of media channels for alerts, including email, SMS, and audio alerts. Because its integration with the operating system is swift, Nagios even knows to generate a WinPopup message with the alert details.</p> <p>On a side note, there is an alert Nagios plugin that alerts for Prometheus query results.</p> <p>As Nagios doesn't support labels for the metrics, so there is no grouping, routing or deduplication of alerts as Prometheus do. Also the silence of alerts is done individually on each alert, while in Prometheus it's done using labels, which is more powerful.</p>"}, {"location": "monitoring_comparison/#advanced-monitorization", "title": "Advanced monitorization", "text": "<p>Nagios alerting is based on the return codes of scripts, Prometheus on the other hand alerts based on metrics, this fact together with the easy and powerful query language PromQL allows the user to make much more rich alerts that better represent the state of the system to monitor.</p> <p>In Nagios there is no concept of making queries to the gathered data.</p>"}, {"location": "monitoring_comparison/#data-storage", "title": "Data storage", "text": "<p>Nagios has no storage per-se, beyond the current check state. There are plugins which can store data such as for visualisation.</p> <p>Prometheus has a defined amount of data that's available (for example 30 days), to be able to store more you need to use Thanos, the prometheus long term storage solution.</p>"}, {"location": "monitoring_comparison/#high-availability", "title": "High availability", "text": "<p>Nagios servers are standalone, they are not meant to collaborate with other instances, so to achieve high availability you need to do it the old way, with multiple independent instances with a loadbalancer upfront.</p> <p>Prometheus can have different servers running collaboratively, monitoring between themselves. So you get high availability for free without any special configuration.</p>"}, {"location": "monitoring_comparison/#dynamic-infrastructure", "title": "Dynamic infrastructure", "text": "<p>In the past, infrastructure had a low rate of change, it was strange that you needed to add something to the monitorization system. Nowadays, with cloud infrastructures and kubernetes, instances are spawned and killed continuously.</p> <p>In Nagios, you need to manually configure each new service following the push architecture. In prometheus, thanks to the pull architecture and service discovery, new services are added and dead one removed automatically.</p>"}, {"location": "monitoring_comparison/#custom-script-execution", "title": "Custom script execution", "text": "<p>Nagios alerting is based on the return codes of scripts, therefore it's straightforward to create an alert based on a custom script.</p> <p>If you need to monitor something in Prometheus, and nobody has done it before, the development costs of an ad-hoc solutions are incredibly high, compared to Nagios. You'd need either to:</p> <ul> <li> <p>Use the script_exporter with     your script.  I've seen their repo, and the last commit is from March, and     they don't have a helm chart to install     it. I've searched     other alternative exporters, but this one seems to be the best for this     approach.</p> <p>The advantages of this approach is that you don't need to create and maintain a new prometheus exporter.</p> <p>The disadvantages though are that you'd have to:</p> <ul> <li>Manually install the required exporter resources in the cluster until a helm chart     exists.</li> <li>Create the helm charts yourself if they don't develop it.</li> <li> <p>Integrate your tool inside the script_exporter docker through one of these     ways:</p> <ul> <li>Changing the exporter Docker image to add it. Which would mean a Docker image     to maintain.</li> <li>Mounting the binary through a volume inside kubernetes. Which would mean     defining a way on how to upload it and assume the high availability penalty     that a stateful kubernetes service entail with the cluster configuration right     now.<ul> <li>If it's not already in your stack, it would mean adding a new exporter to maintain and a new development team to depend on.</li> </ul> </li> </ul> </li> </ul> <p>Alternatively you can use the script exporter binary in a baremetal or virtualized server instead of using a docker, that way you wouldn't need to maintain the different dockers for the different solutions, but you'd need a \"dedicated\" server for this purpose.</p> </li> <li> <p>Create your own exporter. You'd need to create a docker that exposes the command line functionality through a <code>metrics</code> endpoint. You wouldn't depend on a third party development team and would be able to use your script. On the other side it has the following disadvantages:</p> <ul> <li>We would need to create and maintain a new prometheus exporter. That would mean     creating and maintaining the Docker with the command line tool and a simple http     server that exposes the <code>/metrics</code> endpoint, that will run the command whenever the     Prometheus server accesses this endpoint.</li> <li>We add a new exporter to maintain but we develop it ourselves, so we don't depend on     third party developers.</li> </ul> </li> <li> <p>Use other exporters to do the check. For example, if you can deduce the     critical API call that will decide if the script fails or succeeds, you     could use the blackbox exporter to monitor it instead. The advantages of     this solution are:</p> <ul> <li>We don't add new infrastructure to develop or maintain.</li> <li>We don't depend on third party development teams.</li> </ul> <p>And the disadvantage is that if the logic changes, we would need to update how we do the check.</p> </li> </ul>"}, {"location": "monitoring_comparison/#network-monitorization", "title": "Network monitorization", "text": "<p>Both can use the Simple Network Management Protocol (SNMP) to communicate with network switches or other components by using SNMP protocol to query their status.</p> <p>Not being an expert on the topic, knowing it's been one of the core focus of Nagios in the past years and as I've not been able to find good comparison between both, I'm going to suppose that even though both support network monitoring, Nagios does a better job.</p>"}, {"location": "monitoring_comparison/#summary", "title": "Summary", "text": "Metric Nagios Prometheus Open Source \u2713* \u2713\u2713 Community \u2713 \u2713\u2713 Configuration and usage \u2713 x Visualizations \u2713 \u2713\u2713 Ansible Role \u2713 \u2713\u2713 Helm chart x \u2713 Kubernetes x \u2713 Documentation \u2713 x Integrations \u2713 \u2713\u2713 Alerts \u2713 \u2713\u2713 Advanced monitoring x \u2713 Custom script execution \u2713\u2713 \u2713 Data storage x \u2713 Dynamic infrastructure x \u2713 High availability \u2713 \u2713\u2713 Network Monitoring \u2713 \u2713 <p>* Only Nagios Core and the community contributions are open sourced.</p> <p>Where each symbol means:</p> <ul> <li>x: Doesn't meet the criteria.</li> <li>\u2713: Meets the criteria.</li> <li>\u2713\u2713: Meets the criteria and it's better than the other solution.</li> <li>?: I'm not sure.</li> </ul> <p>Nagios is the reference of the old-school monitoring solutions, suitable for basic monitoring of small, static and/or old-school systems where blackbox probing is sufficient.</p> <p>Prometheus is the reference of the new-wave monitoring solutions, suitable for more advanced monitoring of dynamic, new-wave systems (web applications, cloud, containers or Kubernetes) where whitebox monitoring is desired.</p>"}, {"location": "monitoring_comparison/#references", "title": "References", "text": "<ul> <li>Logz io post on Prometheus vs Nagios</li> </ul>"}, {"location": "ombi/", "title": "Ombi", "text": "<p>Ombi is a self-hosted web application that automatically gives your shared Jellyfin users the ability to request content by themselves! Ombi can be linked to multiple TV Show and Movie DVR tools to create a seamless end-to-end experience for your users.</p> <p>If Ombi is not for you, you may try Overseerr.</p>"}, {"location": "ombi/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Docs</li> </ul>"}, {"location": "orgmode/", "title": "Orgmode", "text": "<p><code>nvim-orgmode</code> is a Orgmode clone written in Lua for Neovim. Org-mode is a flexible note-taking system that was originally created for Emacs. It has gained wide-spread acclaim and was eventually ported to Neovim.</p>"}, {"location": "orgmode/#installation", "title": "Installation", "text": "<p>Add to your plugin config:</p> <pre><code>use {'nvim-orgmode/orgmode', config = function()\n  require('orgmode').setup{}\nend\n}\n</code></pre> <p>Then install it with <code>:PackerInstall</code>.</p> <p>Tweak the configuration:</p> <pre><code>-- init.lua\n\n-- Load custom treesitter grammar for org filetype\nrequire('orgmode').setup_ts_grammar()\n\n-- Treesitter configuration\nrequire('nvim-treesitter.configs').setup {\n  -- If TS highlights are not enabled at all, or disabled via `disable` prop,\n  -- highlighting will fallback to default Vim syntax highlighting\n  highlight = {\n    enable = true,\n    -- Required for spellcheck, some LaTex highlights and\n    -- code block highlights that do not have ts grammar\n    additional_vim_regex_highlighting = {'org'},\n  },\n  ensure_installed = {'org'}, -- Or run :TSUpdate org\n}\n\nrequire('orgmode').setup({\n  org_agenda_files = {'~/Dropbox/org/*', '~/my-orgs/**/*'},\n  org_default_notes_file = '~/Dropbox/org/refile.org',\n})\n</code></pre> <p>You can check the default configuration file here.</p>"}, {"location": "orgmode/#key-bindings", "title": "Key bindings", "text": "<p>Mappings or Key bindings can be changed on the <code>mappings</code> attribute of the <code>setup</code>. The program has these kinds of mappings:</p> <ul> <li>Org</li> <li>Agenda</li> <li>Capture</li> <li>Global</li> </ul> <p>For example the <code>global</code> mappings live under <code>mappings.global</code> and can be overridden like this:</p> <pre><code>require('orgmode').setup({\n  mappings = {\n    global = {\n      org_agenda = 'gA',\n      org_capture = 'gC'\n    }\n  }\n})\n</code></pre>"}, {"location": "orgmode/#be-ready-when-breaking-changes-come", "title": "Be ready when breaking changes come", "text": "<p>The developers have created an issue to track breaking changes, subscribe to it so you're notified in advance.</p>"}, {"location": "orgmode/#usage", "title": "Usage", "text": "<p>If you are new to Orgmode, check the vim Dotoo video, it's another plugin but the developers say it's the same. If you, like me, prefer written tutorials check the hands-on tutorial.</p> <p>If you get lost in any view you can use <code>g?</code> to get the mappings of the view.</p>"}, {"location": "orgmode/#org-file", "title": "Org File", "text": ""}, {"location": "orgmode/#headings", "title": "Headings", "text": "<p>Any line starting with one or more asterisks <code>*</code> but without any preceding whitespace is a heading (also called headline).</p> <pre><code>* Org Bullets\n* Vim table-mode\n</code></pre> <p>Once you are over a header, you can create a new header at the same level below the current subtree <code>&lt;leader&gt;&lt;enter&gt;</code>, if you want to add a heading after the current item use <code>&lt;control&gt;&lt;enter&gt;</code> if you use these key bindings:</p> <pre><code>require('orgmode').setup({\n  mappings = {\n    org = {\n      org_meta_return = '&lt;c-cr&gt;',\n      org_insert_heading_respect_content = '&lt;leader&gt;&lt;cr&gt;',\n    }\n  }\n})\n</code></pre> <p>To be able to use it in insert mode too add below the org-mode configuration:</p> <pre><code>vim.cmd[[\n  imap &lt;c-cr&gt; &lt;c-o&gt;&lt;c-cr&gt;\n  imap &lt;leader&gt;&lt;cr&gt; &lt;c-o&gt;&lt;leader&gt;&lt;cr&gt;\n]]\n</code></pre> <p>The number of asterisks denotes the level of the heading: the more asterisks, the deeper the level. That is how we achieve nested structures in our org files.</p> <pre><code>* Org Bullets\n** Synopsis\n* Vim table-mode\n</code></pre> <p>The content within a heading can be free form text, include links, be a list, or any combination thereof. For example:</p> <pre><code>* Org Bullets\n** Synopsis\n   This plugin is a clone of org-bullets. It replaces the asterisks in org\n   syntax with unicode characters.\n* Vim table-mode\n</code></pre> <p>The full syntax for a headline is</p> <pre><code>STARS KEYWORD PRIORITY TITLE TAGS\n*     TODO    [#A]     foo   :bar:baz:\n</code></pre> <p>Where:</p> <ul> <li><code>KEYWORD</code>: if present, turns the heading into a <code>TODO</code> item. </li> <li><code>PRIORITY</code> sets a priority level to be used in the Agenda.</li> <li><code>TITLE</code> is the main body of the heading.</li> <li><code>TAGS</code> is a colon surrounded and delimited list of tags used in searching in the Agenda.</li> </ul>"}, {"location": "orgmode/#toogle-line-to-headline", "title": "Toogle line to headline", "text": "<p>You can change a text line into a headline with <code>&lt;leader&gt;h</code> (Default: <code>&lt;leader&gt;o*</code>) with the next configuration:</p> <pre><code>org = {\n  org_toggle_heading = '&lt;leader&gt;h',\n}\n</code></pre> <p>If you have a checkbox inside a TODO item, it will transform it to a children TODO item.</p>"}, {"location": "orgmode/#change-heading-level", "title": "Change heading level", "text": "<p>To change the heading level use <code>&lt;&lt;</code> or <code>&gt;&gt;</code>. It doesn't work in visual mode though, if you want to change the level of the whole subtree you can use <code>&lt;S</code> and <code>&gt;S</code>. </p> <pre><code>org = {\n  org_do_promote = '&lt;&lt;',\n  org_do_demote = '&gt;&gt;',\n  org_promote_subtree = '&lt;S',\n  org_demote_subtree = '&gt;S',\n}\n</code></pre> <p>To be able to use it in insert mode too add below the org-mode configuration:</p> <pre><code>vim.cmd[[\n  imap &gt;&gt; &lt;esc&gt;&gt;&gt;\n  imap &lt;&lt; &lt;esc&gt;&lt;&lt;\n  imap &gt;S &lt;esc&gt;&gt;s\n  imap &lt;S &lt;esc&gt;&lt;s\n\n]]\n</code></pre> <p>If you don't like seeing so many stars, you can enable the <code>org_hide_leading_stars = true</code> option. To me it looks much cleaner.</p>"}, {"location": "orgmode/#moving-headings", "title": "Moving headings", "text": "<p>To move the subtrees up and down you can use <code>J</code> (Default <code>&lt;leader&gt;oJ</code>) and <code>K</code> (Default <code>&lt;leader&gt;oK</code>) with the next conf:</p> <pre><code>    org = {\n      org_move_subtree_up = \"K\",\n      org_move_subtree_down = \"J\",\n    }\n</code></pre>"}, {"location": "orgmode/#folding-headings", "title": "Folding headings", "text": "<p>To fold the headings you can use either the normal vim bindings <code>zc</code>, <code>zo</code>, <code>zM</code>, ... or <code>&lt;tab&gt;</code> to toogle the fold of an element or <code>&lt;shift&gt;&lt;tab&gt;</code> to toogle the whole file.</p>"}, {"location": "orgmode/#navigate-through-headings", "title": "Navigate through headings", "text": "<p>It's easy to navigate through your heading tree with:</p> <ul> <li>Next/previous heading of any level with <code>&lt;control&gt;j</code>/<code>&lt;control&gt;k</code> (Default <code>}</code>/<code>{</code>)</li> <li>Next/previous heading of the same level with <code>&lt;control&gt;n</code>/<code>&lt;control&gt;p</code> (Default <code>]]</code>/<code>[[</code>)</li> <li>Go to the parent heading with <code>gp</code> (Default <code>g{</code>)</li> </ul> <pre><code>org = {\n  org_next_visible_heading = '&lt;c-j&gt;',\n  org_previous_visible_heading = '&lt;c-k&gt;',\n  org_forward_heading_same_level = '&lt;c-n&gt;',\n  org_backward_heading_same_level = '&lt;c-p&gt;',\n  outline_up_heading = 'gp',\n}\n</code></pre> <p>To be able to use it in insert mode too add below the org-mode configuration:</p> <pre><code>vim.cmd[[\n  imap &lt;c-j&gt; &lt;esc&gt;&lt;c-j&gt;\n  imap &lt;c-k&gt; &lt;esc&gt;&lt;c-k&gt;\n  imap &lt;c-n&gt; &lt;esc&gt;&lt;c-n&gt;\n  imap &lt;c-p&gt; &lt;esc&gt;&lt;c-p&gt;\n]]\n</code></pre>"}, {"location": "orgmode/#todo-items", "title": "TODO items", "text": "<p><code>TODO</code> items are meant to model tasks that evolve between states. </p> <p>As creating <code>TODO</code> items is quite common you can:</p> <ul> <li>Create an item with the same level as the item above in the current position with <code>;t</code> (by default is <code>&lt;leader&gt;oit</code>).</li> <li>Create an item with the same level as the item above after all the children of the item above with <code>;T</code> (by default is <code>&lt;leader&gt;oit</code>).</li> </ul> <pre><code>org = {\n  org_insert_todo_heading = \";t\",\n  org_insert_todo_heading_respect_content = \";T\",\n}\n</code></pre> <p>To be able to use it in insert mode too add below the org-mode configuration:</p> <pre><code>vim.cmd[[\n  imap ;t &lt;c-o&gt;;t\n  imap ;T &lt;c-o&gt;;T\n]]\n</code></pre> <p>You can transition the state forward and backwards by using <code>t</code>/<code>T</code> (Default: <code>cit</code>/<code>ciT</code>) if you use:</p> <pre><code>org = {\n  org_todo = 't',\n  org_todo_prev = 'T',\n}\n</code></pre>"}, {"location": "orgmode/#todo-state-customization", "title": "TODO state customization", "text": "<p>By default they are <code>TODO</code> or <code>DONE</code> but you can define your own using the <code>org_todo_keywords</code> configuration. It accepts a list of unfinished states and finished states separated by a <code>'|'</code>. For example:</p> <pre><code>org_todo_keywords = { 'TODO', 'NEXT', '|', 'DONE' }\n</code></pre> <p>You can also use fast access states:</p> <pre><code>org_todo_keywords = { 'TODO(t)', 'NEXT(n)', '|', 'DONE(d)' }\n</code></pre> <p>Sadly you can't yet use different todo sequences.</p>"}, {"location": "orgmode/#priority", "title": "Priority", "text": "<p>TODO items can also have a priority, by default you have 3 levels <code>A</code>, <code>B</code> and <code>C</code>. If you don't set a priority it's set to <code>B</code>.</p> <p>You can increase/decrease the priority with <code>=</code>/<code>-</code> (Default: <code>ciR</code>/<code>cir</code>):</p> <pre><code>org = {\n  org_priority_up = '-',\n  org_priority_down = '=',\n}\n</code></pre> <p>I feel more comfortable with these priorities:</p> <ul> <li><code>A</code>: Critical</li> <li><code>B</code>: High</li> <li><code>C</code>: Normal</li> <li><code>D</code>: Low</li> </ul> <p>This gives you room to usually work on priorities <code>B-D</code> and if something shows up that is really really important, you can use <code>A</code>. You can set this setting with the next snippet:</p> <pre><code>require('orgmode').setup({\n  org_priority_highest = 'A',\n  org_priority_default = 'C',\n  org_priority_lowest = 'D',\n})\n</code></pre>"}, {"location": "orgmode/#dates", "title": "Dates", "text": "<p>TODO items can also have timestamps which are specifications of a date (possibly with a time or a range of times) in a special format, either <code>&lt;2003-09-16 Tue&gt;</code> or <code>&lt;2003-09-16 Tue 09:39&gt;</code> or <code>&lt;2003-09-16 Tue 12:00-12:30&gt;</code>. A timestamp can appear anywhere in the headline or body of an Org tree entry. Its presence causes entries to be shown on specific dates in the agenda.</p>"}, {"location": "orgmode/#date-types", "title": "Date types", "text": ""}, {"location": "orgmode/#appointments", "title": "Appointments", "text": "<p>Meant to be used for elements of the org file that have a defined date to occur, think of a calendar appointment. In the agenda display, the headline of an entry associated with a plain timestamp is shown exactly on that date. </p> <pre><code>* TODO Meet with Marie\n&lt;2023-02-24 Fri&gt;\n</code></pre> <p>When you insert the timestamps with the date popup picker with <code>;d</code> (Default: <code>&lt;leader&gt;oi.</code>) you can only select the day and not the time, but you can add it manually. </p> <p>You can also define a timestamp range that spans through many days <code>&lt;2023-02-24 Fri&gt;--&lt;2023-02-26 Sun&gt;</code>. The headline then is shown on the first and last day of the range, and on any dates that are displayed and fall in the range.  </p>"}, {"location": "orgmode/#recurring-tasks", "title": "Recurring tasks", "text": "<p>A timestamp may contain a repeater interval, indicating that it applies not only on the given date, but again and again after a certain interval of N hours (h), days (d), weeks (w), months (m), or years (y). The following shows up in the agenda every Wednesday:</p> <pre><code>* TODO Go to pilates\n  &lt;2007-05-16 Wed 12:30 +1w&gt;\n</code></pre> <p>When you mark a recurring task with the TODO keyword \u2018DONE\u2019, it no longer produces entries in the agenda. The problem with this is, however, is that then also the next instance of the repeated entry will not be active. Org mode deals with this in the following way: when you try to mark such an entry as done, it shifts the base date of the repeating timestamp by the repeater interval, and immediately sets the entry state back to TODO.</p> <p>As a consequence of shifting the base date, this entry is no longer visible in the agenda when checking past dates, but all future instances will be visible. </p> <p>With the <code>+1m</code> cookie, the date shift is always exactly one month. So if you have not paid the rent for three months, marking this entry DONE still keeps it as an overdue deadline. Depending on the task, this may not be the best way to handle it. For example, if you forgot to call your father for 3 weeks, it does not make sense to call him 3 times in a single day to make up for it. For these tasks you can use the <code>++</code> operator, for example <code>++1m</code>. Finally, there are tasks, like changing batteries, which should always repeat a certain time after the last time you did it you can use the <code>.+</code> operator. For example:</p> <pre><code>** TODO Call Father\n   DEADLINE: &lt;2008-02-10 Sun ++1w&gt;\n   Marking this DONE shifts the date by at least one week, but also\n   by as many weeks as it takes to get this date into the future.\n   However, it stays on a Sunday, even if you called and marked it\n   done on Saturday.\n\n** TODO Empty kitchen trash\n   DEADLINE: &lt;2008-02-08 Fri 20:00 ++1d&gt;\n   Marking this DONE shifts the date by at least one day, and also\n   by as many days as it takes to get the timestamp into the future.\n   Since there is a time in the timestamp, the next deadline in the\n   future will be on today's date if you complete the task before\n   20:00.\n\n** TODO Check the batteries in the smoke detectors\n   DEADLINE: &lt;2005-11-01 Tue .+1m&gt;\n   Marking this DONE shifts the date to one month after today.\n\n** TODO Wash my hands\n   DEADLINE: &lt;2019-04-05 08:00 Fri .+1h&gt;\n   Marking this DONE shifts the date to exactly one hour from now.\n</code></pre>"}, {"location": "orgmode/#scheduled", "title": "Scheduled", "text": "<p><code>SCHEDULED</code> defines when you are plan to start working on that task.</p> <p>The headline is listed under the given date. In addition, a reminder that the scheduled date has passed is present in the compilation for today, until the entry is marked as done.</p> <pre><code>*** TODO Call Trillian for a date on New Years Eve.\n    SCHEDULED: &lt;2004-12-25 Sat&gt;\n</code></pre> <p>If you want to delay the display of this task in the agenda, use <code>SCHEDULED: &lt;2004-12-25 Sat -2d&gt;</code> the task is still scheduled on the 25th but will appear two days later. In case the task contains a repeater, the delay is considered to affect all occurrences; if you want the delay to only affect the first scheduled occurrence of the task, use <code>--2d</code> instead. </p> <p>Scheduling an item in Org mode should not be understood in the same way that we understand scheduling a meeting. Setting a date for a meeting is just a simple appointment, you should mark this entry with a simple plain timestamp, to get this item shown on the date where it applies. This is a frequent misunderstanding by Org users. In Org mode, scheduling means setting a date when you want to start working on an action item. </p> <p>You can set it with <code>&lt;leader&gt;s</code> (Default: <code>&lt;leader&gt;ois</code>)</p>"}, {"location": "orgmode/#deadline", "title": "Deadline", "text": "<p><code>DEADLINE</code> defines when the task is supposed to be finished on. On the deadline date, the task is listed in the agenda. In addition, the agenda for today carries a warning about the approaching or missed deadline, starting <code>org_deadline_warning_days</code> before the due date (14 by default), and continuing until the entry is marked as done. An example:</p> <pre><code>* TODO Do this \nDEADLINE: &lt;2023-02-24 Fri&gt;\n</code></pre> <p>You can set it with <code>&lt;leader&gt;d</code> (Default: <code>&lt;leader&gt;oid</code>).</p> <p>Using too many tasks with a <code>DEADLINE</code> will clutter your agenda. Use it only for the actions that you need to have a reminder, instead try to using appointment dates instead. </p> <p>If you need a different warning period for a special task, you can specify it. For example setting a warning period of 5 days <code>DEADLINE: &lt;2004-02-29 Sun -5d&gt;</code>. To configure the default number of days add:</p> <pre><code>require('orgmode').setup({\n  org_deadline_warning_days = 10,\n})\n</code></pre>"}, {"location": "orgmode/#date-management", "title": "Date management", "text": "<pre><code>  org = {\n    org_deadline = '&lt;leader&gt;d',\n    org_schedule = '&lt;leader&gt;s',\n    org_time_stamp = ';d',\n  }\n</code></pre> <p>To edit existing dates you can:</p> <ul> <li>Increase/decrease the date under the cursor by 1 day with <code>&lt;shift&gt;&lt;up&gt;</code>/` <li>Increase/decrease the part of the date under the cursor with <code>&lt;control&gt;a</code>/<code>&lt;control&gt;x</code></li> <li>Bring the date pop up with <code>&lt;control&gt;e</code> (Default <code>cid</code>)</li> <pre><code>  org = {\n    org_change_date = '&lt;c-e&gt;',\n  }\n</code></pre> <p>To be able to use the bindings in insert mode too add below the org-mode configuration:</p> <pre><code>vim.cmd[[\n  imap ;d &lt;c-o&gt;;d\n  imap &lt;c-e&gt; &lt;c-o&gt;&lt;c-e&gt;\n]]\n</code></pre> <p>You can also use the next abbreviations:</p> <ul> <li><code>:today:</code>: expands to today's date (example: &lt;2021-06-29 Tue&gt;)</li> <li><code>:itoday:</code>: expands to an invactive version of today's date (example: [2021-06-29 Tue])</li> <li><code>:now:</code>: expands to today's date and current time (example: &lt;2021-06-29 Tue 15:32&gt;)</li> <li><code>:inow:</code>: expands to invactive version of today's date and current time (example: [2021-06-29 Tue 15:32]</li> </ul>"}, {"location": "orgmode/#tags", "title": "Tags", "text": "<p>You can also use tags to organize your items. To edit them use <code>&lt;leader&gt;g</code> (Default <code>&lt;leader&gt;ot</code>).</p> <pre><code>  org = {\n    org_set_tags_command = '&lt;leader&gt;g',\n  },\n</code></pre> <p>When you press that key you can type:</p> <ul> <li><code>tag1</code>: It will add <code>:tag1:</code>.</li> <li><code>tag1:tag2</code>: It will add <code>:tag1:tag2:</code>.</li> <li>Press <code>ESC</code>: It will remove all tags from the item.</li> </ul> <p>Tags are seen as <code>:tag1:tag2:</code> on the right of the TODO item description.</p> <p>Tags make use of the hierarchical structure of outline trees. If a heading has a certain tag, all subheadings inherit the tag as well. For example, in the list</p> <pre><code>* Meeting with the French group      :work:\n** Summary by Frank                  :boss:notes:\n*** TODO Prepare slides for him      :action:\n</code></pre> <p>The final heading has the tags <code>work</code>, <code>boss</code>, <code>notes</code>, and <code>action</code> even though the final heading is not explicitly marked with those tags. You can also set tags that all entries in a file should inherit just as if these tags were defined in a hypothetical level zero that surrounds the entire file. Using a line like the next one:</p> <pre><code>#+FILETAGS: :Peter:Boss:Secret:\n</code></pre>"}, {"location": "orgmode/#lists", "title": "Lists", "text": "<p>Lists start with a dash:</p> <pre><code>- Org bullets\n</code></pre> <p>To create new list item press <code>&lt;control&gt;&lt;enter&gt;</code>.</p>"}, {"location": "orgmode/#checkboxes", "title": "Checkboxes", "text": "<p>Checkboxes or checklists are a special type of list:</p> <pre><code>- [ ] Item 1\n  - [ ] Subitem 1\n  - [ ] Subitem 2\n- [ ] Item 2\n</code></pre> <p>If you're over an item you can create new ones with <code>&lt;control&gt;&lt;enter&gt;</code> (if you have the <code>org_meta_return = '&lt;c-cr&gt;'</code> binding set). </p> <p>You can change the checkbox state with <code>&lt;control&gt;&lt;space&gt;</code>, if you check a subitem the parent item will be marked as started <code>&lt;3</code> automatically:</p> <pre><code>- [-] Item 1\n  - [X] Subitem 1\n  - [ ] Subitem 2\n- [ ] Item 2\n</code></pre> <p>You can't yet manage the checkboxes as you do the headings by promoting, demoting and moving them around.</p> <p>Follow this issue if you want to see the progress of it's children at the parent checkbox.</p>"}, {"location": "orgmode/#links", "title": "Links", "text": "<p>One final aspect of the org file syntax are links. Links are of the form <code>[[link][description]]</code>, where link can be an:</p> <ul> <li>Internal reference</li> <li>External reference</li> </ul> <p>A link that does not look like a URL refers to the current document. You can follow it with <code>gx</code> when point is on the link (Default <code>&lt;leader&gt;oo</code>) if you use the next configuration.</p> <pre><code>org = {\n  org_open_at_point = 'gx',\n}\n</code></pre>"}, {"location": "orgmode/#internal-document-links", "title": "Internal document links", "text": "<p>Org provides several refinements to internal navigation within a document. Most notably:</p> <ul> <li><code>[[*Some section]]</code>: points to a headline with the name <code>Some section</code>.</li> <li><code>[[#my-custom-id]]</code>: targets the entry with the <code>CUSTOM_ID</code> property set to <code>my-custom-id</code>. </li> </ul> <p>When the link does not belong to any of the cases above, Org looks for a dedicated target: the same string in double angular brackets, like <code>&lt;&lt;My Target&gt;&gt;</code>.</p> <p>If no dedicated target exists, the link tries to match the exact name of an element within the buffer. Naming is done, unsurprisingly, with the <code>NAME</code> keyword, which has to be put in the line before the element it refers to, as in the following example</p> <pre><code>#+NAME: My Target\n| a  | table      |\n|----+------------|\n| of | four cells |\n</code></pre> <p>Ultimately, if none of the above succeeds, Org searches for a headline that is exactly the link text but may also include a <code>TODO</code> keyword and tags, or initiates a plain text search.</p> <p>Note that you must make sure custom IDs, dedicated targets, and names are unique throughout the document. Org provides a linter to assist you in the process, if needed, but I have not searched yet one for nvim.</p>"}, {"location": "orgmode/#external-links", "title": "External links", "text": "<ul> <li>URL (<code>http://</code>, <code>https://</code>)</li> <li>Path to a file (<code>file:/path/to/org/file</code>). File links can contain additional information to jump to a particular location in the file when following a link. This can be:</li> <li><code>file:~/code/main.c::255</code>: A line number </li> <li><code>file:~/xx.org::My Target</code>: A search for <code>&lt;&lt;My Target&gt;&gt;</code></li> <li><code>file:~/xx.org::#my-custom-id</code>: A   search for-  a custom ID</li> </ul>"}, {"location": "orgmode/#properties", "title": "Properties", "text": "<p>Properties are key-value pairs associated with an entry. They live in a special drawer with the name <code>PROPERTIES</code>. Each property is specified on a single line, with the key (surrounded by colons) first, and the value after it:</p> <pre><code>* CD collection\n** Classic\n*** Goldberg Variations\n    :PROPERTIES:\n    :Title:     Goldberg Variations\n    :Composer:  J.S. Bach\n    :Publisher: Deutsche Grammophon\n    :NDisks:    1\n    :END:\n</code></pre> <p>You may define the allowed values for a particular property <code>Xyz</code> by setting a property <code>Xyz_ALL</code>. This special property is inherited, so if you set it in a level 1 entry, it applies to the entire tree. When allowed values are defined, setting the corresponding property becomes easier and is less prone to typing errors. For the example with the CD collection, we can pre-define publishers and the number of disks in a box like this:</p> <pre><code>* CD collection\n  :PROPERTIES:\n  :NDisks_ALL:  1 2 3 4\n  :Publisher_ALL: \"Deutsche Grammophon\" Philips EMI\n  :END:\n</code></pre> <p>If you want to set properties that can be inherited by any entry in a file, use a line like:</p> <pre><code>#+PROPERTY: NDisks_ALL 1 2 3 4\n</code></pre> <p>This can be interesting for example if you want to track when was a header created:</p> <pre><code>*** Title of header\n   :PROPERTIES:\n   :CREATED: &lt;2023-03-03 Fri 12:11&gt; \n   :END:\n</code></pre>"}, {"location": "orgmode/#archiving", "title": "Archiving", "text": "<p>When we no longer need certain parts of our org files, they can be archived. You can archive items by pressing <code>;A</code> (Default <code>&lt;Leader&gt;o$</code>) while on the heading. This will also archive any child headings. The default location for archived headings is <code>&lt;name-of-current-org-file&gt;.org_archive</code>, which can be changed with the <code>org_archive_location</code> option.</p> <p>The problem is that when you archive an element you loose the context of the item unless it's a first level item. </p> <p>Another way to archive is by adding the <code>:ARCHIVE:</code> tag with <code>;a</code> and once all elements are archived move it to the archive.</p> <pre><code>org = {\n  org_toggle_archive_tag = ';a',\n  org_archive_subtree = ';A',\n}\n\nThere are some work in progress to improve archiving in the next issues [1](https://github.com/nvim-orgmode/orgmode/issues/413), [2](https://github.com/nvim-orgmode/orgmode/issues/369) and [3](https://github.com/joaomsa/telescope-orgmode.nvim/issues/2). \n\n## Refiling\n\nRefiling lets you easily move around elements of your org file, such as headings or TODOs. You can refile with `&lt;leader&gt;r` with the next snippet:\n\n```lua\norg = {\n  org_refile = '&lt;leader&gt;r',\n}\n</code></pre> <p>When you press the refile key binding you are supposed to press <code>&lt;tab&gt;</code> to see the available options, once you select the correct file, if you will be shown a autocomplete with the possible items to refile it to. Luckily there is a Telescope plugin.</p> <p>Install it by adding to your plugin config:</p> <pre><code>use 'joaomsa/telescope-orgmode.nvim'\n</code></pre> <p>Then install it with <code>:PackerInstall</code>.</p> <p>You can setup the extension by doing:</p> <pre><code>require('telescope').load_extension('orgmode')\n</code></pre> <p>To replace the default refile prompt:</p> <pre><code>vim.api.nvim_create_autocmd('FileType', {\n  pattern = 'org',\n  group = vim.api.nvim_create_augroup('orgmode_telescope_nvim', { clear = true })\n  callback = function()\n    vim.keymap.set('n', '&lt;leader&gt;r', require('telescope').extensions.orgmode.refile_heading)\n    vim.keymap.set('n', '&lt;leader&gt;g', require('telescope').extensions.orgmode.search_headings)\n  end,\n})\n</code></pre> <p>If the auto command doesn't override the default <code>orgmode</code> one, bind it to another keys and never use it.</p> <p>The plugin also allows you to use <code>telescope</code> to search through the headings of the different files with <code>search_headings</code>, with the configuration above you'd use <code>&lt;leader&gt;g</code>.</p>"}, {"location": "orgmode/#agenda", "title": "Agenda", "text": "<p>The org agenda is used to get an overview of all your different org files. Pressing <code>ga</code> (Default: <code>&lt;leader&gt;oa</code>) gives you an overview of the various specialized views into the agenda that are available. Remember that you can press <code>g?</code> to see all the available key mappings for each view.</p> <pre><code>  global = {\n    org_agenda = 'ga',\n  },\n</code></pre> <p>You'll be presented with the next views:</p> <ul> <li><code>a</code>: Agenda for current week or day                                                                      </li> <li><code>t</code>: List of all TODO entries                                                                            </li> <li><code>m</code>: Match a TAGS/PROP/TODO query                                                                        </li> <li><code>M</code>: Like <code>m</code>, but only TODO entries                                                                       </li> <li><code>s</code>: Search for keywords                                                                                 </li> <li><code>q</code>: Quit                                                                                                </li> </ul> <p>So far the <code>nvim-orgmode</code> agenda view lacks the next features:</p> <ul> <li>Custom agenda commands</li> <li>These interactions with the items:</li> <li>Remove it</li> <li>Promote/demote it</li> <li>Order it up and down</li> </ul>"}, {"location": "orgmode/#move-around-the-agenda-view", "title": "Move around the agenda view", "text": "<ul> <li><code>.</code>: Go to Today</li> <li><code>J</code>: Opens a popup that allows you to select the date to jump to.</li> <li><code>&lt;c-n&gt;</code>: Next agenda span (Default <code>f</code>). For example if you are in the week view it will go to the next week.</li> <li><code>&lt;c-p&gt;</code>: Previous agenda span (Default <code>b</code>).</li> <li><code>/</code>: Opens a prompt that allows filtering current agenda view by category, tags and title. </li> </ul> <p>For example, having a <code>todos.org</code> file with headlines that have tags <code>mytag</code> or <code>myothertag</code>, and some of them have check in content, searching by <code>todos+mytag/check/</code> returns all headlines that are in <code>todos.org</code> file, that have <code>mytag</code> tag, and have <code>check</code> in headline title. </p> <p>Note that <code>regex</code> is case sensitive by default. Use the vim regex flag <code>\\c</code> to make it case insensitive. For more information see <code>:help vim.regex()</code> and <code>:help /magic</code>.</p> <p>Pressing <code>&lt;TAB&gt;</code> in filter prompt autocompletes categories and tags.</p> <ul> <li><code>q</code>: Quit                                                                                                </li> </ul> <pre><code>  agenda = {\n    org_agenda_later = '&lt;c-n&gt;',\n    org_agenda_earlier = '&lt;c-p&gt;',\n  },\n</code></pre>"}, {"location": "orgmode/#act-on-the-agenda-elements", "title": "Act on the agenda elements", "text": "<ul> <li><code>&lt;enter&gt;</code>: Open the file containing the element on your cursor position. By default it opens it in the same buffer as the agenda view, which is a bit uncomfortable for me, I prefer the behaviour of <code>&lt;tab&gt;</code> so I'm using that instead.</li> <li><code>t</code>: Change <code>TODO</code> state of an item both in the agenda and the original Org file</li> <li><code>=</code>/<code>-</code>: Change the priority of the element</li> <li><code>r</code>: Reload all org files and refresh the current agenda view.</li> </ul> <pre><code>  agenda = {\n    org_agenda_switch_to = '&lt;tab&gt;',\n    org_agenda_goto = '&lt;cr&gt;',\n    org_agenda_priority_up = '=',\n    org_agenda_set_tags = '&lt;leader&gt;g',\n    org_agenda_deadline = '&lt;leader&gt;d',\n    org_agenda_schedule = '&lt;leader&gt;s',\n  },\n</code></pre>"}, {"location": "orgmode/#agenda-views", "title": "Agenda views:", "text": "<ul> <li><code>vd</code>: Show the agenda of the day</li> <li><code>vw</code>: Show the agenda of the week</li> <li><code>vm</code>: Show the agenda of the month</li> <li><code>vy</code>: Show the agenda of the year</li> </ul> <p>Once you open one of the views you can do most of the same stuff that you on othe org mode file:</p> <p>There is still no easy way to define your custom agenda views, but it looks possible 1 and 2.</p>"}, {"location": "orgmode/#agenda-searches", "title": "Agenda searches", "text": "<p>When using the search agenda view you can:</p> <ul> <li>Search by TODO states with <code>/WAITING</code></li> <li> <p>Search by tags <code>+home</code>. The syntax for such searches follows a simple boolean logic:</p> </li> <li> <p><code>|</code>: or</p> </li> <li><code>&amp;</code>: and</li> <li><code>+</code>: include matches</li> <li><code>-</code>: exclude matches </li> </ul> <p>Here are a few examples:</p> <ul> <li><code>+computer&amp;+urgent</code>: Returns all items tagged both <code>computer</code> and <code>urgent</code>.</li> <li><code>+computer|+urgent</code>: Returns all items tagged either <code>computer</code> or <code>urgent</code>.</li> <li><code>+computer&amp;-urgent</code>: Returns all items tagged <code>computer</code> and not <code>urgent</code>.</li> </ul> <p>As you may have noticed, the syntax above can be a little verbose, so org-mode offers convenient ways of shortening it. First, <code>-</code> and <code>+</code> imply <code>and</code> if no boolean operator is stated, so example three above could be rewritten simply as:</p> <pre><code>+computer-urgent\n</code></pre> <p>Second, inclusion of matches is implied if no <code>+</code> or <code>-</code> is present, so example three could be further shortened to:</p> <pre><code>computer-urgent\n</code></pre> <p>Example number two, meanwhile, could be shortened to:</p> <pre><code>computer|urgent\n</code></pre> <p>There is no way (as yet) to express search grouping with parentheses. The <code>and</code> operators (<code>&amp;</code>, <code>+</code>, and <code>-</code>) always bind terms together more strongly than <code>or</code> (<code>|</code>). For instance, the following search</p> <pre><code>computer|work+email\n</code></pre> <p>Results in all headlines tagged either with <code>computer</code> or both <code>work</code> and <code>email</code>. An expression such as <code>(computer|work)&amp;email</code> is not supported at the moment. You can construct a regular expression though:</p> <pre><code>+{computer\\|work}+email\n</code></pre> <ul> <li>Search by properties: You can search by properties with the <code>PROPERTY=\"value\"</code> syntax. Properties with numeric values can be queried with inequalities <code>PAGES&gt;100</code>. To search by partial searches use a regular expression, for example if the entry had <code>:BIB_TITLE: Mysteries of the Amazon</code> you could use <code>BIB_TITLE={Amazon}</code></li> </ul>"}, {"location": "orgmode/#capture", "title": "Capture", "text": "<p>Capture lets you quickly store notes with little interruption of your work flow. It works the next way:</p> <ul> <li>Open the interface with <code>;c</code> (Default <code>&lt;leader&gt;oc</code>) that asks you what kind of element you want to capture. </li> <li>Select the template you want to use. By default you only have the <code>Task</code> template, that introduces a task into the same file where you're at, select it by pressing <code>t</code>.</li> <li>Fill up the template.</li> <li>Choose what to do with the captured content:</li> <li>Save it to the configured file by pressing <code>;w</code> (Default <code>&lt;control&gt;c</code>)</li> <li>Refile it to a file by pressing <code>;r</code> (Default <code>&lt;leader&gt;or</code>).</li> <li>Abort the capture <code>;q</code> (Default <code>&lt;leader&gt;ok</code>).</li> </ul> <pre><code>mappings = {\n  global = {\n    org_capture = ';c',\n    },\n  capture = {\n    org_capture_finalize = ';w',\n    org_capture_refile = ';r',\n    org_capture_kill = ';q',\n  },\n}\n</code></pre>"}, {"location": "orgmode/#configure-the-capture-templates", "title": "Configure the capture templates", "text": "<p>Capture lets you define different templates for the different inputs. Each template has the next elements:</p> <ul> <li>Keybinding: Keys to press to activate the template</li> <li>Description: What to show in the capture menu to describe the template</li> <li>Template: The actual template of the capture, look below to see how to create them.</li> <li>Target: The place where the captured element will be inserted to. For example <code>~/org/todo.org</code>. If you don't define it it will go to the file configured in <code>org_default_notes_file</code>.</li> <li>Headline: An optional headline of the Target file to insert the element. </li> </ul> <p>For example:</p> <pre><code>org_capture_templates = {\n  t = { description = 'Task', template = '* TODO %?\\n  %u' }\n}\n</code></pre> <p>For the template you can use the next variables:</p> <ul> <li><code>%?:</code>Default cursor position when template is opened</li> <li><code>%t</code>: Prints current date (Example: <code>&lt;2021-06-10 Thu&gt;</code>)</li> <li><code>%T</code>: Prints current date and time (Example: <code>&lt;2021-06-10 Thu 12:30&gt;</code>)</li> <li><code>%u</code>: Prints current date in inactive format (Example: <code>[2021-06-10 Thu]</code>)</li> <li><code>%U</code>: Prints current date and time in inactive format (Example: <code>[2021-06-10 Thu 12:30]</code>)</li> <li><code>%&lt;FORMAT&gt;</code>: Insert current date/time formatted according to lua date format (Example: <code>%&lt;%Y-%m-%d %A&gt;</code> produces <code>2021-07-02 Friday</code>)</li> <li><code>%x</code>: Insert content of the clipboard via the \"+\" register (see <code>:help clipboard</code>)</li> <li><code>%^{PROMPT|DEFAULT|COMPLETION...}</code>: Prompt for input, if completion is provided an <code>:h inputlist</code> will be used</li> <li><code>%(EXP)</code>: Runs the given lua code and inserts the result. NOTE: this will internally pass the content to the lua <code>load()</code> function. So the body inside <code>%()</code> should be the body of a function that returns a string.</li> <li><code>%f</code>: Prints the file of the buffer capture was called from.</li> <li><code>%F</code>: Like <code>%f</code> but inserts the full path.</li> <li><code>%n</code>: Inserts the current <code>$USER</code></li> <li><code>%a</code>: File and line number from where capture was initiated (Example: <code>[[file:/home/user/projects/myfile.txt +2]]</code>)</li> </ul> <p>For example:</p> <pre><code>{ \n  T = {\n    description = 'Todo',\n    template = '* TODO %?\\n %u',\n    target = '~/org/todo.org'\n  },\n  j = {\n    description = 'Journal',\n    template = '\\n*** %&lt;%Y-%m-%d %A&gt;\\n**** %U\\n\\n%?',\n    target = '~/sync/org/journal.org'\n  },\n  -- Nested key example:\n  e =  'Event',\n  er = {\n    description = 'recurring',\n    template = '** %?\\n %T',\n    target = '~/org/calendar.org',\n    headline = 'recurring'\n  },\n  eo = {\n    description = 'one-time',\n    template = '** %?\\n %T',\n    target = '~/org/calendar.org',\n    headline = 'one-time'\n  },\n  -- Example using a lua function\n  r = {\n    description = \"Repo URL\",\n    template = \"* [[%x][%(return string.match('%x', '([^/]+)$'))]]%?\",\n    target = \"~/org/repos.org\",\n  }\n}\n</code></pre>"}, {"location": "orgmode/#use-capture", "title": "Use capture", "text": ""}, {"location": "orgmode/#synchronize-with-external-calendars", "title": "Synchronize with external calendars", "text": "<p>You may want to synchronize your calendar entries with external ones shared with other people, such as nextcloud calendar or google.</p> <p>The orgmode docs have a tutorial to sync with google and suggests some orgmode packages that do that, sadly it won't work with <code>nvim-orgmode</code>. We'll need to go the \"ugly way\" by:</p> <ul> <li>Downloading external calendar events to ics with <code>vdirsyncer</code>.</li> <li>Importing the ics to orgmode</li> <li>Editing the events in orgmode</li> <li>Exporting from orgmode to ics</li> <li>Uploading then changes to the external calendar events with <code>vdirsyncer</code>.</li> </ul>"}, {"location": "orgmode/#importing-the-ics-to-orgmode", "title": "Importing the ics to orgmode", "text": "<p>There are many tools that do this:</p> <ul> <li><code>ical2orgpy</code> </li> <li><code>ical2org</code> in go</li> </ul> <p>They import an <code>ics</code> file</p>"}, {"location": "orgmode/#exporting-from-orgmode-to-ics", "title": "Exporting from orgmode to ics", "text": ""}, {"location": "orgmode/#other-interesting-features", "title": "Other interesting features", "text": "<p>Some interesting features for the future are:</p> <ul> <li>Effort estimates</li> <li>Clocking</li> </ul>"}, {"location": "orgmode/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "orgmode/#sometimes-doesnt-work", "title": "Sometimes  doesn't work <p>Close the terminal and open a new one (pooooltergeist!).</p>", "text": ""}, {"location": "orgmode/#comparison-with-markdown", "title": "Comparison with Markdown", "text": "<p>What I like of Org mode over Markdown:</p> <ul> <li>The whole interface to interact with the elements of the document through key bindings:</li> <li>Move elements around.</li> <li>Create elements</li> <li>The TODO system is awesome</li> <li>The Agenda system</li> <li>How it handles checkboxes &lt;3</li> <li>Easy navigation between references in the document</li> <li>Archiving feature</li> <li>Refiling feature</li> <li><code>#</code> is used for comments.</li> <li>Create internal document links is easier, you can just copy and paste the heading similar to <code>[[*This is the heading]]</code> on markdown you need to edit it to <code>[](#this-is-the-heading)</code>.</li> </ul> <p>What I like of markdown over Org mode:</p> <ul> <li>The syntax of the headings <code>## Title</code> better than <code>** Title</code>. Although it makes sense to have <code>#</code> for comments.</li> <li>The syntax of the links: <code>[reference](link)</code> is prettier to read and write than <code>[[link][reference]]</code>, although this can be improved if only the reference is shown by your editor (nvim-orgmode doesn't do his yet)</li> </ul>"}, {"location": "orgmode/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Docs</li> <li>Developer docs</li> <li>List of supported commands</li> </ul>"}, {"location": "pipx/", "title": "Pipx", "text": "<p>Pipx is a command line tool to install and run Python applications in isolated environments.</p> <p>Very useful not to pollute your user or device python environments.</p>"}, {"location": "pipx/#installation", "title": "Installation", "text": "<pre><code>pip install pipx\n</code></pre>"}, {"location": "pipx/#usage", "title": "Usage", "text": "<p>Now that you have pipx installed, you can install a program:</p> <pre><code>pipx install PACKAGE\n</code></pre> <p>for example</p> <pre><code>pipx install pycowsay\n</code></pre> <p>You can list programs installed:</p> <pre><code>pipx list\n</code></pre> <p>Or you can run a program without installing it:</p> <pre><code>pipx run pycowsay moooo!\n</code></pre> <p>You can view documentation for all commands by running pipx --help.</p>"}, {"location": "pipx/#upgrade", "title": "Upgrade", "text": "<p>You can use <code>pipx upgrade-all</code> to upgrade all your installed packages. If you want to just upgrade one, use <code>pipx upgrade PACKAGE</code>.</p> <p>If the package doesn't change the requirements of their dependencies so that the installed don't meet them, they won't be upgraded unless you use the <code>--pip-args '--upgrade-strategy eager'</code> flag.</p> <p>It uses the pip flag <code>upgrade-strategy</code> which can be one of:</p> <ul> <li><code>eager</code>: dependencies are upgraded regardless of whether the currently   installed version satisfies the requirements of the upgraded package(s).</li> <li><code>only-if-needed</code>: dependencies are upgraded only when they do not satisfy the   requirements of the upgraded package(s). This is the default value.</li> </ul>"}, {"location": "pipx/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "pyment/", "title": "Pyment", "text": "<p>Pyment is a python3 program to automatically create, update or convert docstrings in existing Python files, managing several styles.</p>"}, {"location": "pyment/#installation", "title": "Installation", "text": "<pre><code>pip install pyment\n</code></pre>"}, {"location": "pyment/#usage", "title": "Usage", "text": "<pre><code>$: pyment  myfile.py    # will generate a patch\n$: pyment -w myfile.py  # will overwrite the file\n</code></pre> <p>As of 2021-11-17, the program is not production ready yet for me, I've tested it in one of my projects and found some bugs that needed to be fixed before it's usable. Despite the number of stars, it looks like the development pace has dropped dramatically, so it needs our help to get better :).</p>"}, {"location": "pyment/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "pytest/", "title": "Python pytest", "text": "<p>pytest is a Python framework to makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries.</p> <p>Pytest stands out over other test frameworks in:</p> <ul> <li>Simple tests are simple to write in pytest.</li> <li>Complex tests are still simple to write.</li> <li>Tests are easy to read.</li> <li>You can get started in seconds.</li> <li>You use <code>assert</code> to fail a test, not things like <code>self.assertEqual()</code> or   <code>self.assertLessThan()</code>. Just <code>assert</code>.</li> <li>You can use pytest to run tests written for unittest or nose.</li> </ul> <p>Note: You can use this cookiecutter template to create a python project with <code>pytest</code> already configured.</p>"}, {"location": "pytest/#install", "title": "Install", "text": "<pre><code>pip install pytest\n</code></pre>"}, {"location": "pytest/#usage", "title": "Usage", "text": "<p>Run in the project directory.</p> <pre><code>pytest\n</code></pre> <p>If you need more information run it with <code>-v</code>.</p> <p>Pytest automatically finds which tests to run in a phase called test discovery. It will get the tests that match one of the following conditions:</p> <ul> <li>Test files that are named <code>test_{{ something }}.py</code> or   <code>{{ something }}_test.py</code>.</li> <li>Test methods and functions named <code>test_{{ something }}</code>.</li> <li>Test classes named <code>Test{{ Something }}</code>.</li> </ul> <p>There are several possible outcomes of a test function:</p> <ul> <li>PASSED (.): The test ran successfully.</li> <li>FAILED (F): The test did not run usccessfully (or XPASS + strict).</li> <li>SKIPPED (s): The test was skipped. You can tell pytest to skip a test by   using enter the <code>@pytest.mark.skip()</code> or <code>pytest.mark.skipif()</code> decorators.</li> <li>xfail (x): The test was not supposed to pass, ran, and failed. You can tell   pytest that a test is expected to fail by using the <code>@pytest.mark.xfail()</code>   decorator.</li> <li>XPASS (X): The tests was not supposed to pass, ran, and passed.</li> <li>ERROR (E): An exception happened outside of the test function, in either a   fixture or a hook function.</li> </ul> <p>Pytest supports several cool flags like:</p> <ul> <li><code>-k EXPRESSION</code>: Used to select a subset of tests to run. For example   <code>pytest   -k \"asdict or defaults\"</code> will run both <code>test_asdict()</code> and   <code>test_defaults()</code>.</li> <li><code>--lf</code> or <code>--last-failed</code>: Just run the tests that have failed in the previous   run.</li> <li><code>-x</code>, or <code>--exitfirst</code>: Exit on first failed test.</li> <li><code>-l</code> or <code>--showlocals</code>: Print out the local variables in a test if the test   fails.</li> <li><code>-s</code> Allows any output that normally would be printed to <code>stdout</code> to actually   be printed to <code>stdout</code>. It's an alias of <code>--capture=no</code>, so the output is not   captured when the tests are run, which is the default behavior. This is useful   to debug with <code>print()</code> statements.</li> <li><code>--durations=N</code>: It reports the slowest <code>N</code> number of tests/setups/teardowns   after the test run. If you pass in <code>--durations=0</code>, it reports everything in   order of slowest to fastest.</li> <li><code>--setup-show</code>: Show the fixtures in use.</li> </ul>"}, {"location": "pytest/#fixtures", "title": "Fixtures", "text": "<p>Fixtures are functions that are run by pytest before (and sometimes after) the actual test functions.</p> <p>You can use fixtures to get a data set for the tests to work on, or use them to get a system into a known state before running a test. They are also used to get data ready for multiple tests.</p> <p>Here's a simple fixture that returns a number:</p> <pre><code>import pytest\n\n@pytest.fixture()\ndef some_data()\n\"\"\" Return answer to the ultimate question \"\"\"\n    return 42\n\ndef test_some_data(some_data):\n\"\"\" Use fixture return value in a test\"\"\"\n    assert some_data == 42\n</code></pre> <p>The <code>@pytest.fixture()</code> decorator is used to tell pytest that a function is a fixture.When you include the fixture name in the parameter list of a test function,pytest knows to run it before running the test. Fixtures can do work, and can also return data to the test function.</p> <p>The test test_some_data() has the name of the fixture, some_data, as a parameter.pytest will see this and look for a fixture with this name. Naming is significant in pytest. pytest will look in the module of the test for a fixture of that name.</p> <p>If the function is defined in the same file as where it's being used pylint will raise an <code>W0621: Redefining name %r from outer scope (line %s)</code> error. To solve it either move the fixture to other file or name the decorated function <code>fixture_&lt;fixturename&gt;</code> and then use <code>@pytest.fixture(name='&lt;fixturename&gt;')</code>.</p>"}, {"location": "pytest/#sharing-fixtures-through-conftestpy", "title": "Sharing fixtures through conftest.py", "text": "<p>You can put fixtures into individual test files, but to share fixtures among multiple test files, you need to use a <code>conftest.py</code> file somewhere centrally located for all of the tests. Additionally you can have <code>conftest.py</code> files in subdirectories of the top <code>tests</code> directory. If you do, fixtures defined in these lower level <code>conftest.py</code> files will be available to tests in that directory and subdirectories.</p> <p>Although <code>conftest.py</code> is a Python module, it should not be imported by test files. The file gets read by pytest, and is considered a local plugin.</p> <p>Another option is to save the fixtures in a file by creating a local pytest plugin.</p> <p>File: <code>tests/unit/conftest.py</code></p> <pre><code>pytest_plugins = [\n    \"tests.unit.fixtures.some_stuff\",\n]\n</code></pre> <p>File: <code>tests/unit/fixtures/some_stuff.py</code>:</p> <pre><code>import pytest\n\n\n@pytest.fixture\ndef foo():\n    return \"foobar\"\n</code></pre>"}, {"location": "pytest/#specifying-fixture-scope", "title": "Specifying fixture scope", "text": "<p>Fixtures include an optional parameter called scope, which controls how often a fixture gets set up and torn down. The scope parameter to <code>@pytest.fixture()</code> can have the values of function, class, module, or session.</p> <p>Here\u2019s a rundown of each scope value:</p> <ul> <li><code>scope='function'</code>: Run once per test function. The setup portion is run   before each test using the fixture. The teardown portion is run after each   test using the fixture. This is the default scope used when no scope parameter   is specified.</li> <li><code>scope='class'</code>: Run once per test class, regardless of how many test methods   are in the class.</li> <li><code>scope='module'</code>: Run once per module, regardless of how many test functions   or methods or other fixtures in the module use it.</li> <li><code>scope='session'</code> Run once per session. All test methods and functions using a   fixture of session scope share one setup and teardown call.</li> </ul>"}, {"location": "pytest/#using-fixtures-at-class-level", "title": "Using fixtures at class level", "text": "<p>Sometimes test functions do not directly need access to a fixture object. For example, tests may require to operate with an empty directory as the current working directory but otherwise do not care for the concrete directory.</p> <pre><code>@pytest.mark.usefixtures(\"cleandir\")\nclass TestDirectoryInit:\n    ...\n</code></pre> <p>Due to the <code>usefixtures</code> marker, the <code>cleandir</code> fixture will be required for the execution of each test method, just as if you specified a <code>cleandir</code> function argument to each of them.</p> <p>You can specify multiple fixtures like this:</p> <pre><code>@pytest.mark.usefixtures(\"cleandir\", \"anotherfixture\")\n</code></pre>"}, {"location": "pytest/#useful-fixtures", "title": "Useful Fixtures", "text": ""}, {"location": "pytest/#the-tmp_path-fixture", "title": "The tmp_path fixture", "text": "<p>You can use the <code>tmp_path</code> fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory.</p> <p><code>tmp_path</code> is a <code>pathlib.Path</code> object. Here is an example test usage:</p> <pre><code>def test_create_file(tmp_path):\n    d = tmp_path / \"sub\"\n    d.mkdir()\n    p = d / \"hello.txt\"\n    p.write_text(CONTENT)\n    assert p.read_text() == CONTENT\n    assert len(list(tmp_path.iterdir())) == 1\n    assert 0\n</code></pre>"}, {"location": "pytest/#the-tmpdir-fixture", "title": "The tmpdir fixture", "text": "<p>Warning: Don't use <code>tmpdir</code> use <code>tmp_path</code> instead because <code>tmpdir</code> uses <code>py</code> which is unmaintained and has unpatched vulnerabilities.</p> <p>You can use the <code>tmpdir</code> fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory.</p> <p><code>tmpdir</code> is a <code>py.path.local</code> object which offers <code>os.path</code> methods and more. Here is an example test usage:</p> <p>File: <code>test_tmpdir.py</code>:</p> <pre><code>from py._path.local import LocalPath\n\n\ndef test_create_file(tmpdir: LocalPath):\n    p = tmpdir.mkdir(\"sub\").join(\"hello.txt\")\n    p.write(\"content\")\n    assert p.read() == \"content\"\n    assert len(tmpdir.listdir()) == 1\n    assert 0\n</code></pre> <p>The <code>tmpdir</code> fixture has a scope of <code>function</code> so you can't make a session directory. Instead use the <code>tmpdir_factory</code> fixture.</p> <pre><code>from _pytest.tmpdir import TempPathFactory\n\n\n@pytest.fixture(scope=\"session\")\ndef image_file(tmpdir_factory: TempPathFactory):\n    img = compute_expensive_image()\n    fn = tmpdir_factory.mktemp(\"data\").join(\"img.png\")\n    img.save(str(fn))\n    return fn\n\n\ndef test_histogram(image_file):\n    img = load_image(image_file)\n    # compute and test histogram\n</code></pre>"}, {"location": "pytest/#make-a-subdirectory", "title": "Make a subdirectory", "text": "<pre><code>p = tmpdir.mkdir(\"sub\").join(\"hello.txt\")\n</code></pre>"}, {"location": "pytest/#the-caplog-fixture", "title": "The caplog fixture", "text": "<p>pytest captures log messages of level WARNING or above automatically and displays them in their own section for each failed test in the same manner as captured stdout and stderr.</p> <p>You can change the default logging level in the pytest configuration:</p> <p>File: <code>pytest.ini</code>:</p> <pre><code>[pytest]\n\nlog_level = debug\n</code></pre> <p>Although it may not be a good idea in most cases. It's better to change the log level in the tests that need a lower level.</p> <p>All the logs sent to the logger during the test run are available on the fixture in the form of both the <code>logging.LogRecord</code> instances and the final log text. This is useful for when you want to assert on the contents of a message:</p> <pre><code>from _pytest.logging import LogCaptureFixture\n\n\ndef test_baz(caplog: LogCaptureFixture):\n    func_under_test()\n    for record in caplog.records:\n        assert record.levelname != \"CRITICAL\"\n    assert \"wally\" not in caplog.text\n</code></pre> <p>You can also resort to record_tuples if all you want to do is to ensure that certain messages have been logged under a given logger name with a given severity and message:</p> <pre><code>def test_foo(caplog: LogCaptureFixture):\n    logging.getLogger().info(\"boo %s\", \"arg\")\n\n    assert (\"root\", logging.INFO, \"boo arg\") in caplog.record_tuples\n</code></pre> <p>You can call <code>caplog.clear()</code> to reset the captured log records in a test.</p>"}, {"location": "pytest/#change-the-log-level", "title": "Change the log level", "text": "<p>Inside tests it's possible to change the log level for the captured log messages.</p> <pre><code>def test_foo(caplog: LogCaptureFixture):\n    caplog.set_level(logging.INFO)\n    pass\n</code></pre> <p>If you just want to change the log level of a dependency you can use:</p> <pre><code>caplog.set_level(logging.WARNING, logger=\"urllib3\")\n</code></pre>"}, {"location": "pytest/#the-capsys-fixture", "title": "The capsys fixture", "text": "<p>The <code>capsys</code> builtin fixture provides two bits of functionality: it allows you to retrieve stdout and stderr from some code, and it disables output capture temporarily.</p> <p>Suppose you have a function to print a greeting to stdout:</p> <pre><code>def greeting(name):\n    print(f\"Hi, {name}\")\n</code></pre> <p>You can test the output by using <code>capsys</code>.</p> <pre><code>from _pytest.capture import CaptureFixture\n\n\ndef test_greeting(capsys: CaptureFixture[Any]):\n    greeting(\"Earthling\")\n    out, err = capsys.readouterr()\n    assert out == \"Hi, Earthling\\n\"\n    assert err == \"\"\n</code></pre> <p>The return value is whatever has been captured since the beginning of the function, or from the last time it was called.</p>"}, {"location": "pytest/#freezegun", "title": "freezegun", "text": "<p>freezegun lets you freeze time in both the test and fixtures.</p>"}, {"location": "pytest/#install_1", "title": "Install", "text": "<pre><code>pip install pytest-freezegun\n</code></pre>"}, {"location": "pytest/#usage_1", "title": "Usage", "text": ""}, {"location": "pytest/#global-usage", "title": "Global usage", "text": "<p>Most of the tests work with frozen time, so it's better to freeze it by default and unfreeze it on the ones that actually need time to move.</p> <p>To do that set in your <code>tests/conftest.py</code> a globally used fixture:</p> <pre><code>if TYPE_CHECKING:\n    from freezegun.api import FrozenDateTimeFactory\n\n\n@pytest.fixture(autouse=True)\ndef frozen_time() -&gt; Generator[FrozenDateTimeFactory, None, None]:\n\"\"\"Freeze all tests time\"\"\"\n    with freezegun.freeze_time() as freeze:\n        yield freeze\n</code></pre> <p>Freeze time by using the freezer fixture:</p>"}, {"location": "pytest/#manual-use", "title": "Manual use", "text": "<pre><code>if TYPE_CHECKING:\n    from freezegun.api import FrozenDateTimeFactory\n\n\ndef test_frozen_date(freezer: FrozenDateTimeFactory):\n    now = datetime.now()\n    time.sleep(1)\n    later = datetime.now()\n    assert now == later\n</code></pre> <p>This can then be used to move time:</p> <pre><code>def test_moving_date(freezer):\n    now = datetime.now()\n    freezer.move_to(\"2017-05-20\")\n    later = datetime.now()\n    assert now != later\n</code></pre> <p>You can also pass arguments to freezegun by using the <code>freeze_time</code> mark:</p> <pre><code>@pytest.mark.freeze_time(\"2017-05-21\")\ndef test_current_date():\n    assert date.today() == date(2017, 5, 21)\n</code></pre> <p>The <code>freezer</code> fixture and <code>freeze_time</code> mark can be used together, and they work with other fixtures:</p> <pre><code>@pytest.fixture\ndef current_date():\n    return date.today()\n\n\n@pytest.mark.freeze_time()\ndef test_changing_date(current_date, freezer):\n    freezer.move_to(\"2017-05-20\")\n    assert current_date == date(2017, 5, 20)\n    freezer.move_to(\"2017-05-21\")\n    assert current_date == date(2017, 5, 21)\n</code></pre> <p>They can also be used in class-based tests:</p> <pre><code>class TestDate:\n    @pytest.mark.freeze_time\n    def test_changing_date(self, current_date, freezer):\n        freezer.move_to(\"2017-05-20\")\n        assert current_date == date(2017, 5, 20)\n        freezer.move_to(\"2017-05-21\")\n        assert current_date == date(2017, 5, 21)\n</code></pre>"}, {"location": "pytest/#customize-nested-fixtures", "title": "Customize nested fixtures", "text": "<p>Sometimes you need to tweak your fixtures so they can be used in different tests. As usual, there are different solutions to the same problem.</p> <p>Note: \"TL;DR: For simple cases parametrize your fixtures or use parametrization to override the default valued fixture. As your test suite get's more complex migrate to pytest-case.\"</p> <p>Let's say you're running along merrily with some fixtures that create database objects for you:</p> <pre><code>@pytest.fixture\ndef supplier(db):\n    s = Supplier(\n        ref=random_ref(),\n        name=random_name(),\n        country=\"US\",\n    )\n    db.add(s)\n    yield s\n    db.remove(s)\n\n\n@pytest.fixture()\ndef product(db, supplier):\n    p = Product(\n        ref=random_ref(),\n        name=random_name(),\n        supplier=supplier,\n        net_price=9.99,\n    )\n    db.add(p)\n    yield p\n    db.remove(p)\n</code></pre> <p>And now you're writing a new test and you suddenly realize you need to customize your default \"supplier\" fixture:</p> <pre><code>def test_US_supplier_has_total_price_equal_net_price(product):\n    assert product.total_price == product.net_price\n\n\ndef test_EU_supplier_has_total_price_including_VAT(supplier, product):\n    supplier.country = \"FR\"  # oh, this doesn't work\n    assert product.total_price == product.net_price * 1.2\n</code></pre> <p>There are different ways to modify your fixtures</p>"}, {"location": "pytest/#add-more-fixtures", "title": "Add more fixtures", "text": "<p>We can just create more fixtures, and try to do a bit of DRY by extracting out common logic:</p> <pre><code>def _default_supplier():\n    return Supplier(\n        ref=random_ref(),\n        name=random_name(),\n    )\n\n\n@pytest.fixture\ndef us_supplier(db):\n    s = _default_supplier()\n    s.country = \"US\"\n    db.add(s)\n    yield s\n    db.remove(s)\n\n\n@pytest.fixture\ndef eu_supplier(db):\n    s = _default_supplier()\n    s.country = \"FR\"\n    db.add(s)\n    yield s\n    db.remove(s)\n</code></pre> <p>That's just one way you could do it, maybe you can figure out ways to reduce the duplication of the <code>db.add()</code> stuff as well, but you are going to have a different, named fixture for each customization of Supplier, and eventually you may decide that doesn't scale.</p>"}, {"location": "pytest/#use-factory-fixtures", "title": "Use factory fixtures", "text": "<p>Instead of a fixture returning an object directly, it can return a function that creates an object, and that function can take arguments:</p> <pre><code>@pytest.fixture\ndef make_supplier(db):\n    s = Supplier(\n        ref=random_ref(),\n        name=random_name(),\n    )\n\n    def _make_supplier(country):\n        s.country = country\n        db.add(s)\n        return s\n\n    yield _make_supplier\n    db.remove(s)\n</code></pre> <p>The problem with this is that, once you start, you tend to have to go all the way, and make all of your fixture hierarchy into factory functions:</p> <pre><code>def test_EU_supplier_has_total_price_including_VAT(make_supplier, product):\n    supplier = make_supplier(country=\"FR\")\n    product.supplier = (\n        supplier  # OH, now this doesn't work, because it's too late again\n    )\n    assert product.total_price == product.net_price * 1.2\n</code></pre> <p>And so...</p> <pre><code>@pytest.fixture\ndef make_product(db):\n    p = Product(\n        ref=random_ref(),\n        name=random_name(),\n    )\n\n    def _make_product(supplier):\n        p.supplier = supplier\n        db.add(p)\n        return p\n\n    yield _make_product\n    db.remove(p)\n\n\ndef test_EU_supplier_has_total_price_including_VAT(make_supplier, make_product):\n    supplier = make_supplier(country=\"FR\")\n    product = make_product(supplier=supplier)\n    assert product.total_price == product.net_price * 1.2\n</code></pre> <p>That works, but firstly now everything is a factory-fixture, which makes them more convoluted, and secondly, your tests are filling up with extra calls to <code>make_things</code>, and you're having to embed some of the domain knowledge of what-depends-on-what into your tests as well as your fixtures. Ugly!</p>"}, {"location": "pytest/#parametrize-your-fixtures", "title": "Parametrize your fixtures", "text": "<p>You can also parametrize your fixtures.</p> <pre><code>@pytest.fixture(params=[\"US\", \"FR\"])\ndef supplier(db, request):\n    s = Supplier(ref=random_ref(), name=random_name(), country=request.param)\n    db.add(s)\n    yield s\n    db.remove(s)\n</code></pre> <p>Now any test that depends on supplier, directly or indirectly, will be run twice, once with <code>supplier.country = US</code> and once with <code>FR</code>.</p> <p>That's really cool for checking that a given piece of logic works in a variety of different cases, but it's not really ideal in our case. We have to build a bunch of if logic into our tests:</p> <pre><code>def test_US_supplier_has_no_VAT_but_EU_supplier_has_total_price_including_VAT(product):\n    # this test is magically run twice, but:\n    if product.supplier.country == \"US\":\n        assert product.total_price == product.net_price\n    if product.supplier.country == \"FR\":\n        assert product.total_price == product.net_price * 1.2\n</code></pre> <p>So that's ugly, and on top of that, now every single test that depends (indirectly) on supplier gets run twice, and some of those extra test runs may be totally irrelevant to what the country is.</p>"}, {"location": "pytest/#use-pytest-parametrization-to-override-the-default-valued-fixtures", "title": "Use pytest parametrization to override the default valued fixtures", "text": "<p>We introduce an extra fixture that holds a default value for the country field:</p> <pre><code>@pytest.fixture()\ndef country():\n    return \"US\"\n\n\n@pytest.fixture\ndef supplier(db, country):\n    s = Supplier(\n        ref=random_ref(),\n        name=random_name(),\n        country=country,\n    )\n    db.add(s)\n    yield s\n    db.remove(s)\n</code></pre> <p>And then in the tests that need to change it, we can use parametrize to override the default value of country, even though the country fixture isn't explicitly named in that test:</p> <pre><code>@pytest.mark.parametrize(\"country\", [\"US\"])\ndef test_US_supplier_has_total_price_equal_net_price(product):\n    assert product.total_price == product.net_price\n\n\n@pytest.mark.parametrize(\"country\", [\"EU\"])\ndef test_EU_supplier_has_total_price_including_VAT(product):\n    assert product.total_price == product.net_price * 1.2\n</code></pre> <p>The only problem is that you're now likely to build a implicit dependencies where the only way to find out what's actually happening is to spend ages spelunking in conftest.py.</p>"}, {"location": "pytest/#use-pytest-case", "title": "Use pytest-case", "text": "<p>pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations.</p> <p>Check that file for further information.</p>"}, {"location": "pytest/#use-a-fixture-more-than-once-in-a-function", "title": "Use a fixture more than once in a function", "text": "<p>One solution is to make your fixture return a factory instead of the resource directly:</p> <pre><code>@pytest.fixture(name='make_user')\ndef make_user_():\n    created = []\n    def make_user():\n        u = models.User()\n        u.commit()\n        created.append(u)\n        return u\n\n    yield make_user\n\n    for u in created:\n        u.delete()\n\ndef test_two_users(make_user):\n    user1 = make_user()\n    user2 = make_user()\n    # test them\n\n\n# you can even have the normal fixture when you only need a single user\n@pytest.fixture\ndef user(make_user):\n    return make_user()\n\ndef test_one_user(user):\n    # test him/her\n</code></pre>"}, {"location": "pytest/#marks", "title": "Marks", "text": "<p>Pytest marks can be used to group tests. It can be useful to:</p> <p><code>slow</code> : Mark the tests that are slow.</p> <p><code>secondary</code> : Mart the tests that use functionality that is being tested in the same file.</p> <p>To mark a test, use the <code>@pytest.mark</code> decorator. For example:</p> <pre><code>@pytest.mark.slow\ndef test_really_slow_test():\n    pass\n</code></pre> <p>Pytest requires you to register your marks, do so in the <code>pytest.ini</code> file</p> <pre><code>[pytest]\nmarkers =\nslow: marks tests as slow (deselect with '-m \"not slow\"')\nsecondary: mark tests that use functionality tested in the same file (deselect with '-m \"not secondary\"')\n</code></pre>"}, {"location": "pytest/#snippets", "title": "Snippets", "text": ""}, {"location": "pytest/#mocking-sysexit", "title": "Mocking sys.exit", "text": "<pre><code>with pytest.raises(SystemExit):\n    # Code to test\n</code></pre>"}, {"location": "pytest/#testing-exceptions-with-pytest", "title": "Testing exceptions with pytest", "text": "<pre><code>def test_value_error_is_raised():\n    with pytest.raises(ValueError, match=\"invalid literal for int() with base 10: 'a'\"):\n        int(\"a\")\n</code></pre>"}, {"location": "pytest/#excluding-code-from-coverage", "title": "Excluding code from coverage", "text": "<p>You may have code in your project that you know won't be executed, and you want to tell <code>coverage.py</code> to ignore it. For example, if you have some code in abstract classes that is going to be tested on the subclasses, you can ignore it with <code># pragma: no cover</code>.</p> <p>If you want other code to be excluded, for example the statements inside the <code>if TYPE_CHECKING:</code> add to your <code>pyproject.toml</code>:</p> <pre><code>[tool.coverage.report]\nexclude_lines = [ \"pragma: no cover\", \"if TYPE_CHECKING:\",]\n</code></pre>"}, {"location": "pytest/#running-tests-in-parallel", "title": "Running tests in parallel", "text": "<p><code>pytest-xdist</code> makes it possible to run the tests in parallel, useful when the test suit is large or when the tests are slow.</p>"}, {"location": "pytest/#installation", "title": "Installation", "text": "<pre><code>pip install pytest-xdist\n</code></pre>"}, {"location": "pytest/#usage_2", "title": "Usage", "text": "<pre><code>pytest -n 4\n</code></pre> <p>It will run the tests with <code>4</code> workers. If you use <code>auto</code> it will adapt the number of workers to the number of CPUS, or 1 if you use <code>--pdb</code>.</p> <p>To configure it in the <code>pyproject.toml</code> use the <code>addopts</code></p> <pre><code>[tool.pytest.ini_options]\nminversion = \"6.0\"\naddopts = \"-vv --tb=short -n auto\"\n</code></pre>"}, {"location": "pytest/#enforce-serial-execution-of-related-tests", "title": "Enforce serial execution of related tests", "text": ""}, {"location": "pytest/#use-a-lock", "title": "Use a lock", "text": "<p>Implement a <code>serial</code> fixture with a session-scoped file <code>lock</code> fixture using the <code>filelock</code> package. You can add this to your <code>conftest.py</code>:</p> <pre><code>pip install filelock\n</code></pre> <pre><code>import contextlib\nimport os\n\nimport filelock\nimport pytest\nfrom filelock import BaseFileLock\n\n\n@pytest.fixture(name=\"lock\", scope=\"session\")\ndef lock_(\n    tmp_path_factory: pytest.TempPathFactory,\n) -&gt; Generator[BaseFileLock, None, None]:\n\"\"\"Create lock file.\"\"\"\n    base_temp = tmp_path_factory.getbasetemp()\n    lock_file = base_temp.parent / \"serial.lock\"\n\n    yield FileLock(lock_file=str(lock_file))\n\n    with contextlib.suppress(OSError):\n        os.remove(path=lock_file)\n\n\n@pytest.fixture(name=\"serial\")\ndef _serial(lock: BaseFileLock) -&gt; Generator[None, None, None]:\n\"\"\"Fixture to run tests in serial.\"\"\"\n    with lock.acquire(poll_interval=0.1):\n        yield\n</code></pre> <p>Then inject the <code>serial</code> fixture in any test that requires serial execution. All tests that use the serial fixture are executed serially while any tests that do not use the fixture are executed in parallel.</p>"}, {"location": "pytest/#mark-them-and-run-separately", "title": "Mark them and run separately", "text": "<p>Mark the tests you want to execute serially with a special mark, say serial:</p> <pre><code>@pytest.mark.serial\nclass Test:\n    ...\n\n\n@pytest.mark.serial\ndef test_foo():\n    ...\n</code></pre> <p>Execute your parallel tests, excluding those with the serial mark:</p> <pre><code>$ py.test -n auto -m \"not serial\"\n</code></pre> <p>Next, execute your serial tests in a separate session:</p> <pre><code>$ py.test -n0 -m \"serial\"\n</code></pre>"}, {"location": "pytest/#setting-a-timeout-for-your-tests", "title": "Setting a timeout for your tests", "text": "<p>To make your tests fail if they don't end in less than X seconds, use pytest-timeout.</p> <p>Install it with:</p> <pre><code>pip install pytest-timeout\n</code></pre> <p>You can set a global timeout in your <code>pyproject.toml</code>:</p> <pre><code>[pytest]\ntimeout = 300\n</code></pre> <p>Or define it for each test with:</p> <pre><code>@pytest.mark.timeout(60)\ndef test_foo():\n    pass\n</code></pre>"}, {"location": "pytest/#rerun-tests-that-fail-sometimes", "title": "Rerun tests that fail sometimes", "text": "<p>pytest-rerunfailures is a plugin for pytest that re-runs tests to eliminate intermittent failures. Using this plugin is generally a bad idea, it would be best to solve the reason why your code is not reliable. It's useful when you rely on non robust third party software in a way that you can't solve, or if the error is not in your code but in the testing code, and again you are not able to fix it.</p> <p>Install it with:</p> <pre><code>pip install pytest-rerunfailures\n</code></pre> <p>To re-run all test failures, use the <code>--reruns</code> command line option with the maximum number of times you\u2019d like the tests to run:</p> <pre><code>pytest --reruns 5\n</code></pre> <p>Failed fixture or setup_class will also be re-executed.</p> <p>To add a delay time between re-runs use the <code>--reruns-delay</code> command line option with the amount of seconds that you would like wait before the next test re-run is launched:</p> <pre><code>pytest --reruns 5 --reruns-delay 1\n</code></pre> <p>To mark individual tests as flaky, and have them automatically re-run when they fail, add the <code>flaky</code> mark with the maximum number of times you\u2019d like the test to run:</p> <pre><code>@pytest.mark.flaky(reruns=5)\ndef test_example():\n    import random\n\n    assert random.choice([True, False])\n</code></pre>"}, {"location": "pytest/#run-tests-in-a-random-order", "title": "Run tests in a random order", "text": "<p><code>pytest-random-order</code> is a pytest plugin that randomises the order of tests. This can be useful to detect a test that passes just because it happens to run after an unrelated test that leaves the system in a favourable state.</p> <p>To use it add the <code>--random-order</code> to your pytest run.</p>"}, {"location": "pytest/#capture-deprecation-warnings", "title": "Capture deprecation warnings", "text": "<p>Python and its ecosystem does not have an assumption of strict SemVer, and has a tradition of providing deprecation warnings. If you have good CI, you should be able to catch warnings even before your users see them. Try the following pytest configuration:</p> <pre><code>[tool.pytest.ini_options]\nfilterwarnings = [ \"error\",]\n</code></pre> <p>This will turn warnings into errors and allow your CI to break before users break.</p> <p>You can ignore specific warnings as well. For example, the configuration below will ignore all user warnings and specific deprecation warnings matching a regex, but will transform all other warnings into errors.</p> <pre><code>[tool.pytest.ini_options]\nfilterwarnings = [ \"error\", \"ignore::UserWarning\", \"ignore:function ham\\\\(\\\\) is deprecated:DeprecationWarning\",]\n</code></pre> <p>When a warning matches more than one option in the list, the action for the last matching option is performed.</p> <p>If you want to ignore the warning of a specific package use:</p> <pre><code>filterwarnings = [ \"error\", \"ignore::DeprecationWarning:pytest_freezegun.*\",]\n</code></pre> <p>Note: It's better to suppress a warning instead of disabling it for the whole code, check how here.</p>"}, {"location": "pytest/#ensuring-code-triggers-a-deprecation-warning", "title": "Ensuring code triggers a deprecation warning", "text": "<p>You can also use pytest.deprecated_call() for checking that a certain function call triggers a <code>DeprecationWarning</code> or <code>PendingDeprecationWarning</code>:</p> <pre><code>import pytest\n\n\ndef test_myfunction_deprecated():\n    with pytest.deprecated_call():\n        myfunction(17)\n</code></pre>"}, {"location": "pytest/#asserting-warnings-with-the-warns-function", "title": "Asserting warnings with the warns function", "text": "<p>You can check that code raises a particular warning using pytest.warns(), which works in a similar manner to raises:</p> <pre><code>import warnings\nimport pytest\n\n\ndef test_warning():\n    with pytest.warns(UserWarning):\n        warnings.warn(\"my warning\", UserWarning)\n</code></pre> <p>The test will fail if the warning in question is not raised. The keyword argument match to assert that the exception matches a text or regex:</p> <pre><code>&gt;&gt;&gt; with pytest.warns(UserWarning, match='must be 0 or None'):\n...     warnings.warn(\"value must be 0 or None\", UserWarning)\n</code></pre>"}, {"location": "pytest/#recording-warnings", "title": "Recording warnings", "text": "<p>You can record raised warnings either using <code>pytest.warns()</code> or with the <code>recwarn</code> fixture.</p> <p>To record with <code>pytest.warns()</code> without asserting anything about the warnings, pass no arguments as the expected warning type and it will default to a generic Warning:</p> <pre><code>with pytest.warns() as record:\n    warnings.warn(\"user\", UserWarning)\n    warnings.warn(\"runtime\", RuntimeWarning)\n\nassert len(record) == 2\nassert str(record[0].message) == \"user\"\nassert str(record[1].message) == \"runtime\"\n</code></pre> <p>The <code>recwarn</code> fixture will record warnings for the whole function:</p> <pre><code>import warnings\n\n\ndef test_hello(recwarn):\n    warnings.warn(\"hello\", UserWarning)\n    assert len(recwarn) == 1\n    w = recwarn.pop(UserWarning)\n    assert issubclass(w.category, UserWarning)\n    assert str(w.message) == \"hello\"\n    assert w.filename\n    assert w.lineno\n</code></pre> <p>Both <code>recwarn</code> and <code>pytest.warns()</code> return the same interface for recorded warnings: a <code>WarningsRecorder</code> instance. To view the recorded warnings, you can iterate over this instance, call <code>len</code> on it to get the number of recorded warnings, or index into it to get a particular recorded warning.</p>"}, {"location": "pytest/#show-logging-messages-on-the-test-run", "title": "Show logging messages on the test run", "text": "<p>Add to your <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\nlog_cli = true\nlog_cli_level = 10\n</code></pre> <p>Or run it in the command itself <code>pytest -o log_cli=true --log-cli-level=10 func.py</code>.</p> <p>Remember you can change the log level of the different components in case it's too verbose.</p>"}, {"location": "pytest/#pytest-integration-with-vim", "title": "Pytest integration with Vim", "text": "<p>Integrating pytest into your Vim workflow enhances your productivity while writing code, thus making it easier to code using TDD.</p> <p>I use Janko-m's Vim-test plugin (which can be installed through Vundle) with the following configuration.</p> <pre><code>nmap &lt;silent&gt; t :TestNearest --pdb&lt;CR&gt;\nnmap &lt;silent&gt; &lt;leader&gt;t :TestSuite tests/unit&lt;CR&gt;\nnmap &lt;silent&gt; &lt;leader&gt;i :TestSuite tests/integration&lt;CR&gt;\nnmap &lt;silent&gt; &lt;leader&gt;T :TestFile&lt;CR&gt;\n\nlet test#python#runner = 'pytest'\nlet test#strategy = \"neovim\"\n</code></pre> <p>I often open Vim with a vertical split (<code>:vs</code>), in the left window I have the tests and in the right the code. Whenever I want to run a single test I press <code>t</code> when the cursor is inside that test. If you need to make changes in the code, you can press <code>t</code> again while the cursor is at the code you are testing and it will run the last test.</p> <p>Once the unit test has passed, I run the whole unit tests with <code>;t</code> (as <code>;</code> is my <code>&lt;leader&gt;</code>). And finally I use <code>;i</code> to run the integration tests.</p> <p>Finally, if the test suite is huge, I use <code>;T</code> to run only the tests of a single file.</p> <p>As you can see only the <code>t</code> has the <code>--pdb</code> flag, so the rest of them will run en parallel and any pdb trace will fail.</p>"}, {"location": "pytest/#reference", "title": "Reference", "text": "<ul> <li> <p>Book   Python Testing with pytest by Brian Okken.</p> </li> <li> <p>Docs</p> </li> <li> <p>Vim-test plugin</p> </li> </ul>"}, {"location": "pytest_cases/", "title": "Pytest cases", "text": "<p>pytest-cases is a pytest plugin that allows you to separate your test cases from your test functions.</p> <p>In addition, <code>pytest-cases</code> provides several useful goodies to empower <code>pytest</code>. In particular it improves the fixture mechanism to support \"fixture unions\". This is a major change in the internal <code>pytest</code> engine, unlocking many possibilities such as using fixture references as parameter values in a test function.</p>"}, {"location": "pytest_cases/#installing", "title": "Installing", "text": "<pre><code>pip install pytest_cases\n</code></pre> <p>Installing pytest-cases has effects on the order of <code>pytest</code> tests execution, even if you do not use its features. One positive side effect is that it fixed pytest#5054. But if you see less desirable ordering please report it.</p>"}, {"location": "pytest_cases/#why-pytest-cases", "title": "Why <code>pytest-cases</code>?", "text": "<p>Let's consider the following <code>foo</code> function under test, located in <code>example.py</code>:</p> <pre><code>def foo(a, b):\n    return a + 1, b + 1\n</code></pre> <p>If we were using plain <code>pytest</code> to test it with various inputs, we would create a <code>test_foo.py</code> file and use <code>@pytest.mark.parametrize</code>:</p> <pre><code>import pytest\nfrom example import foo\n\n@pytest.mark.parametrize(\"a,b\", [(1, 2), (-1, -2)])\ndef test_foo(a, b):\n    # check that foo runs correctly and that the result is a tuple.\n    assert isinstance(foo(a, b), tuple)\n</code></pre> <p>This is the fastest and most compact thing to do when you have a few number of test cases, that do not require code to generate each test case.</p> <p>Now imagine that instead of <code>(1, 2)</code> and <code>(-1, -2)</code> each of our test cases:</p> <ul> <li>Requires a few lines of code to be generated.</li> <li>Requires documentation to explain the other developers the intent of that     precise test case.</li> <li>Requires external resources (data files on the filesystem, databases...),     with a variable number of cases depending on what is available on the     resource.</li> <li>Requires a readable <code>id</code>, such as     <code>'uniformly_sampled_nonsorted_with_holes'</code> for the above example. Of course     we could use     <code>pytest.param</code>     or     <code>ids=&lt;list&gt;</code>     but that is \"a pain to maintain\" according to <code>pytest</code> doc. Such a design     does not feel right as the id is detached from the case.</li> </ul> <p>With standard <code>pytest</code> there is no particular pattern to simplify your life here. Investigating a little bit, people usually end up trying to mix parameters and fixtures and asking this kind of question: so1, so2. But by design it is not possible to solve this problem using fixtures, because <code>pytest</code> does not handle \"unions\" of fixtures.</p> <p>There is also an example in <code>pytest</code> doc with a <code>metafunc</code> hook.</p> <p>The issue with such workarounds is that you can do anything. And anything is a bit too much: this does not provide any convention / \"good practice\" on how to organize test cases, which is an open door to developing ad-hoc unreadable or unmaintainable solutions.</p> <p><code>pytest_cases</code> was created to provide an answer to this precise situation. It proposes a simple framework to separate test cases from test functions. The test cases are typically located in a separate \"companion\" file:</p> <ul> <li><code>test_foo.py</code> is your usual test file containing the test functions (named     <code>test_&lt;id&gt;</code>).</li> <li><code>test_foo_cases.py</code> contains the test cases, that are also functions. Note: an     alternate file naming style <code>cases_foo.py</code> is also available if you prefer     it.</li> </ul>"}, {"location": "pytest_cases/#basic-usage", "title": "Basic usage", "text": ""}, {"location": "pytest_cases/#case-functions", "title": "Case functions", "text": "<p>Let's create a <code>test_foo_cases.py</code> file. This file will contain test cases generator functions, that we will call case functions for brevity. In these functions, you will typically either parse some test data files, generate some simulated test data, expected results, etc.</p> <p>File: test_foo_cases.py</p> <pre><code>def case_two_positive_ints():\n\"\"\" Inputs are two positive integers \"\"\"\n    return 1, 2\n\ndef case_two_negative_ints():\n\"\"\" Inputs are two negative integers \"\"\"\n    return -1, -2\n</code></pre> <p>Case functions can return anything that is considered useful to run the associated test. You can use all classic pytest mechanism on case functions (id customization, skip/fail marks, parametrization or fixtures injection).</p>"}, {"location": "pytest_cases/#test-functions", "title": "Test functions", "text": "<p>As usual we write our <code>pytest</code> test functions starting with <code>test_</code>, in a <code>test_foo.py</code> file. The only difference is that we now decorate it with <code>@parametrize_with_cases</code> instead of <code>@pytest.mark.parametrize</code> as we were doing previously:</p> <p>File: test_foo.py</p> <pre><code>from example import foo\nfrom pytest_cases import parametrize_with_cases\n\n@parametrize_with_cases(\"a,b\")\ndef test_foo(a, b):\n    # check that foo runs correctly and that the result is a tuple.\n    assert isinstance(foo(a, b), tuple)\n</code></pre> <p>Executing <code>pytest</code> will now run our test function once for every case function:</p> <pre><code>&gt;&gt;&gt; pytest -s -v\n============================= test session starts =============================\n(...)\n&lt;your_project&gt;/tests/test_foo.py::test_foo[two_positive_ints] PASSED [ 50%]\n&lt;your_project&gt;/tests/test_foo.py::test_foo[two_negative_ints] PASSED [ 100%]\n\n========================== 2 passed in 0.24 seconds ==========================\n</code></pre>"}, {"location": "pytest_cases/#usage", "title": "Usage", "text": ""}, {"location": "pytest_cases/#cases-collection", "title": "Cases collection", "text": ""}, {"location": "pytest_cases/#alternate-sources", "title": "Alternate source(s)", "text": "<p>It is not mandatory that case functions should be in a different file than the test functions: both can be in the same file. For this you can use <code>cases='.'</code> or <code>cases=THIS_MODULE</code> to refer to the module in which the test function is located:</p> <pre><code>from pytest_cases import parametrize_with_cases\n\ndef case_one_positive_int():\n    return 1\n\ndef case_one_negative_int():\n    return -1\n\n@parametrize_with_cases(\"i\", cases='.')\ndef test_with_this_module(i):\n    assert i == int(i)\n</code></pre> <p>Only the case functions defined BEFORE the test function in the module file will be taken into account.</p> <p><code>@parametrize_with_cases(cases=...)</code> also accepts explicit list of case functions, classes containing case functions, and modules. See API Reference for details. A typical way to organize cases is to use classes for example:</p> <pre><code>from pytest_cases import parametrize_with_cases\n\nclass Foo:\n    def case_a_positive_int(self):\n        return 1\n\n    def case_another_positive_int(self):\n        return 2\n\n@parametrize_with_cases(\"a\", cases=Foo)\ndef test_foo(a):\n    assert a &gt; 0\n</code></pre> <p>Note that as for <code>pytest</code>, <code>self</code> is recreated for every test and therefore should not be used to store any useful information.</p>"}, {"location": "pytest_cases/#alternate-prefix", "title": "Alternate prefix", "text": "<p><code>case_</code> might not be your preferred prefix, especially if you wish to store in the same module or class various kind of case data.  <code>@parametrize_with_cases</code> offers a <code>prefix=...</code> argument to select an alternate prefix for your case functions. That way, you can store in the same module or class case functions as diverse as datasets (e.g. <code>data_</code>), user descriptions (e.g. <code>user_</code>), algorithms or machine learning models (e.g. <code>model_</code> or <code>algo_</code>), etc.</p> <pre><code>from pytest_cases import parametrize_with_cases, parametrize\n\ndef data_a():\n    return 'a'\n\n@parametrize(\"hello\", [True, False])\ndef data_b(hello):\n    return \"hello\" if hello else \"world\"\n\ndef case_c():\n    return dict(name=\"hi i'm not used\")\n\ndef user_bob():\n    return \"bob\"\n\n@parametrize_with_cases(\"data\", cases='.', prefix=\"data_\")\n@parametrize_with_cases(\"user\", cases='.', prefix=\"user_\")\ndef test_with_data(data, user):\n    assert data in ('a', \"hello\", \"world\")\n    assert user == 'bob'\n</code></pre> <p>Yields</p> <pre><code>test_doc_filters_n_tags.py::test_with_data[bob-a]       PASSED [ 33%]\ntest_doc_filters_n_tags.py::test_with_data[bob-b-True]   PASSED [ 66%]\ntest_doc_filters_n_tags.py::test_with_data[bob-b-False]   PASSED [ 100%]\n</code></pre>"}, {"location": "pytest_cases/#filters-and-tags", "title": "Filters and tags", "text": "<p>The easiest way to select only a subset of case functions in a module or a class, is to specify a custom <code>prefix</code> instead of the default one (<code>'case_'</code>).</p> <p>However sometimes more advanced filtering is required. In that case, you can also rely on three additional mechanisms provided in <code>@parametrize_with_cases</code>:</p> <ul> <li> <p>The <code>glob</code> argument can contain a glob-like pattern for case ids. This can     become handy to separate for example good or bad cases, the latter returning     an expected error type and/or message for use with <code>pytest.raises</code> or with     our alternative     <code>assert_exception</code>.</p> <pre><code>from math import sqrt\nimport pytest\nfrom pytest_cases import parametrize_with_cases\n\n\ndef case_int_success():\n    return 1\n\ndef case_negative_int_failure():\n    # note that we decide to return the expected type of failure to check it\n    return -1, ValueError, \"math domain error\"\n\n\n@parametrize_with_cases(\"data\", cases='.', glob=\"*success\")\ndef test_good_datasets(data):\n    assert sqrt(data) &gt; 0\n\n@parametrize_with_cases(\"data, err_type, err_msg\", cases='.', glob=\"*failure\")\ndef test_bad_datasets(data, err_type, err_msg):\n    with pytest.raises(err_type, match=err_msg):\n        sqrt(data)\n</code></pre> </li> <li> <p>The <code>has_tag</code> argument allows you to filter cases based on tags set on case     functions using the <code>@case</code> decorator. See API reference of     <code>@case</code> and     <code>@parametrize_with_cases</code>.</p> <pre><code>from pytest_cases import parametrize_with_cases, case\n\nclass FooCases:\n    def case_two_positive_ints(self):\n        return 1, 2\n\n    @case(tags='foo')\n    def case_one_positive_int(self):\n        return 1\n\n@parametrize_with_cases(\"a\", cases=FooCases, has_tag='foo')\ndef test_foo(a):\n    assert a &gt; 0\n</code></pre> </li> <li> <p>Finally if none of the above matches your expectations, you can provide     a callable to <code>filter</code>. This callable will receive each collected case     function and should return <code>True</code> in case of success. Note that your     function can leverage the <code>_pytestcase</code> attribute available on the case     function to read the tags, marks and id found on it.</p> <pre><code>@parametrize_with_cases(\"data\", cases='.',\n                        filter=lambda cf: \"success\" in cf._pytestcase.id)\ndef test_good_datasets2(data):\n    assert sqrt(data) &gt; 0\n</code></pre> </li> </ul> <p>pytest marks such as <code>@pytest.mark.skipif</code> can be applied on case functions the same way as with test functions.</p> <pre><code>import sys\nimport pytest\n\n@pytest.mark.skipif(sys.version_info &lt; (3, 0), reason=\"Not useful on python 2\")\ndef case_two_positive_ints():\n    return 1, 2\n</code></pre>"}, {"location": "pytest_cases/#pytest-marks-skip-xfail-on-cases", "title": "Pytest marks (<code>skip</code>, <code>xfail</code>...) on cases", "text": ""}, {"location": "pytest_cases/#case-generators", "title": "Case generators", "text": "<p>In many real-world usage we want to generate one test case per <code>&lt;something&gt;</code>. The most intuitive way would be to use a <code>for</code> loop to create the case functions, and to use the <code>@case</code> decorator to set their names ; however this would not be very readable.</p> <p>Instead, case functions can be parametrized the same way as with test functions: simply add the parameter names as arguments in their signature and decorate with <code>@pytest.mark.parametrize</code>. Even better, you can use the enhanced <code>@parametrize</code> from <code>pytest-cases</code> so as to benefit from its additional usability features:</p> <pre><code>from pytest_cases import parametrize, parametrize_with_cases\n\nclass CasesFoo:\n    def case_hello(self):\n        return \"hello world\"\n\n    @parametrize(who=('you', 'there'))\n    def case_simple_generator(self, who):\n        return \"hello %s\" % who\n\n\n@parametrize_with_cases(\"msg\", cases=CasesFoo)\ndef test_foo(msg):\n    assert isinstance(msg, str) and msg.startswith(\"hello\")\n</code></pre> <p>Yields</p> <pre><code>test_generators.py::test_foo[hello] PASSED               [ 33%]\ntest_generators.py::test_foo[simple_generator-who=you] PASSED [ 66%]\ntest_generators.py::test_foo[simple_generator-who=there] PASSED [100%]\n</code></pre>"}, {"location": "pytest_cases/#cases-requiring-fixtures", "title": "Cases requiring fixtures", "text": "<p>Cases can use fixtures the same way as test functions do: simply add the fixture names as arguments in their signature and make sure the fixture exists either in the same module, or in a <code>conftest.py</code> file in one of the parent packages. See <code>pytest</code> documentation on sharing fixtures.</p> <p>Use <code>@fixture</code> instead of <code>@pytest.fixture</code></p> <p>If a fixture is used by some of your cases only, then you should use the <code>@fixture</code> decorator from pytest-cases instead of the standard <code>@pytest.fixture</code>. Otherwise you fixture will be setup/teardown for all cases even those not requiring it. See <code>@fixture</code> doc.</p> <pre><code>from pytest_cases import parametrize_with_cases, fixture, parametrize\n\n@fixture(scope='session')\ndef db():\n    return {0: 'louise', 1: 'bob'}\n\ndef user_bob(db):\n    return db[1]\n\n@parametrize(id=range(2))\ndef user_from_db(db, id):\n    return db[id]\n\n@parametrize_with_cases(\"a\", cases='.', prefix='user_')\ndef test_users(a, db, request):\n    print(\"this is test %r\" % request.node.nodeid)\n    assert a in db.values()\n</code></pre> <p>Yields</p> <pre><code>test_fixtures.py::test_users[a_is_bob]\ntest_fixtures.py::test_users[a_is_from_db-id=0]\ntest_fixtures.py::test_users[a_is_from_db-id=1]\n</code></pre>"}, {"location": "pytest_cases/#parametrize-fixtures-with-cases", "title": "Parametrize fixtures with cases", "text": "<p>In some scenarios you might wish to parametrize a fixture with the cases, rather than the test function. For example:</p> <ul> <li>To inject the same test cases in several test functions without     duplicating the <code>@parametrize_with_cases</code> decorator on each of them.</li> <li>To generate the test cases once for the whole session, using     a <code>scope='session'</code> fixture or another     scope.</li> <li>To modify the test cases, log some message, or perform some other action     before injecting them into the test functions, and/or after executing the     test function (thanks to yield     fixtures).</li> </ul> <p>For this, simply use <code>@fixture</code> from <code>pytest_cases</code> instead of <code>@pytest.fixture</code> to define your fixture. That allows your fixtures to be easily parametrized with <code>@parametrize_with_cases</code>, <code>@parametrize</code>, and even <code>@pytest.mark.parametrize</code>.</p> <pre><code>from pytest_cases import fixture, parametrize_with_cases\n\n@fixture\n@parametrize_with_cases(\"a,b\")\ndef c(a, b):\n    return a + b\n\ndef test_foo(c):\n    assert isinstance(c, int)\n</code></pre>"}, {"location": "pytest_cases/#pytest-cases-internals", "title": "Pytest-cases internals", "text": ""}, {"location": "pytest_cases/#fixture", "title": "<code>@fixture</code>", "text": "<p><code>@fixture</code> is similar to <code>pytest.fixture</code> but without its <code>param</code> and <code>ids</code> arguments. Instead, it is able to pick the parametrization from <code>@pytest.mark.parametrize</code> marks applied on fixtures. This makes it very intuitive for users to parametrize both their tests and fixtures.</p> <p>Finally it now supports unpacking, see unpacking feature.</p> <p><code>@fixture</code> deprecation if/when <code>@pytest.fixture</code> supports <code>@pytest.mark.parametrize</code></p> <p>The ability for pytest fixtures to support the <code>@pytest.mark.parametrize</code> annotation is a feature that clearly belongs to <code>pytest</code> scope, and has been requested already. It is therefore expected that <code>@fixture</code> will be deprecated in favor of <code>@pytest_fixture</code> if/when the <code>pytest</code> team decides to add the proposed feature. As always, deprecation will happen slowly across versions (at least two minor, or one major version update) so as for users to have the time to update their code bases.</p>"}, {"location": "pytest_cases/#unpack_fixture-unpack_into", "title": "<code>unpack_fixture</code> / <code>unpack_into</code>", "text": "<p>In some cases fixtures return a tuple or a list of items. It is not easy to refer to a single of these items in a test or another fixture. With <code>unpack_fixture</code> you can easily do it:</p> <pre><code>import pytest\nfrom pytest_cases import unpack_fixture, fixture\n\n@fixture\n@pytest.mark.parametrize(\"o\", ['hello', 'world'])\ndef c(o):\n    return o, o[0]\n\na, b = unpack_fixture(\"a,b\", c)\n\ndef test_function(a, b):\n    assert a[0] == b\n</code></pre> <p>Note that you can also use the <code>unpack_into=</code> argument of <code>@fixture</code> to do the same thing:</p> <pre><code>import pytest\nfrom pytest_cases import fixture\n\n@fixture(unpack_into=\"a,b\")\n@pytest.mark.parametrize(\"o\", ['hello', 'world'])\ndef c(o):\n    return o, o[0]\n\ndef test_function(a, b):\n    assert a[0] == b\n</code></pre> <p>And it is also available in <code>fixture_union</code>:</p> <pre><code>import pytest\nfrom pytest_cases import fixture, fixture_union\n\n@fixture\n@pytest.mark.parametrize(\"o\", ['hello', 'world'])\ndef c(o):\n    return o, o[0]\n\n@fixture\n@pytest.mark.parametrize(\"o\", ['yeepee', 'yay'])\ndef d(o):\n    return o, o[0]\n\nfixture_union(\"c_or_d\", [c, d], unpack_into=\"a, b\")\n\ndef test_function(a, b):\n    assert a[0] == b\n</code></pre>"}, {"location": "pytest_cases/#param_fixtures", "title": "<code>param_fixture[s]</code>", "text": "<p>If you wish to share some parameters across several fixtures and tests, it might be convenient to have a fixture representing this parameter. This is relatively easy for single parameters, but a bit harder for parameter tuples.</p> <p>The two utilities functions <code>param_fixture</code> (for a single parameter name) and <code>param_fixtures</code> (for a tuple of parameter names) handle the difficulty for you:</p> <pre><code>import pytest\nfrom pytest_cases import param_fixtures, param_fixture\n\n# create a single parameter fixture\nmy_parameter = param_fixture(\"my_parameter\", [1, 2, 3, 4])\n\n@pytest.fixture\ndef fixture_uses_param(my_parameter):\n    ...\n\ndef test_uses_param(my_parameter, fixture_uses_param):\n    ...\n\n# -----\n# create a 2-tuple parameter fixture\narg1, arg2 = param_fixtures(\"arg1, arg2\", [(1, 2), (3, 4)])\n\n@pytest.fixture\ndef fixture_uses_param2(arg2):\n    ...\n\ndef test_uses_param2(arg1, arg2, fixture_uses_param2):\n    ...\n</code></pre>"}, {"location": "pytest_cases/#fixture_union", "title": "<code>fixture_union</code>", "text": "<p>As of <code>pytest</code> 5, it is not possible to create a \"union\" fixture, i.e. a parametrized fixture that would first take all the possible values of fixture A, then all possible values of fixture B, etc. Indeed all fixture dependencies of each test node are grouped together, and if they have parameters a big \"cross-product\" of the parameters is done by <code>pytest</code>.</p> <pre><code>from pytest_cases import fixture, fixture_union\n\n@fixture\ndef first():\n    return 'hello'\n\n@fixture(params=['a', 'b'])\ndef second(request):\n    return request.param\n\n# c will first take all the values of 'first', then all of 'second'\nc = fixture_union('c', [first, second])\n\ndef test_basic_union(c):\n    print(c)\n</code></pre> <p>yields</p> <pre><code>&lt;...&gt;::test_basic_union[c_is_first] hello   PASSED\n&lt;...&gt;::test_basic_union[c_is_second-a] a    PASSED\n&lt;...&gt;::test_basic_union[c_is_second-b] b    PASSED\n</code></pre>"}, {"location": "pytest_cases/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "pytest_parametrized_testing/", "title": "Python parametrized testing", "text": "<p>Parametrization is a process of running the same test with varying sets of data. Each combination of a test and data is counted as a new test case.</p> <p>There are multiple ways to parametrize your tests, each differs in complexity and flexibility.</p>"}, {"location": "pytest_parametrized_testing/#parametrize-the-test", "title": "Parametrize the test", "text": "<p>The most simple form of parametrization is at test level:</p> <pre><code>@pytest.mark.parametrize(\"number\", [1, 2, 3, 0, 42])\ndef test_foo(number):\n    assert number &gt; 0\n</code></pre> <p>In this case we are getting five tests: for number 1, 2, 3, 0 and 42. Each of those tests can fail independently of one another (if in this example the test with 0 will fail, and four others will pass).</p>"}, {"location": "pytest_parametrized_testing/#parametrize-the-fixtures", "title": "Parametrize the fixtures", "text": "<p>Fixtures may have parameters. Those parameters are passed as a list to the argument <code>params</code> of <code>@pytest.fixture()</code> decorator.</p> <p>Those parameters must be iterables, such as lists. Each parameter to a fixture is applied to each function using this fixture. If a few fixtures are used in one test function, pytest generates a Cartesian product of parameters of those fixtures.</p> <p>To use those parameters, a fixture must consume a special fixture named <code>request</code>. It provides the special (built-in) fixture with some information on the function it deals with. <code>request</code> also contains <code>request.param</code> which contains one element from <code>params</code>. The fixture called as many times as the number of elements in the iterable of <code>params</code> argument, and the test function is called with values of fixtures the same number of times. (basically, the fixture is called <code>len(iterable)</code> times with each next element of iterable in the <code>request.param</code>).</p> <pre><code>@pytest.fixture(params=[\"one\", \"uno\"])\ndef fixture1(request):\n    return request.param\n\n@pytest.fixture(params=[\"two\", \"duo\"])\ndef fixture2(request):\n    return request.paramdef test_foobar(fixture1, fixture2):\n    assert type(fixture1) == type(fixture2)\n</code></pre> <p>The output is:</p> <pre><code>#OUTPUT 3\ncollected 4 itemstest_3.py::test_foobar[one-two] PASSED  [ 25%]\ntest_3.py::test_foobar[one-duo] PASSED  [ 50%]\ntest_3.py::test_foobar[uno-two] PASSED  [ 75%]\ntest_3.py::test_foobar[uno-duo] PASSED  [100%]\n</code></pre>"}, {"location": "pytest_parametrized_testing/#parametrization-with-pytest_generate_tests", "title": "Parametrization with <code>pytest_generate_tests</code>", "text": "<p>There is an another way to generate arbitrary parametrization at collection time. It\u2019s a bit more direct and verbose, but it provides introspection of test functions, including the ability to see all other fixture names.</p> <p>At collection time Pytest looks up for and calls (if found) a special function in each module, named <code>pytest_generate_tests</code>. This function is not a fixture, but just a regular function. It receives the argument <code>metafunc</code>, which itself is not a fixture, but a special object.</p> <p><code>pytest_generate_tests</code> is called for each test function in the module to give a chance to parametrize it. Parametrization may happen only through fixtures that test function requests. There is no way to parametrize a test function like this:</p> <p>def test_simple():    assert 2+2 == 4</p> <p>You need some variables to be used as parameters, and those variables should be arguments to the test function. Pytest will replace those arguments with values from fixtures, and if there are a few values for a fixture, then this is parametrization at work.</p> <p><code>metafunc</code> argument to <code>pytest_generate_tests</code> provides some useful information on a test function:</p> <ul> <li>Ability to see all fixture names that function requests.</li> <li>Ability to see the name of the function.</li> <li>Ability to see code of the function.</li> </ul> <p>Finally, <code>metafunc</code> has a parametrize function, which is the way to provide multiple variants of values for fixtures.</p> <p>The same case as before written with the <code>pytest_generate_tests</code> function is:</p> <pre><code>def pytest_generate_tests(metafunc):\n    if \"fixture1\" in metafunc.fixturenames:\n        metafunc.parametrize(\"fixture1\", [\"one\", \"uno\"])\n    if \"fixture2\" in metafunc.fixturenames:\n        metafunc.parametrize(\"fixture2\", [\"two\", \"duo\"])\n\ndef test_foobar(fixture1, fixture2):\n    assert type(fixture1) == type(fixture2)\n</code></pre> <p>This solution is a little bit magical, so I'd avoid it in favor of pytest-cases.</p>"}, {"location": "pytest_parametrized_testing/#use-pytest-cases", "title": "Use pytest-cases", "text": "<p>pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations.</p> <p>Check that file for further information.</p>"}, {"location": "pytest_parametrized_testing/#customizations", "title": "Customizations", "text": ""}, {"location": "pytest_parametrized_testing/#change-the-tests-name", "title": "Change the tests name", "text": "<p>Sometimes you want to change how the tests are shown so you can understand better what the test is doing. You can use the <code>ids</code> argument to <code>pytest.mark.parametrize</code>.</p> <p>File tests/unit/test_func.py</p> <pre><code>tasks_to_try = (\n    Task('sleep', done=True),\n    Task('wake', 'brian'),\n    Task('wake', 'brian'),\n    Task('breathe', 'BRIAN', True),\n    Task('exercise', 'BrIaN', False),\n)\n\ntask_ids = [\n    f'Task({task.summary}, {task.owner}, {task.done})'\n    for task in tasks_to_try\n]\n\n@pytest.mark.parametrize('task', tasks_to_try, ids=task_ids)\ndef test_add_4(task):\n    task_id = tasks.add(task)\n    t_from_db = tasks.get(task_id)\n    assert equivalent(t_from_db, task)\n</code></pre> <pre><code>$ pytest -v test_func.py::test_add_4\n===================== test session starts ======================\ncollected 5 items\n\ntest_add_variety.py::test_add_4[Task(sleep,None,True)] PASSED\ntest_add_variety.py::test_add_4[Task(wake,brian,False)0] PASSED\ntest_add_variety.py::test_add_4[Task(wake,brian,False)1] PASSED\ntest_add_variety.py::test_add_4[Task(breathe,BRIAN,True)] PASSED\ntest_add_variety.py::test_add_4[Task(exercise,BrIaN,False)] PASSED\n\n=================== 5 passed in 0.04 seconds ===================\n</code></pre> <p>Those identifiers can be used to run that specific test. For example <code>pytest -v \"test_func.py::test_add_4[Task(breathe,BRIAN,True)]\"</code>.</p> <p><code>parametrize()</code> can be applied to classes as well.</p> <p>If the test id can't be derived from the parameter value, use the <code>id</code> argument for the <code>pytest.param</code>:</p> <pre><code>@pytest.mark.parametrize('task', [\n    pytest.param(Task('create'), id='just summary'),\n    pytest.param(Task('inspire', 'Michelle'), id='summary/owner'),\n])\ndef test_add_6(task):\n    ...\n</code></pre> <p>Will yield:</p> <pre><code>$ pytest-v test_add_variety.py::test_add_6\n\n=================== test session starts ====================\ncollected 2 items\n\ntest_add_variety.py::test_add_6[justsummary]PASSED\ntest_add_variety.py::test_add_6[summary/owner]PASSED\n\n================= 2 passed in 0.05 seconds =================\n</code></pre>"}, {"location": "pytest_parametrized_testing/#references", "title": "References", "text": "<ul> <li>A deep dive into Pytest parametrization by George Shulkin</li> <li>Book Python Testing with pytest by Brian Okken.</li> </ul>"}, {"location": "python_anti_patterns/", "title": "Python Anti-patterns", "text": ""}, {"location": "python_anti_patterns/#mutable-default-arguments", "title": "Mutable default arguments", "text": ""}, {"location": "python_anti_patterns/#what-you-wrote", "title": "What You Wrote", "text": "<pre><code>def append_to(element, to=[]):\n    to.append(element)\n    return to\n</code></pre>"}, {"location": "python_anti_patterns/#what-you-might-have-expected-to-happen", "title": "What You Might Have Expected to Happen", "text": "<pre><code>my_list = append_to(12)\nprint(my_list)\n\nmy_other_list = append_to(42)\nprint(my_other_list)\n</code></pre> <p>A new list is created each time the function is called if a second argument isn\u2019t provided, so that the output is:</p> <pre><code>[12]\n[42]\n</code></pre> <p>What Does Happen</p> <pre><code>[12]\n[12, 42]\n</code></pre> <p>A new list is created once when the function is defined, and the same list is used in each successive call.</p> <p>Python\u2019s default arguments are evaluated once when the function is defined, not each time the function is called. This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well.</p>"}, {"location": "python_anti_patterns/#what-you-should-do-instead", "title": "What You Should Do Instead", "text": "<p>Create a new object each time the function is called, by using a default arg to signal that no argument was provided (None is often a good choice).</p> <pre><code>def append_to(element, to=None):\n    if to is None:\n        to = []\n    to.append(element)\n    return to\n</code></pre> <p>Do not forget, you are passing a list object as the second argument.</p>"}, {"location": "python_anti_patterns/#when-the-gotcha-isnt-a-gotcha", "title": "When the Gotcha Isn\u2019t a Gotcha", "text": "<p>Sometimes you can specifically \u201cexploit\u201d this behavior to maintain state between calls of a function. This is often done when writing a caching function.</p>"}, {"location": "python_package_management/", "title": "Python Package Management", "text": "<p>Managing Python libraries is a nightmare for most developers.</p> <p>I tried with pip-tools, but I was probably using it wrong. As package management has evolved a lot in the latest years, I'm going to compare Poetry, pipenv, <code>pdm</code> with my current workflow.</p> Tool Stars Forks Latest commit Commits Issues Open/New/Closed PR Open/New/Merged Poetry 17.3k 1.4k 11h 1992 1.1k/58/80 149/13/77 Pipenv 22.5k 1.7k 5d 7226 555/12/54 32/0/22 pdm 1.3k 54 11h 1539 12/3/43 3/2/11 <p>The <code>New</code> and <code>Closed</code> are taken from the Pulse insights of the last month. This data was taken on the 2021-11-30 so it will probably be outdated.</p> <p>Both Poetry and Pipenv are very popular, it looks that <code>Poetry</code> is more alive this last month, but they are both actively developed. <code>pdm</code> is actively developed but at other level.</p> <p>Pipenv has broad support. It is an official project of the Python Packaging Authority, alongside pip. It's also supported by the Heroku Python buildpack, which is useful for anyone with Heroku or Dokku-based deployment strategies.</p> <p>Poetry is a one-stop shop for dependency management and package management. It simplifies creating a package, managing its dependencies, and publishing it. Compared to Pipenv, Poetry's separate add and install commands are more explicit, and it's faster for everything except for a full dependency install.</p>"}, {"location": "python_package_management/#solver", "title": "Solver", "text": "<p>A Solver tries to find a working set of dependencies that all agree with each other. By looking back in time, it\u2019s happy to solve very old versions of packages if newer ones are supposed to be incompatible. This can be helpful, but is slow, and also means you can easily get a very ancient set of packages when you thought you were getting the latest versions.</p> <p>Pip\u2019s solver changed in version 20.3 to become significantly smarter. The old solver would ignore incompatible transitive requirements much more often than the new solver does. This means that an upper cap in a library might have been ignored before, but is much more likely to break things or change the solve now.</p> <p>Poetry has a unique and very strict (and slower) solver that goes even farther hunting for solutions. It forces you to cap Python if a dependency does. One key difference is that Poetry has the original environment specification to work with every time, while pip does not know what the original environment constraints were. This enables Poetry to roll back a dependency on a subsequent solve, while pip does not know what the original requirements were and so does not know if an older package is valid when it encounters a new cap.</p>"}, {"location": "python_package_management/#poetry", "title": "Poetry", "text": "<p>Features I like:</p> <ul> <li>Stores program and development requirements in the <code>pyproject.toml</code>     file.</li> <li>Don't need to manually edit requirements files to add new packages to the     program or dev requirements, simply use <code>poetry add</code>.</li> <li>Easy initialization of the development environment with <code>poetry install</code>.</li> <li> <p>Powerful dependency specification</p> <ul> <li>Installable packages with git dependencies???</li> <li>Easy to specify local directory dependencies, even in editable mode.</li> <li>Specify different dependencies for different python versions</li> </ul> </li> <li> <p>It manage the building of your package, you don't need to manually configure     <code>sdist</code> and <code>wheel</code>.</p> </li> <li>Nice dependency view with <code>poetry show</code>.</li> <li>Nice dependency search interface with <code>poetry search</code>.</li> <li>Sync your environment packages with the lock file.</li> </ul> <p>Things I don't like that much:</p> <ul> <li> <p>It does upper version capping by default, it even ignores your pins and adds     the <code>^&lt;new_version</code> pin if you run <code>poetry add     &lt;package&gt;@latest</code>https://github.com/python-poetry/poetry/issues/3503.     Given that upper version capping is becoming a big problem in     the Python environment I'd stay away from <code>poetry</code>.</p> <p>This is specially useless when you add dependencies that follow CalVer. <code>poetry add</code> packaging will still do <code>^21</code> for the version it adds. You shouldn\u2019t be capping versions, but you really shouldn\u2019t be capping CalVer.</p> <p>It's equally troublesome that it upper pins the python version.</p> </li> <li> <p>Have their own dependency specification format similar to <code>npm</code> and     incompatible with Python's     PEP508.</p> </li> <li> <p>No automatic process to update the dependencies constrains to match the latest     version available.  So if you have constrained a package to be <code>&lt;2.0.0</code> and     <code>3.0.0</code> is out there, you will have to manually edit the <code>pyproject.toml</code> so     that it accepts that new version. At least you can use <code>poetry show     --outdated</code> and it will tell you which is the new version, and if the output     is zero, you're sure you're on the last versions.</p> </li> </ul>"}, {"location": "python_package_management/#pdm", "title": "PDM", "text": "<p>Features I like:</p> <ul> <li>The pin strategy defaults to only add lower     pins helping preventing the upper     capping problem.</li> <li>It can't achieve dependency isolation without virtualenvs.</li> <li>Follows the Python's dependency specification format     PEP508.</li> <li>Supports different strategies to add and update dependencies.</li> <li>Command to update your requirements constrains when updating your packages.</li> <li>Sync your environment packages with the lock file.</li> <li>Easy to install package in editable mode.</li> <li>Easy to install local dependencies.</li> <li>You can force the installation of a package at your own risk even if it breaks     the version constrains. (Useful if you're blocked by a third party upper     bound)</li> <li>Changing the python version is as simple as running <code>python use     &lt;python_version&gt;</code>.</li> <li>Plugin system where adding functionality is feasible (like the <code>publish</code>     subcommand).</li> <li>Both global and local configuration.</li> <li>Nice interface to change the configuration.</li> <li>Automatic management of dependencies cache, where you only have one instance     of each package version, and if no project needs it, it will be removed.</li> <li>Has a nice interface to see the cache usage</li> <li>Has the possibility of managing the global packages too.</li> <li>Allows the definition of scripts possibly removing the need of a makefile</li> <li>It's able to read the version of the program from a file, avoiding the     duplication of the information.</li> <li>You can group your development dependencies in groups.</li> <li>Easy to define extra dependencies for your program.</li> <li>It has sensible defaults for <code>includes</code> and <code>excludes</code> when packaging.</li> <li>It's the fastest      and most     correct     one.</li> </ul> <p>Downsides:</p> <ul> <li>They don't say how to configure your environment to work with     vim.</li> </ul>"}, {"location": "python_package_management/#summary", "title": "Summary", "text": "<p>PDM offers the same features as Poetry with the additions of the possibility of selecting your version capping strategy, and doesn\u2019t cap as badly, and follows more PEP standards.</p>"}, {"location": "python_package_management/#references", "title": "References", "text": "<ul> <li>PDM developer comparison</li> <li>John Franey comparison</li> <li>Frost Ming comparison (developer of PDM)</li> <li>Henry Schreiner analysis on Poetry</li> </ul>"}, {"location": "python_poetry/", "title": "Poetry", "text": "<p>Poetry is a command line program that helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere.</p> <p><code>poetry</code> saves all the information in the <code>pyproject.toml</code> file, including the project development and program dependencies, for example:</p> <pre><code>[tool.poetry]\nname = \"poetry-demo\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"S\u00e9bastien Eustace &lt;sebastien@eustace.io&gt;\"]\n\n[tool.poetry.dependencies]\npython = \"*\"\n\n[tool.poetry.dev-dependencies]\npytest = \"^3.4\"\n</code></pre>"}, {"location": "python_poetry/#installation", "title": "Installation", "text": "<p>Although the official docs tell you to run:</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p><code>pip install poetry</code> works too, which looks safer than executing arbitrary code from an url.</p> <p>To enable shell completion for <code>zsh</code> run:</p> <pre><code># Zsh\npoetry completions zsh &gt; ~/.zfunc/_poetry\n\n# Oh-My-Zsh\nmkdir $ZSH_CUSTOM/plugins/poetry\npoetry completions zsh &gt; $ZSH_CUSTOM/plugins/poetry/_poetry\n</code></pre> <p>For <code>zsh</code>, you must then add the following line in your <code>~/.zshrc</code> before <code>compinit</code>:</p> <pre><code>fpath+=~/.zfunc\n</code></pre> <p>For <code>oh-my-zsh</code>, you must then enable poetry in your <code>~/.zshrc</code> plugins:</p> <pre><code>plugins(\npoetry\n    ...\n    )\n</code></pre>"}, {"location": "python_poetry/#basic-usage", "title": "Basic Usage", "text": ""}, {"location": "python_poetry/#initializing-a-pre-existing-project", "title": "Initializing a pre-existing project", "text": "<p>Instead of creating a new project, Poetry can be used to \u2018initialise\u2019 a pre-populated directory with <code>poetry init</code>. You can use the next options</p> <ul> <li><code>--name</code>: Name of the package.</li> <li><code>--description</code>: Description of the package.</li> <li><code>--author</code>: Author of the package.</li> <li><code>--python</code>: Compatible Python versions.</li> <li><code>--dependency</code>: Package to require with a version constraint. Should be in     format <code>foo:1.0.0</code>.</li> <li><code>--dev-dependency</code>: Development requirements, see <code>--require</code>.</li> </ul>"}, {"location": "python_poetry/#installing-dependencies", "title": "Installing dependencies", "text": "<p>To install the defined dependencies for your project, just run the install command.</p> <pre><code>poetry install\n</code></pre> <p>When you run this command, one of two things may happen:</p> <ul> <li> <p>Installing without poetry.lock: If you have never run the command before and     there is also no <code>poetry.lock</code> file present, Poetry simply resolves all     dependencies listed in your <code>pyproject.toml</code> file and downloads the latest     version of their files.</p> <p>When Poetry has finished installing, it writes all of the packages and the exact versions of them that it downloaded to the <code>poetry.lock</code> file, locking the project to those specific versions. You should commit the <code>poetry.lock</code> file to your project repo so that all people working on the project are locked to the same versions of dependencies.</p> </li> <li> <p>Installing with poetry.lock: If there is already a <code>poetry.lock</code> file as     well as a <code>pyproject.toml</code>, <code>poetry</code> resolves and installs all dependencies     that you listed in <code>pyproject.toml</code>, but Poetry uses the exact versions listed     in <code>poetry.lock</code> to ensure that the package versions are consistent for     everyone working on your project. As a result you will have all dependencies     requested by your <code>pyproject.toml</code> file, but they may not all be at the very     latest available versions (some of the dependencies listed in the     <code>poetry.lock</code> file may have released newer versions since the file was     created). This is by design, it ensures that your project does not break     because of unexpected changes in dependencies.</p> </li> </ul> <p>The current project is installed in editable mode by default.</p> <p>If you don't want the development requirements use the <code>--no-dev</code> flag.</p> <p>To remove the untracked dependencies that are no longer in the lock file, use <code>--remove-untracked</code>.</p>"}, {"location": "python_poetry/#updating-dependencies-to-their-latest-versions", "title": "Updating dependencies to their latest versions", "text": "<p>The <code>poetry.lock</code> file prevents you from automatically getting the latest versions of your dependencies. To update to the latest versions, use the <code>update</code> command. This will fetch the latest matching versions (according to your <code>pyproject.toml</code> file) and update the lock file with the new versions. (This is equivalent to deleting the <code>poetry.lock</code> file and running <code>install</code> again.)</p> <p>The main problem is that <code>poetry add</code> does upper pinning of dependencies by default, which is a really bad idea. And they don't plan to change.</p> <p>There is currently no way of updating your <code>pyproject.toml</code> dependency definitions so they match the latest version beyond your constrains. So if you have constrained a package to be <code>&lt;2.0.0</code> and <code>3.0.0</code> is out there, you will have to manually edit the <code>pyproject.toml</code> so that it accepts that new version.  There is no automatic process that does this. At least you can use <code>poetry show --outdated</code> and it will tell you which is the new version, and if the output is zero, you're sure you're on the last versions.</p> <p>Some workarounds exists though, if you run <code>poetry add dependency@latest</code> it will update the lock to the latest. MousaZeidBaker made poetryup, a tool that is able to update the requirements to the latest version with <code>poetryup --latest</code> (although it still has some bugs). Given that it uses <code>poetry add &lt;package&gt;@latest</code> behind the scenes, it will change your version pin to  <code>^&lt;new_version&gt;</code>, which  as we've seen it's awful.</p> <p>Again, you should not be trying to do this, it's better to improve how you manage your dependencies.</p>"}, {"location": "python_poetry/#debugging-why-a-package-is-not-updated-to-the-latest-version", "title": "Debugging why a package is not updated to the latest version", "text": "<p>Sometimes packages are not updated with <code>poetry update</code> or <code>poetryup</code>, to debug why, you need to understand if some package is setting a constrain that prevents the upgrade. To do that, first check the outdated packages with <code>poetry show -o</code> and for each of them:</p> <ul> <li>Check what packages are using the     dependency.</li> <li>Search if there is an issue asking the maintainers to update their     dependencies, if it doesn't exist, create it.</li> </ul>"}, {"location": "python_poetry/#removing-a-dependency", "title": "Removing a dependency", "text": "<pre><code>poetry remove pendulum\n</code></pre> <p>With the <code>-D</code> or <code>--dev</code> flag, it removes the dependency from the development ones.</p>"}, {"location": "python_poetry/#building-the-package", "title": "Building the package", "text": "<p>Before you can actually publish your library, you will need to package it.</p> <pre><code>poetry build\n</code></pre> <p>This command will package your library in two different formats: <code>sdist</code> which is the source format, and <code>wheel</code> which is a compiled package.</p> <p>Once that\u2019s done you are ready to publish your library.</p>"}, {"location": "python_poetry/#publishing-to-pypi", "title": "Publishing to PyPI", "text": "<p>Poetry will publish to PyPI by default. Anything that is published to PyPI is available automatically through Poetry.</p> <pre><code>poetry publish\n</code></pre> <p>This will package and publish the library to PyPI, at the condition that you are a registered user and you have configured your credentials properly.</p> <p>If you pass the <code>--build</code> flag, it will also build the package.</p>"}, {"location": "python_poetry/#publishing-to-a-private-repository", "title": "Publishing to a private repository", "text": "<p>Sometimes, you may want to keep your library private but also being accessible to your team. In this case, you will need to use a private repository.</p> <p>You will need to add it to your global list of repositories.</p> <p>Once this is done, you can actually publish to it like so:</p> <pre><code>poetry publish -r my-repository\n</code></pre>"}, {"location": "python_poetry/#specifying-dependencies", "title": "Specifying dependencies", "text": "<p>If you want to add dependencies to your project, you can specify them in the <code>tool.poetry.dependencies</code> section.</p> <pre><code>[tool.poetry.dependencies]\npendulum = \"^1.4\"\n</code></pre> <p>As you can see, it takes a mapping of package names and version constraints.</p> <p>Poetry uses this information to search for the right set of files in package \u201crepositories\u201d that you register in the <code>tool.poetry.repositories</code> section, or on PyPI by default.</p> <p>Also, instead of modifying the <code>pyproject.toml</code> file by hand, you can use the add command.</p> <pre><code>poetry add pendulum\n</code></pre> <p>It will automatically find a suitable version constraint and install the package and subdependencies.</p> <p>If you want to add the dependency to the development ones, use the <code>-D</code> or <code>--dev</code> flag.</p>"}, {"location": "python_poetry/#using-your-virtual-environment", "title": "Using your virtual environment", "text": "<p>By default, <code>poetry</code> creates a virtual environment in <code>{cache-dir}/virtualenvs</code>. You can change the <code>cache-dir</code> value by editing the <code>poetry</code> config. Additionally, you can use the <code>virtualenvs.in-project</code> configuration variable to create virtual environment within your project directory.</p> <p>There are several ways to run commands within this virtual environment.</p> <p>To run your script simply use <code>poetry run python your_script.py</code>. Likewise if you have command line tools such as <code>pytest</code> or <code>black</code> you can run them using <code>poetry run pytest</code>.</p> <p>The easiest way to activate the virtual environment is to create a new shell with <code>poetry shell</code>.</p>"}, {"location": "python_poetry/#version-management", "title": "Version Management", "text": "<p><code>poetry version</code> shows the current version of the project. If you pass an argument, it will bump the version of the package, for example <code>poetry version minor</code>. But it doesn't read your commits to decide what kind of bump you apply, so I'd keep on using <code>pip-compile</code>.</p>"}, {"location": "python_poetry/#dependency-specification", "title": "Dependency Specification", "text": "<p>Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.</p> <p>They don't follow Python's specification PEP508</p>"}, {"location": "python_poetry/#caret-requirements", "title": "Caret Requirements", "text": "<p>Caret requirements allow SemVer compatible updates to a specified version. An update is allowed if the new version number does not modify the left-most non-zero digit in the major, minor, patch grouping. In this case, if we ran <code>poetry update requests</code>, <code>poetry</code> would update us to the next versions:</p> Requirement Versions allowed <code>^1.2.3</code> <code>&gt;=1.2.3 &lt;2.0.0</code> <code>^1.2</code> <code>&gt;=1.2.0 &lt;2.0.0</code> <code>^1</code> <code>&gt;=1.0.0 &lt;2.0.0</code> <code>^0.2.3</code> <code>&gt;=0.2.3 &lt;0.3.0</code> <code>^0.0.3</code> <code>&gt;=0.0.3 &lt;0.0.4</code> <code>^0.0</code> <code>&gt;=0.0.0 &lt;0.1.0</code> <code>^0</code> <code>&gt;=0.0.0 &lt;1.0.0</code>"}, {"location": "python_poetry/#tilde-requirements", "title": "Tilde requirements", "text": "<p>Tilde requirements specify a minimal version with some ability to update. If you specify a major, minor, and patch version or only a major and minor version, only patch-level changes are allowed. If you only specify a major version, then minor- and patch-level changes are allowed.</p> Requirement Versions allowed <code>~1.2.3</code> <code>&gt;=1.2.3 &lt;1.3.0</code> <code>~1.2</code> <code>&gt;=1.2.0 &lt;1.3.0</code> <code>~1</code> <code>&gt;=1.0.0 &lt;2.0.0</code>"}, {"location": "python_poetry/#wildcard-requirements", "title": "Wildcard requirements", "text": "<p>Wildcard requirements allow for the latest (dependency dependent) version where the wildcard is positioned.</p> Requirement Versions allowed <code>*</code> <code>&gt;=0.0.0</code> <code>1.*</code> <code>&gt;=1.0.0 &lt;2.0.0</code> <code>1.2.*</code> <code>&gt;=1.2.0 &lt;1.3.0</code>"}, {"location": "python_poetry/#inequality-requirements", "title": "Inequality requirements", "text": "<p>Inequality requirements allow manually specifying a version range or an exact version to depend on.</p> <p>Here are some examples of inequality requirements:</p> <pre><code>&gt;= 1.2.0\n&gt; 1\n&lt; 2\n!= 1.2.3\n</code></pre>"}, {"location": "python_poetry/#exact-requirements", "title": "Exact requirements", "text": "<p>You can specify the exact version of a package. This will tell Poetry to install this version and this version only. If other dependencies require a different version, the solver will ultimately fail and abort any install or update procedures.</p> <p>Multiple version requirements can also be separated with a comma, e.g. <code>&gt;= 1.2, &lt; 1.5</code>.</p>"}, {"location": "python_poetry/#git-dependencies", "title": "git dependencies", "text": "<p>To depend on a library located in a git repository, the minimum information you need to specify is the location of the repository with the git key:</p> <pre><code>[tool.poetry.dependencies]\nrequests = { git = \"https://github.com/requests/requests.git\" }\n</code></pre> <p>Since we haven\u2019t specified any other information, Poetry assumes that we intend to use the latest commit on the <code>master</code> branch to build our project.</p> <p>You can combine the git key with the branch key to use another branch. Alternatively, use <code>rev</code> or <code>tag</code> to pin a dependency to a specific commit hash or tagged ref, respectively. For example:</p> <pre><code>[tool.poetry.dependencies]\n# Get the latest revision on the branch named \"next\"\nrequests = { git = \"https://github.com/kennethreitz/requests.git\", branch = \"next\" }\n# Get a revision by its commit hash\nflask = { git = \"https://github.com/pallets/flask.git\", rev = \"38eb5d3b\" }\n# Get a revision by its tag\nnumpy = { git = \"https://github.com/numpy/numpy.git\", tag = \"v0.13.2\" }\n</code></pre> <p>When using <code>poetry add</code> you can add:</p> <ul> <li>A https cloned repo: <code>poetry add     git+https://github.com/sdispater/pendulum.git</code></li> <li>A ssh cloned repo: <code>poetry add     git+ssh://git@github.com/sdispater/pendulum.git</code></li> </ul> <p>If you need to checkout a specific branch, tag or revision, you can specify it when using add:</p> <pre><code>poetry add git+https://github.com/sdispater/pendulum.git#develop\npoetry add git+https://github.com/sdispater/pendulum.git#2.0.5\n</code></pre>"}, {"location": "python_poetry/#path-dependencies", "title": "path dependencies", "text": "<p>To depend on a library located in a local directory or file, you can use the path property:</p> <pre><code>[tool.poetry.dependencies]\n# directory\nmy-package = { path = \"../my-package/\", develop = false }\n\n# file\nmy-package = { path = \"../my-package/dist/my-package-0.1.0.tar.gz\" }\n</code></pre> <p>When using <code>poetry add</code>, you can point them directly to the package or the file:</p> <pre><code>poetry add ./my-package/\npoetry add ../my-package/dist/my-package-0.1.0.tar.gz\npoetry add ../my-package/dist/my_package-0.1.0.whl\n</code></pre> <p>If you want the dependency to be installed in editable mode you can specify it in the <code>pyproject.toml</code> file. It means that changes in the local directory will be reflected directly in environment.</p> <pre><code>[tool.poetry.dependencies]\nmy-package = {path = \"../my/path\", develop = true}\n</code></pre>"}, {"location": "python_poetry/#url-dependencies", "title": "url dependencies", "text": "<p>To depend on a library located on a remote archive, you can use the url property:</p> <pre><code>[tool.poetry.dependencies]\n# directory\nmy-package = { url = \"https://example.com/my-package-0.1.0.tar.gz\" }\n</code></pre> <p>With the corresponding add call:</p> <pre><code>poetry add https://example.com/my-package-0.1.0.tar.gz\n</code></pre>"}, {"location": "python_poetry/#python-restricted-dependencies", "title": "Python restricted dependencies", "text": "<p>You can also specify that a dependency should be installed only for specific Python versions:</p> <pre><code>[tool.poetry.dependencies]\npathlib2 = { version = \"^2.2\", python = \"~2.7\" }\n\n[tool.poetry.dependencies]\npathlib2 = { version = \"^2.2\", python = \"~2.7 || ^3.2\" }\n</code></pre>"}, {"location": "python_poetry/#multiple-constraints-dependencies", "title": "Multiple constraints dependencies", "text": "<p>Sometimes, one of your dependency may have different version ranges depending on the target Python versions.</p> <p>Let\u2019s say you have a dependency on the package <code>foo</code> which is only compatible with Python <code>&lt;3.0</code> up to version <code>1.9</code> and compatible with Python <code>3.4+</code> from version <code>2.0</code>. You would declare it like so:</p> <pre><code>[tool.poetry.dependencies]\nfoo = [\n    {version = \"&lt;=1.9\", python = \"^2.7\"},\n    {version = \"^2.0\", python = \"^3.4\"}\n]\n</code></pre>"}, {"location": "python_poetry/#show-the-available-packages", "title": "Show the available packages", "text": "<p>To list all of the available packages, you can use the show command.</p> <pre><code>poetry show\n</code></pre> <p>If you want to see the details of a certain package, you can pass the package name.</p> <pre><code>poetry show pendulum\n\nname        : pendulum\nversion     : 1.4.2\ndescription : Python datetimes made easy\n\ndependencies:\n - python-dateutil &gt;=2.6.1\n - tzlocal &gt;=1.4\n - pytzdata &gt;=2017.2.2\n</code></pre> <p>By default it will print all the dependencies, if you pass <code>--no-dev</code> it will only show your package's ones.</p> <p>With the <code>-l</code> or <code>--latest</code> it will show the latest version of the packages, and with <code>-o</code> or <code>--outdated</code> it will show the latest version but only for packages that are outdated.</p>"}, {"location": "python_poetry/#search-for-dependencies", "title": "Search for dependencies", "text": "<p>This command searches for packages on a remote index.</p> <pre><code>poetry search requests pendulum\n</code></pre>"}, {"location": "python_poetry/#export-requirements-to", "title": "[Export requirements to", "text": "<p>requirements.txt](https://python-poetry.org/docs/cli/#export)</p> <pre><code>poetry export -f requirements.txt --output requirements.txt\n</code></pre>"}, {"location": "python_poetry/#project-setup", "title": "Project setup", "text": "<p>If you don't already have a cookiecutter for your python projects, you can use <code>poetry new poetry-demo</code>, and it will create the <code>poetry-demo</code> directory with the following content:</p> <pre><code>poetry-demo\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.rst\n\u251c\u2500\u2500 poetry_demo\n\u2502   \u2514\u2500\u2500 __init__.py\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 test_poetry_demo.py\n</code></pre> <p>If you want to use the <code>src</code> project structure, pass the <code>--src</code> flag.</p>"}, {"location": "python_poetry/#checking-what-package-is-using-a-dependency", "title": "Checking what package is using a dependency", "text": "<p>Even though <code>poetry</code> is supposed to show the information of which packages depend on a specific package with <code>poetry show package</code>, I don't see it.</p> <p>Luckily snejus made a small script that shows the information. Save it somewhere in your <code>PATH</code>.</p> <pre><code>_RED='\\\\\\\\e[1;31m&amp;\\\\\\\\e[0m'\n_GREEN='\\\\\\\\e[1;32m&amp;\\\\\\\\e[0m'\n_YELLOW='\\\\\\\\e[1;33m&amp;\\\\\\\\e[0m'\n_format () {\ntr -d '\"' |\nsed \"s/ \\+&gt;[^ ]* \\+&lt;.*/$_YELLOW/\" | # ~ / ^ / &lt; &gt;= ~ a window\nsed \"s/ \\+&gt;[^ ]* *$/$_GREEN/\" |     # &gt;= no upper limit\nsed \"/&gt;/ !s/&lt;.*$/$_RED/\" |          # &lt; ~ upper limit\nsed \"/&gt;\\|&lt;/ !s/ .*/  $_RED/\"        # == ~ locked version\n}\n\n_requires () {\nsed -n \"/^name = \\\"$1\\\"/I,/\\[\\[package\\]\\]/{\n                /\\[package.dep/,/^$/{\n                    /^[^[]/ {\n                        s/= {version = \\(\\\"[^\\\"]*\\\"\\).*/, \\1/p;\n                        s/ =/,/gp\n             }}}\" poetry.lock |\nsed \"/,.*,/!s/&lt;/,&lt;/; s/^[^&lt;]\\+$/&amp;,/\" |\ncolumn -t -s , | _format\n}\n\n_required_by () {\nsed -n \"/\\[metadata\\]/,//d;\n            /\\[package\\]\\|\\[package\\.depen/,/^$/H;\n            /^name\\|^$1 = /Ip\" poetry.lock |\nsed -n \"/^$1/I{x;G;p};h\" |\nsed 's/.*\"\\(.*\\)\".*/\\1/' |\nsed '$!N;s/\\n/ /' |\ncolumn -t | _format\n}\n\ndeps() {\necho\necho -e \"\\e[1mREQUIRES\\e[0m\"\n_requires \"$1\" | xargs -i echo -e \"\\t{}\"\necho\necho -e \"\\e[1mREQUIRED BY\\e[0m\"\n_required_by \"$1\" | xargs -i echo -e \"\\t{}\"\necho\n}\n\ndeps $1\n</code></pre>"}, {"location": "python_poetry/#configuration", "title": "Configuration", "text": "<p>Poetry can be configured via the <code>config</code> command (see more about its usage here) or directly in the <code>config.toml</code> file that will be automatically be created when you first run that command. This file can typically be found in <code>~/.config/pypoetry</code>.</p> <p>Poetry also provides the ability to have settings that are specific to a project by passing the <code>--local</code> option to the config command.</p> <pre><code>poetry config virtualenvs.create false --local\n</code></pre>"}, {"location": "python_poetry/#list-the-current-configuration", "title": "List the current configuration", "text": "<p>To list the current configuration you can use the <code>--list</code> option of the <code>config</code> command:</p> <pre><code>poetry config --list\n</code></pre> <p>Which will give you something similar to this:</p> <pre><code>cache-dir = \"/path/to/cache/directory\"\nvirtualenvs.create = true\nvirtualenvs.in-project = null\nvirtualenvs.path = \"{cache-dir}/virtualenvs\"  # /path/to/cache/directory/virtualenvs\n</code></pre>"}, {"location": "python_poetry/#adding-or-updating-a-configuration-setting", "title": "Adding or updating a configuration setting", "text": "<p>To change or otherwise add a new configuration setting, you can pass a value after the setting\u2019s name:</p> <pre><code>poetry config virtualenvs.path /path/to/cache/directory/virtualenvs\n</code></pre> <p>For a full list of the supported settings see Available settings.</p>"}, {"location": "python_poetry/#removing-a-specific-setting", "title": "Removing a specific setting", "text": "<p>If you want to remove a previously set setting, you can use the <code>--unset</code> option:</p> <pre><code>poetry config virtualenvs.path --unset\n</code></pre>"}, {"location": "python_poetry/#adding-a-repository", "title": "Adding a repository", "text": "<p>Adding a new repository is easy with the <code>config</code> command.</p> <pre><code>poetry config repositories.foo https://foo.bar/simple/\n</code></pre> <p>This will set the url for repository <code>foo</code> to <code>https://foo.bar/simple/</code>.</p>"}, {"location": "python_poetry/#configuring-credentials", "title": "Configuring credentials", "text": "<p>If you want to store your credentials for a specific repository, you can do so easily:</p> <pre><code>poetry config http-basic.foo username password\n</code></pre> <p>If you do not specify the password you will be prompted to write it.</p> <p>To publish to PyPI, you can set your credentials for the repository named <code>pypi</code>.</p> <p>Note that it is recommended to use API tokens when uploading packages to PyPI. Once you have created a new token, you can tell Poetry to use it:</p> <pre><code>poetry config pypi-token.pypi my-token\n</code></pre> <p>If a system keyring is available and supported, the password is stored to and retrieved from the keyring. In the above example, the credential will be stored using the name <code>poetry-repository-pypi</code>. If access to keyring fails or is unsupported, this will fall back to writing the password to the <code>auth.toml</code> file along with the username.</p> <p>Keyring support is enabled using the keyring library. For more information on supported backends refer to the library documentation. It doesn't support pass by default, but Steffen Vogel created a specific keyring backend. Alternatively, you can use environment variables to provide the credentials:</p> <pre><code>export POETRY_PYPI_TOKEN_PYPI=my-token\nexport POETRY_HTTP_BASIC_PYPI_USERNAME=username\nexport POETRY_HTTP_BASIC_PYPI_PASSWORD=password\n</code></pre> <p>I've tried setting up the keyring but I get the next error:</p> <pre><code>  UploadError\n\n  HTTP Error 403: Invalid or non-existent authentication information. See https://pypi.org/help/#invalid-auth for more information.\n\n  at ~/.venvs/autodev/lib/python3.9/site-packages/poetry/publishing/uploader.py:216 in _upload\n      212\u2502                     self._register(session, url)\n      213\u2502                 except HTTPError as e:\n      214\u2502                     raise UploadError(e)\n      215\u2502\n    \u2192 216\u2502             raise UploadError(e)\n      217\u2502\n      218\u2502     def _do_upload(\n      219\u2502         self, session, url, dry_run=False\n      220\u2502     ):  # type: (requests.Session, str, Optional[bool]) -&gt; None\n</code></pre> <p>The keyring was configured with:</p> <pre><code>poetry config pypi-token.pypi internet/pypi.token\n</code></pre> <p>And I'm sure that the keyring works because <code>python -m keyring get internet pypi.token</code> works.</p> <p>I've also tried with the environmental variable <code>POETRY_PYPI_TOKEN_PYPI</code> but it didn't work either. And setting the configuration as <code>poetry config http-basic.pypi __token__ internet/pypi.token</code>.</p> <p>Finally I had to hardcode the token with <code>poetry config pypi-token.pypi \"$(pass show internet/pypi.token)</code>. Although I can't find where it's storing the value :S.</p>"}, {"location": "python_poetry/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "renovate/", "title": "Renovate", "text": "<p>Renovate is a program that does automated dependency updates. Multi-platform and multi-language.</p> <p>Why use Renovate?</p> <ul> <li>Get pull requests to update your dependencies and lock files.</li> <li>Reduce noise by scheduling when Renovate creates PRs.</li> <li>Renovate finds relevant package files automatically, including in monorepos.</li> <li>You can customize the bot's behavior with configuration files.</li> <li>Share your configuration with ESLint-like config presets.</li> <li>Get replacement PRs to migrate from a deprecated dependency to the community     suggested replacement (npm packages only).</li> <li>Open source.</li> <li>Popular (more than 9.7k stars and 1.3k forks)</li> <li>Beautifully integrate with main Git web applications (Gitea, Gitlab, Github).</li> <li>It supports most important languages: Python, Docker, Kubernetes, Terraform,     Ansible, Node, ...</li> </ul>"}, {"location": "renovate/#behind-the-scenes", "title": "Behind the scenes", "text": ""}, {"location": "renovate/#how-renovate-updates-a-package-file", "title": "How Renovate updates a package file", "text": "<p>Renovate:</p> <ul> <li>Scans your repositories to detect package files and their dependencies.</li> <li>Checks if any newer versions exist.</li> <li>Raises Pull Requests for available updates.</li> </ul> <p>The Pull Requests patch the package files directly, and include Release Notes for the newer versions (if they are available).</p> <p>By default:</p> <ul> <li>You'll get separate Pull Requests for each dependency.</li> <li>Major updates are kept separate from non-major updates.</li> </ul>"}, {"location": "renovate/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "sanoid/", "title": "Sanoid", "text": "<p>Sanoid is the most popular tool right now, with it you can create, automatically thin, and monitor snapshots and pool health from a single eminently human-readable TOML config file at <code>/etc/sanoid/sanoid.conf</code>. Sanoid also requires a \"defaults\" file located at /etc/sanoid/sanoid.defaults.conf, which is not user-editable. A typical Sanoid system would have a single cron job:</p> <p><pre><code>* * * * * TZ=UTC /usr/local/bin/sanoid --cron\n</code></pre> Note: Using UTC as timezone is recommend to prevent problems with daylight saving times</p> <p>And its <code>/etc/sanoid/sanoid.conf</code> might look something like this:</p> <pre><code>[data/home]\nuse_template = production\n[data/images]\nuse_template = production\nrecursive = yes\nprocess_children_only = yes\n[data/images/win7]\nhourly = 4\n\n#############################\n# templates below this line #\n#############################\n\n[template_production]\nfrequently = 0\nhourly = 36\ndaily = 30\nmonthly = 3\nyearly = 0\nautosnap = yes\nautoprune = yes\n</code></pre> <p>Which would be enough to tell <code>sanoid</code> to take and keep 36 hourly snapshots, 30 dailies, 3 monthlies, and no yearlies for all datasets under <code>data/images</code> (but not <code>data/images</code> itself, since <code>process_children_only</code> is set). Except in the case of <code>data/images/win7</code>, which follows the same template (since it's a child of <code>data/images</code>) but only keeps 4 hourlies for whatever reason.</p> <p>For more full details on sanoid.conf settings see their wiki page</p> <p>The monitorization is designed to be done with Nagios, although there is some work in progress to add Prometheus metrics and there is an exporter</p> <p>What I like of <code>sanoid</code>:</p> <ul> <li>It's popular</li> <li>It has hooks to run your scripts at various stages in the lifecycle of a snapshot.</li> <li>It also handles the process of sending the backups to other locations with <code>syncoid</code></li> <li>It lets you search on all changes of a given file (or folder) over all available snapshots. This is useful in case you need to recover a file or folder but don't want to rollback an entire snapshot. with <code>findoid</code> (although when I used it it gave me an error :/)</li> <li>It's in the official repos</li> </ul> <p>What I don't like:</p> <ul> <li>Last release is almost 2 years ago.</li> <li>The last commit to <code>master</code> is done a year ago.</li> <li>It's made in Perl</li> </ul>"}, {"location": "sanoid/#installation", "title": "Installation", "text": "<p>The tool is in the official repositories so:</p> <pre><code>sudo apt-get install sanoid\n</code></pre> <p>You can find the example config file at <code>/usr/share/doc/sanoid/examples/sanoid.conf</code> and can copy it to <code>/etc/sanoid/sanoid.conf</code></p> <pre><code>mkdir /etc/sanoid/\ncp /usr/share/doc/sanoid/examples/sanoid.conf /etc/sanoid/sanoid.conf\ncp /usr/share/sanoid/sanoid.defaults.conf /etc/sanoid/sanoid.defaults.conf\n</code></pre> <p>Edit <code>/etc/sanoid/sanoid.conf</code> to suit your needs. The <code>/etc/sanoid/sanoid.defaults.conf</code> file contains the default values and should not be touched, use it only for reference.</p> <p>An example of a configuration can be:</p> <pre><code>######################\n# Filesystem Backups #\n######################\n\n[main/backup]\nuse_template = daily\n[main/lyz]\nuse_template = frequent\n\n#############\n# Templates #\n#############\n\n[template_daily]\ndaily = 30\nmonthly = 6\n\n[template_frequent]\nfrequently = 4\nhourly = 25\ndaily = 30\nmonthly = 6\n</code></pre> <p>During installation from the Debian repositories, the <code>systemd</code> timer unit <code>sanoid.timer</code> is created which is set to run <code>sanoid</code> every 15 minutes. Therefore there is no need to create an entry in crontab. Having a crontab entry in addition to the <code>sanoid.timer</code> will result in errors similar to <code>cannot create snapshot '&lt;pool&gt;/&lt;dataset&gt;@&lt;snapshot&gt;': dataset already exists</code>.</p> <p>By default, the <code>sanoid.timer</code> timer unit runs the <code>sanoid-prune</code> service followed by the <code>sanoid</code> service. To edit any of the command-line options, you can edit these service files (<code>/lib/systemd/system/sanoid.timer</code>).</p>"}, {"location": "sanoid/#usage", "title": "Usage", "text": "<p><code>sanoid</code> runs in the back with the <code>systemd</code> service, so there is nothing you need to do for it to run.</p> <p>To check the logs use <code>journalctl -eu sanoid</code>.</p> <p>To manage the snapshots look at the <code>zfs</code> article.</p>"}, {"location": "sanoid/#syncoid", "title": "Syncoid", "text": "<p><code>Sanoid</code> also includes a replication tool, <code>syncoid</code>, which facilitates the asynchronous incremental replication of ZFS filesystems. A typical <code>syncoid</code> command might look like this:</p> <pre><code>syncoid data/images/vm backup/images/vm\n</code></pre> <p>Which would replicate the specified ZFS filesystem (aka dataset) from the data pool to the backup pool on the local system, or</p> <pre><code>syncoid data/images/vm root@remotehost:backup/images/vm\n</code></pre> <p>Which would push-replicate the specified ZFS filesystem from the local host to remotehost over an SSH tunnel, or</p> <pre><code>syncoid root@remotehost:data/images/vm backup/images/vm\n</code></pre> <p>Which would pull-replicate the filesystem from the remote host to the local system over an SSH tunnel.</p> <p><code>Syncoid</code> supports recursive replication (replication of a dataset and all its child datasets) and uses mbuffer buffering, lzop compression, and pv progress bars if the utilities are available on the systems used. If ZFS supports resumeable send/receive streams on both the source and target those will be enabled as default. It also automatically supports and enables resume of interrupted replication when both source and target support this feature.</p>"}, {"location": "sanoid/#configuration", "title": "Configuration", "text": ""}, {"location": "sanoid/#syncoid-configuration-caveats", "title": "Syncoid configuration caveats", "text": "<p>One key point is that pruning is not done by <code>syncoid</code> but only and always by <code>sanoid</code>. This means <code>sanoid</code> has to be run on the backup datasets as well, but without creating snapshots, only pruning (as set in the template).</p> <p>Also, the template is called <code>template_something</code> and only <code>something</code> must be use with <code>use_template</code>.</p> <p><pre><code>[SAN200/projects]\nuse_template = production\nrecursive = yes\nprocess_children_only = yes\n\n[BACKUP/SAN200/projects]\nuse_template = backup\nrecursive = yes\nprocess_children_only = yes\n</code></pre> Also note that <code>post_snapshot_script</code> cannot be used with <code>syncoid</code> especially with <code>recursive = yes</code>. This is because there cannot be two zfs send and receive at the same time on the same dataset.</p> <p><code>sanoid</code> does not wait for the script completion before continuing. This mean that should the <code>syncoid</code> process take a bit too much time, a new one will be spawned. And for reasons unknown to me yet, a new syncoid process will cancel the previous one (instead of just leaving). As some of the spawned <code>syncoid</code> will produce errors, the entire <code>sanoid</code> process will fail.</p> <p>So this approach does not work and has to be done independently, it seems. The good news is that the SystemD service of <code>Type= oneshot</code> can have several <code>Execstart=</code> lines.</p>"}, {"location": "sanoid/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "sanoid/#syncoid-no-tty-present-and-no-askpass-program-specified", "title": "Syncoid no tty present and no askpass program specified", "text": "<p>If you try to just sync a ZFS dataset between two machines, something like <code>syncoid pool/dataset user@remote:pool/dataset</code>, you\u2019ll eventually see <code>syncoid</code> throwing a sudo error: <code>sudo: no tty present and no askpass program specified</code>. That\u2019s because it\u2019s trying to run a sudo command on the remote, and sudo doesn\u2019t have a way to ask for a password with the way <code>syncoid</code>\u2019s running commands in the remote.</p> <p>Searching online, many people just saying to enable SSH as root, which might be fine on a local network, but not the best solution. Instead, you can enable passwordless <code>sudo</code> for <code>zfs</code> commands on a unprivileged user. Getting this done was very simple:</p> <pre><code>sudo visudo /etc/sudoers.d/zfs_receive_for_syncoid\n</code></pre> <p>And then fill it with the following:</p> <pre><code>&lt;your user&gt; ALL=NOPASSWD: /usr/sbin/zfs *\n</code></pre> <p>If you really want to put in the effort, you can even take a look at which <code>zfs</code> commands that <code>syncoid</code> is actually invoking, and then restrict passwordless sudo only for those commands. It\u2019s important that you do this for all commands that <code>syncoid</code> uses. Syncoid runs a few <code>zfs</code> commands with sudo to list snapshots and get some other information on the remote machine before doing the transfer.</p>"}, {"location": "sanoid/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Docs</li> </ul>"}, {"location": "semantic_versioning/", "title": "Semantic Versioning", "text": "<p>Semantic Versioning is a way to define your program's version based on the type of changes you've introduced. It's defined as a three-number string (separated with a period) in the format of <code>MAJOR.MINOR.PATCH</code>.</p> <p>Usually, it starts with 0.0.0. Then depending on the type of change you make to the library, you increment one of these and set subsequent numbers to zero:</p> <ul> <li><code>MAJOR</code> version if you make backward-incompatible changes.</li> <li><code>MINOR</code> version if you add a new feature.</li> <li><code>PATCH</code> version if you fix bugs.</li> </ul> <p>The version number in this context is used as a contract between the library developer and the systems pulling it in about how freely they can upgrade. For example, if you wrote your web server against <code>Django 3</code>, you should be good to go with all <code>Django 3</code> releases that are at least as new as your current one. This allows you to express your Django dependency in the format of <code>Django &gt;= 3.0.2, &lt;4</code>.</p> <p>In addition, we have to take into account the following considerations:</p> <ul> <li>A normal version number MUST take the form X.Y.Z where X, Y, and Z are   non-negative integers, and MUST NOT contain leading zeroes.</li> <li>Once a versioned package has been released, the contents of that version MUST   NOT be modified. Any modifications MUST be released as a new version.</li> <li>Major version zero (0.y.z) is for initial development. Anything may change at   any time. The public API should not be considered stable. But don't fall into   using ZeroVer instead.</li> <li>Releasing the version 1.0.0 is a declaration of intentions to your users that     the code is to be considered stable.</li> <li>Patch version Z (x.y.Z | x &gt; 0) MUST be incremented if only backwards   compatible bug fixes are introduced. A bug fix is defined as an internal   change that fixes incorrect behavior.</li> <li>Minor version Y (x.Y.z | x &gt; 0) MUST be incremented if new, backwards   compatible functionality is introduced to the public API. It MUST be   incremented if any public API functionality is marked as deprecated. It MAY be   incremented if substantial new functionality or improvements are introduced   within the private code. It MAY include patch level changes. Patch version   MUST be reset to 0 when minor version is incremented.</li> <li>Major version X (X.y.z | X &gt; 0) MUST be incremented if any backwards   incompatible changes are introduced to the public API. It MAY include minor   and patch level changes. Patch and minor version MUST be reset to 0 when major   version is incremented.</li> </ul> <p>!!! note \"Encoding this information in the version is just an extremely lossy, but very fast to parse and interpret, which may lead into issues</p> <p>By using this format whenever you rebuild your application, you\u2019ll automatically pull in any new feature/bugfix/security releases of Django, enabling you to use the latest and best version that still in theory guarantees to works with your project.</p> <p>This is great because:</p> <ul> <li>You enable automatic, compatible security fixes.</li> <li>It automatically pulls in bug fixes on the library side.</li> <li>Your application will keep building and working in the future as it did today     because the significant version pin protects you from pulling in versions     whose API would not match.</li> </ul>"}, {"location": "semantic_versioning/#commit-message-guidelines", "title": "Commit message guidelines", "text": "<p>If you like the idea behind Semantic Versioning, it makes sense to follow the Angular commit convention to automate the changelog maintenance and the program version bumping.</p> <p>Each commit message consists of a header, a body and a footer. The header has a defined format that includes a type, a scope and a subject:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n&lt;BLANK LINE&gt;\n&lt;body&gt;\n&lt;BLANK LINE&gt;\n&lt;footer&gt;\n</code></pre> <p>The header is mandatory and the scope of the header is optional.</p> <p>Any line of the commit message cannot be longer 100 characters.</p> <p>The footer could contain a closing reference to an issue.</p> <p>Samples:</p> <pre><code>docs(changelog): update changelog to beta.5\n\nfix(release): need to depend on latest rxjs and zone.js\n\nThe version in our package.json gets copied to the one we publish, and users need the latest of these.\n\ndocs(router): fix typo 'containa' to 'contains' (#36764)\n\nCloses #36763\n\nPR Close #36764\n</code></pre>"}, {"location": "semantic_versioning/#change-types", "title": "Change types", "text": "<p>Must be one of the following:</p> <ul> <li><code>feat</code>: A new feature.</li> <li><code>fix</code>: A bug fix.</li> <li><code>test</code>: Adding missing tests or correcting existing tests.</li> <li><code>docs</code>: Documentation changes.</li> <li><code>chore</code>: A package maintenance change such as updating the requirements.</li> <li><code>bump</code>: A commit to mark the increase of the version number.</li> <li><code>style</code>: Changes that do not affect the meaning of the code (white-space,     formatting, missing semi-colons, etc).</li> <li><code>ci</code>: Changes to our CI configuration files and scripts.</li> <li><code>perf</code>: A code change that improves performance.</li> <li><code>refactor</code>: A code change that neither fixes a bug nor adds a feature.</li> <li><code>build</code>: Changes that affect the build system or external dependencies.</li> </ul>"}, {"location": "semantic_versioning/#subject", "title": "Subject", "text": "<p>The subject contains a succinct description of the change:</p> <ul> <li>Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\".</li> <li>Don't capitalize the first letter.</li> <li>No dot (.) at the end.</li> </ul>"}, {"location": "semantic_versioning/#body", "title": "Body", "text": "<p>Same as in the subject, use the imperative present tense. The body should include the motivation for the change and contrast this with previous behavior.</p>"}, {"location": "semantic_versioning/#footer", "title": "Footer", "text": "<p>The footer should contain any information about Breaking Changes and is also the place to reference issues that this commit Closes.</p> <p>Breaking Changes should start with the word <code>BREAKING CHANGE:</code> with a space or two newlines. The rest of the commit message is then used for this.</p>"}, {"location": "semantic_versioning/#revert", "title": "Revert", "text": "<p>If the commit reverts a previous commit, it should begin with <code>revert:</code> , followed by the header of the reverted commit. In the body it should say: <code>This reverts commit &lt;hash&gt;.</code>, where the hash is the SHA of the commit to revert.</p>"}, {"location": "semantic_versioning/#helpers", "title": "Helpers", "text": ""}, {"location": "semantic_versioning/#use-tool-to-bump-your-program-version", "title": "Use tool to bump your program version", "text": "<p>You can use the commitizen tool to:</p> <ul> <li>Automatically detect which type of change you're introducing and decide     which should be the next version number.</li> <li>Update the changelog</li> </ul> <p>By running <code>cz bump --changelog --no-verify</code>.</p> <p>The <code>--no-verify</code> part is required if you use pre-commit hooks.</p> <p>Whenever you want to release <code>1.0.0</code>, use <code>cz bump --changelog --no-verify --increment MAJOR</code>. If you are on a version <code>0.X.Y</code>, and you introduced a breaking change but don't want to upgrade to <code>1.0.0</code>, use the <code>--increment MINOR</code> flag.</p>"}, {"location": "semantic_versioning/#use-tool-to-create-the-commit-messages", "title": "Use tool to create the commit messages", "text": "<p>To get used to make correct commit messages, you can use the commitizen tool, that guides you through the steps of making a good commit message. Once you're used to the system though, it makes more sense to ditch the tool and write the messages yourself.</p> <p>In Vim, if you're using Vim fugitive you can change the configuration to:</p> <pre><code>nnoremap &lt;leader&gt;gc :terminal cz c&lt;CR&gt;\nnnoremap &lt;leader&gt;gr :terminal cz c --retry&lt;CR&gt;\n\n\" Open terminal mode in insert mode\nif has('nvim')\n    autocmd TermOpen term://* startinsert\nendif\nautocmd BufLeave term://* stopinsert\n</code></pre> <p>If some pre-commit hook fails, make the changes and then use <code>&lt;leader&gt;gr</code> to repeat the same commit message.</p>"}, {"location": "semantic_versioning/#pre-commit", "title": "Pre-commit", "text": "<p>To ensure that your project follows these guidelines, add the following to your pre-commit configuration:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>- repo: https://github.com/commitizen-tools/commitizen\nrev: master\nhooks:\n- id: commitizen\nstages: [commit-msg]\n</code></pre>"}, {"location": "semantic_versioning/#when-to-do-a-major-release", "title": "When to do a major release", "text": "<p>Following the Semantic Versioning idea of a major update is problematic because:</p> <ul> <li>You can quickly get into the high version number problem.</li> <li>The fact that any change may break the users     code makes the definition of when a change should be major     blurry.</li> <li>Often the change that triggered the major change only affects a low percentage     of your users (usually those using that one feature you changed in an     incompatible fashion).</li> </ul> <p>Does dropping Python 2 require a major release? Many (most) packages did this, but the general answer is ironically no, it is not an addition or a breaking change, the version solver will ensure the correct version is used (unless the <code>Requires-Python</code> metadata slot is empty or not updated).</p> <p>If you mark a feature as deprecated (almost always in a minor release), you can remove that feature in a future minor release. You have to define in your library documentation what the deprecation period is. For example, NumPy and Python use three minor releases. Sometimes is useful to implement deprecations based on a period of time. SemVer purists argue that this makes minor releases into major releases, but as we've seen it\u2019s not that simple. The deprecation period ensures the \u201cnext\u201d version works, which is really useful, and usually gives you time to adjust before the removal happens. It\u2019s a great balance for projects that are well kept up using libraries that move forward at a reasonable pace. If you make sure you can see deprecations, you will almost always work with the next several versions.</p>"}, {"location": "semantic_versioning/#semantic-versioning-system-problems", "title": "Semantic versioning system problems", "text": "<p>On paper, semantic versioning seems to be addressing all we need to encode the evolution and state of our library. When implementation time comes some issues are raised though.</p> <p>!!! note \"The pitfalls mentioned below don't invalidate the Semantic Versioning system, you just need to be aware of them.\"</p>"}, {"location": "semantic_versioning/#maintaining-different-versions", "title": "Maintaining different versions", "text": "<p>Version numbers are just a mapping of a sequence of digits to our branching strategy in source control. For instance, if you are doing SemVer then your <code>X.Y.Z</code> version maps a branch to <code>X.Y</code> branch where you're doing your current feature work, an <code>X.Y.Z+1</code> branch for any bugfixes, and potentially an <code>X+1.0.0</code> branch where you doing some crazy new stuff. So you got your next branch, main branch, and bugfix branch. And all three of those branches are alive and receiving updates.</p> <p>For projects that have those 3 kinds of branches going, the concept of SemVer makes much more sense, but how many projects are doing that? You have to be a pretty substantial project typically to have the throughput to justify that much project overhead.</p> <p>There are a lot more projects that have a single <code>bugfix</code> branch and a <code>main</code> branch which has all feature work, whether it be massively backwards-incompatible or not. In that case why carry around two version numbers? This is how you end up with ZeroVer. If you're doing that why not just drop a digit and have your version be <code>X.Y</code>? PEP 440 supports it, and it would more truthfully represent your branching strategy appropriately in your version number. However, most library maintainers/developers out there don\u2019t have enough resources to maintain even two branches.</p> <p>Maintaining a library is very time-consuming, and most libraries have just a few active maintainers available that maintain other many libraries. To complicate matters even further, for most maintainers this is not a full-time job, but something on the side, part of their free time.</p> <p>Given the scarce human resources to maintain a library, in practice, there\u2019s a single supported version for any library at any given point in time: the latest one. Any version before that (be that major, minor, patch) is in essence abandoned:</p> <ul> <li>If you want security updates, you need to move to the latest version.</li> <li>If you want a bug to be fixed, you need to move to the newest version.</li> <li>If you want a new feature, it is only going to be available in the latest     version.</li> </ul> <p>If the only maintained version is the latest, you really just have an <code>X</code> version number that is monotonically increasing. Once again PEP 440 supports it, so why not! It still communicates your branch strategy of there being only a single branch at any one time. Now I know this is a bit too unconventional for some people, and you may get into the high version number problem, then maybe it makes sense to use calendar versioning to use the version number to indicate the release date to signify just how old of a version you\u2019re using, but if stuff is working does that really matter?</p>"}, {"location": "semantic_versioning/#high-version-numbers", "title": "High version numbers", "text": "<p>Another major argument is that people inherently judge a project based on what it\u2019s version number is. They\u2019ll implicitly assume that <code>foo 2.0</code> is better than <code>bar 1.0</code> (and <code>frob 3.0</code> is better still) because the version numbers are higher. However, there is a limit to this, if you go too high too quickly, people assume your project is unstable and shouldn\u2019t really be used, even if the reason that your project is so high is because you removed some tiny edge cases that nobody actually used and didn\u2019t actually impact many people, if any, at all.</p> <p>These are two different expressions of the same thing. The first is that people will look down on a project for not having a high enough version compared to its competitors. While it\u2019s true that some people will do this, it's not a significant reason to throw away the communication benefits of your version number. Ultimately, no matter what you do, people who judge a project as inferior because of something as shallow as \u201csmaller version number\u201d will find some other, equally shallow, reason to pick between projects.</p> <p>The other side of this is a bit different. When you have a large major version, like <code>42.0.0</code>, people assume that your library is not stable and that you regularly break compatibility and if you follow SemVer strictly, it does actually mean that you regularly break compatibility.</p> <p>There are two general cases:</p> <ul> <li>The true positives: where a project that does routinely break it\u2019s public API in meaningful ways.</li> <li>The false positives: Projects that strictly follow semantic versioning were     each change which is not backwards compatible requires bumping a major     version. This means that if you remove some function that nobody actually     uses you need to increase your major version. Do it again and you need to     increase your major version again. Do this enough times, for even very small     changes and you can quickly get into a large version number <code>6</code>. This case     is a false positive for the \u201cstability\u201d test, because the reality is that     your project is actually quite stable.</li> </ul>"}, {"location": "semantic_versioning/#difference-in-change-categorization", "title": "Difference in change categorization", "text": "<p>Here's a thought experiment: you need to add a new warning to your Python package that tries to follow SemVer. Would that single change cause you to increase the major, minor, or patch version number? You might think a patch number bump since it isn't a new feature or breaking anything. You might think it's a minor version bump because it isn't exactly a bugfix. And you might think it's a major version bump because if you ran your Python code with <code>-W</code> error you suddenly introduced a new exception which could break people's code. Brett Cannon did a poll, answered by 231 people with the results:</p> <ul> <li>Patch/Bugfix: 47.2%</li> <li>Minor/enhancement: 44.2%</li> <li>Major/breaking: 8.7%</li> </ul> <p>That speaks volumes to why SemVer does not inherently work: someone's bugfix may be someone else's breaking change. Because in Python we can't statically define what an API change is there will always be a disagreement between you and your dependencies as to what a \"feature\" or \"bugfix\" truly is.</p> <p>That builds one of the arguments for CalVer. Because SemVer is imperfect at describing if a particular change will break someone upgrading the software, that we should instead throw it out and replace it with something that doesn\u2019t purport to tell us that information.</p>"}, {"location": "semantic_versioning/#unintended-changes", "title": "Unintended changes", "text": "<p>A major version bump must happen not only when you rewrite an entire library with its complete API, but also when you\u2019re just renaming a single rarely used function (which some may erroneously view as a minor change). Or even worse, it\u2019s not always clear what\u2019s part of the public API and what\u2019s not.</p> <p>You have a library with some incidental, undocumented, and unspecified behavior that you consider to be obviously not part of the public interface. You change it to solve what seems like a bug to you, and make a patch release, only to find that you have angry hordes at the gate who, thanks to Hyrum\u2019s Law, depend on the old behavior.</p> <p>With a sufficient number of users of an API, it does not matter what you promise in the contract. All observable behaviors of your system will be depended on by somebody.</p> <p>Which has been represented perfectly by the people behind xkcd.</p> <p></p> <p>While every maintainer would like to believe they\u2019ve thought of every use case up-front and created the best API for everything. In practice it's impossible to think on every impact your changes will make.</p> <p>Even if you were very diligent/broad with your interpretation to avoid accidentally breaking people with a bugfix release, bugs can still happen in a bugfix release. It obviously isn't intentional, but it does happen which means SemVer can't protect you from having to test your code to see if a patch version is compatible with your code.</p> <p>This makes \u201ctrue\u201d SemVer pointless. Minor releases are impossible, and patch releases are nearly impossible. If you fix a bug, someone could be depending on the buggy behaviour.</p>"}, {"location": "semantic_versioning/#using-zerover", "title": "Using ZeroVer", "text": "<p>ZeroVer is a joke versioning system similar to Semantic Versioning with the sole difference that <code>MAJOR</code> is always <code>0</code>. From the specification, as long as you are in the <code>0.X.Y</code> versions, you can introduce incompatible changes at any point. It intended to make fun of people who use \u201csemantic versioning\u201d but never make a <code>1.0</code> release, thus defeating the purpose of semver.</p> <p>This one of the consequences of trying to strictly follow Semantic Versioning, because once you give the leap to <code>1.0</code> you need to increase the <code>major</code> on each change quickly leading to the problem of high version numbers. The best way to fight this behaviour is to remember the often overlooked SemVer 2.0 FAQ guideline:</p> <p>If your software is being used in production, it should probably already be 1.0.0. If you have a stable API on which users have come to depend, you should be 1.0.0. If you\u2019re worrying a lot about backwards compatibility, you should probably already be 1.0.0.</p>"}, {"location": "semantic_versioning/#when-to-use-it", "title": "When to use it", "text": "<p>Check the Deciding what version system to use for your programs article section.</p>"}, {"location": "semantic_versioning/#references", "title": "References", "text": "<ul> <li> <p>Home</p> </li> <li> <p>Bernat post on versioning</p> </li> <li>Why I don't like SemVer anymore by Snarky</li> <li>Versioning Software by donald stufft</li> </ul>"}, {"location": "semantic_versioning/#libraries", "title": "Libraries", "text": "<p>These libraries can be used to interact with a git history of commits that follow the semantic versioning commit guidelines.</p> <ul> <li>python-semantic-release</li> </ul>"}, {"location": "storage/", "title": "Storage", "text": "<p>I have a server at home to host some services for my closest ones. The server is an Intel NUC which is super awesome in terms of electric consumption, CPU and RAM versus cost. The downside is that it has no hard drive to store the services data. It does have some USB ports to connect external hard drives though. As the data kept growing I started buying bigger drives. While it was affordable I purchased two so as to have one to store the backup of the data. The problem came when it became unaffordable for me. Then I took the good idea to assume that I could only have one drive of 16TB with my data. Obviously the inevitable happened. The hard drive died and those 10TB of data that were not stored in any backup were lost.</p> <p>Luckily enough, it was not unique data like personal photos. The data could be regenerated by manual processes at the cost of precious time (I'm still suffering this <code>:(</code>). But every cloud has a silver lining, this failure gave me the energy and motivation to improve my home architecture. To prevent this from happening again, the solution needs to be:</p> <ul> <li>Robust: If disks die I will have time to replace them before data is lost.</li> <li>Flexible: It needs to expand as the data grows.</li> <li>Not very expensive.</li> <li>Easy to maintain.</li> </ul> <p>There are two types of solutions to store data:</p> <ul> <li>On one host: All disks are attached to a server and the storage capacity is     shared to other devices by the local network.</li> <li>Distributed: The disks are attached to many servers and they work together to     provide the storage through the local network.</li> </ul> <p>A NAS server represents the first solution, while systems like Ceph or GlusterFS over Odroid HC4 fall into the second.</p> <p>Both are robust and flexible but I'm more inclined towards building a NAS because it can hold the amount of data that I need, it's easier to maintain and the underlying technology has been more battle proven throughout the years.</p>"}, {"location": "tdd/", "title": "TDD", "text": "<p>Test-driven development (TDD) is a software development process that relies on the repetition of a very short development cycle: requirements are turned into very specific test cases, then the code is improved so that the tests pass. This is opposed to software development that allows code to be added that is not proven to meet requirements.</p>"}, {"location": "tdd/#abstractions-in-testing", "title": "Abstractions in testing", "text": "<p>Writing tests that couple our high-level code with low-level details will make your life hard, because as the scenarios we consider get more complex, our tests will get more unwieldy.</p> <p>To avoid it, abstract the low-level code from the high-level one, unit test it and edge-to-edge test the high-level code.</p> <p>Edge-to-edge testing involves writing end-to-end tests, substituting the low level code for fakes that behave in the same way. The advantage of this approach is that our tests act on the exact same function that's used by our production code. The disadvantage is that we have to make our stateful components explicit and pass them around.</p>"}, {"location": "tdd/#fakes-vs-mocks", "title": "Fakes vs Mocks", "text": "<p>Mocks are used to verify how something gets used. Fakes are working implementations of the things they're replacing, but they're designed for use only in tests. They wouldn't work in the real life but they can be used to make assertions about the end state of a system rather than the behaviours along the way.</p> <p>Using fakes instead of mocks have these advantages:</p> <ul> <li>Overuse of mocks leads to complicated test suites that fail to explain the     code</li> <li>Patching out the dependency you're using makes it possible to unit test the     code, but it does nothing to improve the design. Faking makes you identify     the responsibilities of your codebase, and to separate those     responsibilities into small, focused objects that are easy to replace.</li> <li>Tests that use mocks tend to be more coupled to the implementation details     of the codebase. That's because mock tests verify the interactions between     things. This coupling between code and test tends to make tests more     brittle.</li> </ul> <p>Using the right abstractions is tricky, but here are a few questions that may help you:</p> <ul> <li>Can I choose a familiar Python data structure to represent the state of the     messy system and then try to imagine a single function that can return that     state?</li> <li>Where can I draw a line between my systems, where can I carve out a seam to     stick that abstraction in?</li> <li>What is a sensible way of dividing things into components with different     responsibilities? What implicit concepts can I make explicit?</li> <li>What are the dependencies, and what is the core business logic?</li> </ul>"}, {"location": "tdd/#tdd-in-high-gear-and-low-gear", "title": "TDD in High Gear and Low Gear", "text": "<p>Tests are supposed to help us change our system fearlessly, but often we write too many tests against the domain model. This causes problems when we want to change our codebase and find that we need to update tens or even hundreds of unit tests.</p> <p>Every line of code that we put in a test is like a blob of glue, holding the system in a particular shape. The more low-level tests we have, the harder it will be to change things.</p> <p>Tests can be written at the different levels of abstraction, high level tests gives us low feedback, low barrier to change and a high system coverage, while low level tests gives us high feedback, high barrier to change and focused coverage.</p> <p>A test for an HTTP API tells us nothing about the fine grained design of our objects, because it sits at a much higher level of abstraction. On the other hand, we can rewrite our entire application and, so long as we don't change the URLs or request formats, our HTTP tests will continue to pass. This gives us confidence that large-scale changes, like changing the database schema, haven't broken our code.</p> <p>At the other end of the spectrum, tests in the domain model  help us to understand the objects we need. These tests guide us to a design that makes sense and reads in the domain language. When our tests read in the domain language, we feel comfortable that our code matches our intuition about the problem we're trying to solve.</p> <p>We often sketch new behaviours by writing tests at this level to see how the code might look. When we want to improve the design of the code, though, we will need to replace or delete these tests, because they are tightly coupled to a particular implementation.</p> <p>Most of the time, when we are adding a new feature or fixing a bug, we don't need to make extensive changes to the domain model. In these cases, we prefer to write tests against services because of the lower coupling and higher coverage.</p> <p>When starting a new project or when hitting a particularly difficult problem, we will drop back down to writing tests against the domain model so we get better feedback and executable documentation of our intent.</p> <p>Note</p> <p>When starting a journey, the bicycle needs to be in a low gear so that it can overcome inertia. Once we're off and running, we can go faster and more efficiently by changing into a high gear; but if we suddenly encounter a steep hill or are forced to slow down by a hazard, we again drop down to a low gear until we can pick up speed again.</p>"}, {"location": "tdd/#references", "title": "References", "text": "<ul> <li>Architecture Patterns with     Python by     Harry J.W. Percival and Bob Gregory.</li> </ul>"}, {"location": "tdd/#further-reading", "title": "Further reading", "text": "<ul> <li>Martin Fowler o Mocks aren't stubs</li> </ul>"}, {"location": "terraform/", "title": "Terraform", "text": "<p>Terraform is an open-source infrastructure as code software tool created by HashiCorp. It enables users to define and provision a datacenter infrastructure using an awful high-level configuration language known as Hashicorp Configuration Language (HCL), or optionally JSON. Terraform supports a number of cloud infrastructure providers such as Amazon Web Services, IBM Cloud , Google Cloud Platform, DigitalOcean, Linode, Microsoft Azure, Oracle Cloud Infrastructure, OVH, or VMware vSphere as well as OpenNebula and OpenStack.</p>"}, {"location": "terraform/#installation", "title": "Installation", "text": "<p>Go to the releases page, download the latest release, decompress it and add it to your <code>$PATH</code>.</p>"}, {"location": "terraform/#tools", "title": "Tools", "text": "<ul> <li>tfschema: A binary that allows you   to see the attributes of the resources of the different providers. There are   some times that there are complex attributes that aren't shown on the docs   with an example. Here you'll see them clearly.</li> </ul> <pre><code>tfschema resource list aws | grep aws_iam_user\n&gt; aws_iam_user\n&gt; aws_iam_user_group_membership\n&gt; aws_iam_user_login_profile\n&gt; aws_iam_user_policy\n&gt; aws_iam_user_policy_attachment\n&gt; aws_iam_user_ssh_key\n\ntfschema resource show aws_iam_user\n+----------------------+-------------+----------+----------+----------+-----------+\n| ATTRIBUTE            | TYPE        | REQUIRED | OPTIONAL | COMPUTED | SENSITIVE |\n+----------------------+-------------+----------+----------+----------+-----------+\n| arn                  | string      | false    | false    | true     | false     |\n| force_destroy        | bool        | false    | true     | false    | false     |\n| id                   | string      | false    | true     | true     | false     |\n| name                 | string      | true     | false    | false    | false     |\n| path                 | string      | false    | true     | false    | false     |\n| permissions_boundary | string      | false    | true     | false    | false     |\n| tags                 | map(string) | false    | true     | false    | false     |\n| unique_id            | string      | false    | false    | true     | false     |\n+----------------------+-------------+----------+----------+----------+-----------+\n\n# Open the documentation of the resource in the browser\n\ntfschema resource browse aws_iam_user\n</code></pre> <ul> <li> <p>terraforming: Tool to export existing   resources to terraform</p> </li> <li> <p>terraboard: Web dashboard to   visualize and query terraform tfstate, you can search, compare and see the   most active ones. There are deployments for k8s.</p> </li> </ul> <pre><code>export AWS_ACCESS_KEY_ID=XXXXXXXXXXXXXXXXXXXX\nexport AWS_SECRET_ACCESS_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nexport AWS_DEFAULT_REGION=eu-west-1\nexport AWS_BUCKET=terraform-tfstate-20180119\nexport TERRABOARD_LOG_LEVEL=debug\ndocker network create terranet\ndocker run -ti --rm --name db -e POSTGRES_USER=gorm -e POSTGRES_DB=gorm -e POSTGRES_PASSWORD=\"mypassword\" --net terranet postgres\ndocker run -ti --rm -p 8080:8080 -e AWS_REGION=\"$AWS_DEFAULT_REGION\" -e AWS_ACCESS_KEY_ID=\"${AWS_ACCESS_KEY_ID}\" -e AWS_SECRET_ACCESS_KEY=\"${AWS_SECRET_ACCESS_KEY}\" -e AWS_BUCKET=\"$AWS_BUCKET\" -e DB_PASSWORD=\"mypassword\" --net terranet camptocamp/terraboard:latest\n</code></pre> <ul> <li> <p>tfenv: Install different versions of terraform   <pre><code>git clone https://github.com/tfutils/tfenv.git ~/.tfenv\necho 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' &gt;&gt; ~/.bashrc\necho 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' &gt;&gt; ~/.zshrc\ntfenv list-remote\ntfenv install 0.12.8\nterraform version\ntfenv install 0.11.15\nterraform version\ntfenv use 0.12.8\nterraform version\n</code></pre></p> </li> <li> <p>https://github.com/eerkunt/terraform-compliance</p> </li> <li> <p>landscape: A program to   modify the <code>plan</code> and show a nicer version, really useful when it's shown as   json. Right now   it only works for terraform 11.</p> </li> </ul> <pre><code>terraform plan | landscape\n</code></pre> <ul> <li>k2tf: Program to convert k8s yaml   manifestos to HCL.</li> </ul>"}, {"location": "terraform/#editor-plugins", "title": "Editor Plugins", "text": "<p>For Vim:</p> <ul> <li>vim-terraform: Execute tf from     vim and autoformat when saving.</li> <li>vim-terraform-completion:     linter and autocomplete.</li> </ul>"}, {"location": "terraform/#good-practices-and-maintaining", "title": "Good practices and maintaining", "text": "<ul> <li>fmt: Formats the code   following hashicorp best practices.</li> </ul> <pre><code>terraform fmt\n</code></pre> <ul> <li> <p>Validate: Tests that   the syntax is correct.   <pre><code>terraform validate\n</code></pre></p> </li> <li> <p>Documentaci\u00f3n: Generates   a table in markdown with the inputs and outputs.</p> </li> </ul> <pre><code>terraform-docs markdown table *.tf &gt; README.md\n\n## Inputs\n\n| Name | Description | Type | Default | Required |\n|------|-------------|:----:|:-----:|:-----:|\n| broker_numbers | Number of brokers | number | `\"3\"` | no |\n| broker_size | AWS instance type for the brokers | string | `\"kafka.m5.large\"` | no |\n| ebs_size | Size of the brokers disks | string | `\"300\"` | no |\n| kafka_version | Kafka version | string | `\"2.1.0\"` | no |\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| brokers_masked_endpoints | Zookeeper masked endpoints |\n| brokers_real_endpoints | Zookeeper real endpoints |\n| zookeeper_masked_endpoints | Zookeeper masked endpoints |\n| zookeeper_real_endpoints | Zookeeper real endpoints |\n</code></pre> <ul> <li>Terraform lint (tflint): Only works with   some AWS resources. It allows the validation against a third party API. For   example:   <pre><code>  resource \"aws_instance\" \"foo\" {\n    ami           = \"ami-0ff8a91507f77f867\"\n    instance_type = \"t1.2xlarge\" # invalid type!\n  }\n</code></pre></li> </ul> <p>The code is valid, but in AWS there doesn't exist the type <code>t1.2xlarge</code>. This   test avoids this kind of issues.</p> <pre><code>wget https://github.com/wata727/tflint/releases/download/v0.11.1/tflint_darwin_amd64.zip\nunzip tflint_darwin_amd64.zip\nsudo install tflint /usr/local/bin/\ntflint -v\n</code></pre> <p>We can automate all the above to be executed before we do a commit using the pre-commit framework.</p> <pre><code>sudo pip install pre-commit\ncd $proyectoConTerraform\necho \"\"\"repos:\n- repo: git://github.com/antonbabenko/pre-commit-terraform\n  rev: v1.19.0\n  hooks:\n    - id: terraform_fmt\n    - id: terraform_validate\n    - id: terraform_docs\n    - id: terraform_tflint\n\"\"\" &gt; .pre-commit-config.yaml\npre-commit install\npre-commit run terraform_fmt\npre-commit run terraform_validate --file dynamo.tf\npre-commit run -a\n</code></pre>"}, {"location": "terraform/#tests", "title": "Tests", "text": "<p>Motivation</p>"}, {"location": "terraform/#static-analysis", "title": "Static analysis", "text": ""}, {"location": "terraform/#linters", "title": "Linters", "text": "<ul> <li>conftest</li> <li>tflint</li> <li><code>terraform validate</code></li> </ul>"}, {"location": "terraform/#dry-run", "title": "Dry run", "text": "<ul> <li><code>terraform plan</code></li> <li>hashicorp sentinel</li> <li>terraform-compliance</li> </ul>"}, {"location": "terraform/#unit-tests", "title": "Unit tests", "text": "<p>There is no real unit testing in infrastructure code as you need to deploy it in a real environment</p> <ul> <li> <p><code>terratest</code> (works for k8s and terraform)</p> <p>Some sample code in:</p> <ul> <li>github.com/gruntwork-io/infrastructure-as-code-testing-talk</li> <li>gruntwork.io</li> </ul> </li> </ul>"}, {"location": "terraform/#e2e-test", "title": "E2E test", "text": "<ul> <li>Too slow and too brittle to be worth it</li> <li>Use incremental e2e testing</li> </ul>"}, {"location": "terraform/#variables", "title": "Variables", "text": "<p>It's a good practice to name the resource before the particularization of the resource, so you can search all the elements of that resource, for example, instead of <code>client_cidr</code> and <code>operations_cidr</code> use <code>cidr_operations</code> and <code>cidr_client</code></p> <pre><code>variable \"list_example\"{\ndescription = \"An example of a list\"\ntype = \"list\"\ndefault = [1, 2, 3]\n}\n\nvariable \"map_example\"{\ndescription = \"An example of a dictionary\"\ntype = \"map\"\ndefault = {\nkey1 = \"value1\"\nkey2 = \"value2\"\n}\n}\n</code></pre> <p>For the use of maps inside maps or lists investigate <code>zipmap</code></p> <p>To access you have to use <code>\"${var.list_example}\"</code></p> <p>For secret variables we use: <pre><code>variable \"db_password\" {\ndescription = \"The password for the database\"\n}\n</code></pre></p> <p>Which has no default value, we save that password in our keystore and pass it as environmental variable <pre><code>export TF_VAR_db_password=\"{{ your password }}\"\nterragrunt plan\n</code></pre></p> <p>As a reminder, Terraform stores all variables in its state file in plain text, including this database password, which is why your terragrunt config should always enable encryption for remote state storage in S3</p>"}, {"location": "terraform/#interpolation-of-variables", "title": "Interpolation of variables", "text": "<p>You can't interpolate in variables, so instead of <pre><code>variable \"sistemas_gpg\" {\ndescription = \"Sistemas public GPG key for Zena\"\ntype = \"string\"\ndefault = \"${file(\"sistemas_zena.pub\")}\"\n}\n</code></pre> You have to use locals</p> <pre><code>locals {\nsistemas_gpg = \"${file(\"sistemas_zena.pub\")}\"\n}\n\n\"${local.sistemas_gpg}\"\n</code></pre>"}, {"location": "terraform/#show-information-of-the-resources", "title": "Show information of the resources", "text": "<p>Get information of the infrastructure. Output variables show up in the console after you run <code>terraform apply</code>, you can also use <code>terraform output [{{ output_name }}]</code> to see the value of a specific output without applying any changes</p> <pre><code>output \"public_ip\" {\nvalue = \"${aws_instance.example.public_ip}\"\n}\n</code></pre> <pre><code>&gt; terraform apply\naws_security_group.instance: Refreshing state... (ID: sg-db91dba1)\naws_instance.example: Refreshing state... (ID: i-61744350)\nApply complete! Resources: 0 added, 0 changed, 0 destroyed.\nOutputs:\npublic_ip = 54.174.13.5\n</code></pre>"}, {"location": "terraform/#data-source", "title": "Data source", "text": "<p>A data source represents a piece of read-only information that is fetched from the provider every time you run Terraform. It does not create anything new</p> <pre><code>data \"aws_availability_zones\" \"all\" {}\n</code></pre> <p>And you reference it with <code>\"${data.aws_availability_zones.all.names}\"</code></p>"}, {"location": "terraform/#read-only-state-source", "title": "Read-only state source", "text": "<p>With <code>terraform_remote_state</code> you an fetch the Terraform state file stored by another set of templates in a completely read-only manner.</p> <p>From an app template we can read the info of the ddbb with <pre><code>data \"terraform_remote_state\" \"db\" {\nbackend = \"s3\"\nconfig {\nbucket = \"(YOUR_BUCKET_NAME)\"\nkey = \"stage/data-stores/mysql/terraform.tfstate\"\nregion = \"us-east-1\"\n}\n}\n</code></pre></p> <p>And you would access the variables inside the database terraform file with <code>data.terraform_remote_state.db.outputs.port</code></p> <p>To share variables from state, you need to to set them in the <code>outputs.tf</code> file.</p>"}, {"location": "terraform/#template_file-source", "title": "Template_file source", "text": "<p>It is used to load templates, it has two parameters, <code>template</code> which is a string and <code>vars</code> which is a map of variables. it has one output attribute called <code>rendered</code>, which is the result of rendering template. For example</p> <pre><code># File: user-data.sh\n#!/bin/bash\ncat &gt; index.html &lt;&lt;EOF\n&lt;h1&gt;Hello, World&lt;/h1&gt;\n&lt;p&gt;DB address: ${db_address}&lt;/p&gt;\n&lt;p&gt;DB port: ${db_port}&lt;/p&gt;\nEOF\nnohup busybox httpd -f -p \"${server_port}\" &amp;\n</code></pre> <pre><code>data \"template_file\" \"user_data\" {\ntemplate = \"${file(\"user-data.sh\")}\"\nvars {\nserver_port = \"${var.server_port}\"\ndb_address = \"${data.terraform_remote_state.db.address}\"\ndb_port = \"${data.terraform_remote_state.db.port}\"\n}\n}\n</code></pre>"}, {"location": "terraform/#resource-lifecycle", "title": "Resource lifecycle", "text": "<p>The <code>lifecycle</code> parameter is a meta-parameter, it exist on about every resource in Terraform. You can add a <code>lifecycle</code> block to any resource to configure how that resource should be created, updated or destroyed.</p> <p>The available options are: * <code>create_before_destroy</code>: Which if set to true will create a replacement   resource before destroying hte original resource * <code>prevent_destroy</code>: If set to true, any attempt to delete that resource   (<code>terraform destroy</code>), will fail, to delete it you have to first remove the   <code>prevent_destroy</code></p> <pre><code>resource \"aws_launch_configuration\" \"example\" {\nimage_id = \"ami-40d28157\"\ninstance_type = \"t2.micro\"\nsecurity_groups = [\"${aws_security_group.instance.id}\"]\nuser_data = &lt;&lt;-EOF\n              #!/bin/bash\n              echo \"Hello, World\" &gt; index.html\n              nohup busybox httpd -f -p \"${var.server_port}\" &amp;\n              EOF\nlifecycle {\ncreate_before_destroy = true\n}\n}\n</code></pre> <p>If you set the <code>create_before_destroy</code> on a resource, you also have to set it on every resource that X depends on (if you forget, you'll get errors about cyclical dependencies). In the case of the launch configuration, that means you need to set <code>create_before_destroy</code> to true on the security group:</p> <pre><code>resource \"aws_security_group\" \"instance\" {\nname = \"terraform-example-instance\"\ningress {\nfrom_port = \"${var.server_port}\"\nto_port = \"${var.server_port}\"\nprotocol = \"tcp\"\ncidr_blocks = [\"0.0.0.0/0\"]\n}\nlifecycle {\ncreate_before_destroy = true\n}\n}\n</code></pre>"}, {"location": "terraform/#use-collaboratively", "title": "Use collaboratively", "text": ""}, {"location": "terraform/#share-state", "title": "Share state", "text": "<p>The best option is to use S3 as bucket of the config.</p> <p>First create it <pre><code>resource \"aws_s3_bucket\" \"terraform_state\" {\nbucket = \"terraform-up-and-running-state\"\nversioning {\nenabled = true\n}\nlifecycle {\nprevent_destroy = true\n}\n}\n</code></pre></p> <p>And then configure terraform <pre><code>terraform remote config \\\n-backend=s3 \\\n-backend-config=\"bucket=(YOUR_BUCKET_NAME)\" \\\n-backend-config=\"key=global/s3/terraform.tfstate\" \\\n-backend-config=\"region=us-east-1\" \\\n-backend-config=\"encrypt=true\"\n</code></pre></p> <p>In this way terraform will automatically pull the latest state from this bucked and push the latest state after running a command</p>"}, {"location": "terraform/#lock-terraform", "title": "Lock terraform", "text": "<p>To avoid several people running terraform at the same time, we'd use <code>terragrunt</code> a wrapper for terraform that manages remote state for you automatically and provies locking by using DynamoDB (in the free tier)</p> <p>Inside the <code>terraform_config.tf</code> you create the dynamodb table and then configure your <code>s3</code> backend to use it</p> <pre><code># create a dynamodb table for locking the state file\nresource \"aws_dynamodb_table\" \"dynamodb-terraform-state-lock\" {\nname         = \"terraform-state-lock-dynamo\"\nhash_key     = \"LockID\"\nbilling_mode = \"PAY_PER_REQUEST\"\nattribute {\nname = \"LockID\"\ntype = \"S\"\n}\n}\n\nterraform {\nbackend \"s3\" {\nbucket = \"provider-tfstate\"\nkey    = \"global/s3/terraform.tfstate\"\nregion = \"eu-west-1\"\nencrypt = \"true\"\ndynamodb_table = \"global-s3\"\n}\n}\n</code></pre> <p>You'll probably need to execute an <code>terraform apply</code> with the <code>dynamodb_table</code> line commented</p> <p>If you want to unforce a lock, execute:</p> <pre><code>terraform force-unlock {{ unlock_id }}\n</code></pre> <p>You get the <code>unlock_id</code> from an error trying to execute any <code>terraform</code> command</p>"}, {"location": "terraform/#modules", "title": "Modules", "text": "<p>In terraform you can put code inside of a <code>module</code> and reuse in multiple places throughout your code.</p> <p>The provider resource should be specified by the user and not in the modules</p> <p>Whenever you add a module to your terraform template or modify its source parameter you need to run a get command before you run <code>plan</code> or <code>apply</code></p> <pre><code>terraform get\n</code></pre> <p>To extract output variables of a module to the parent tf file you should use</p> <p><code>${module.{{module.name}}.{{output_name}}}</code></p>"}, {"location": "terraform/#basics", "title": "Basics", "text": "<p>Any set of Terraform templates in a directory is a module.</p> <p>The good practice is to have a directory called <code>modules</code> in your parent project directory. There you git clone the desired modules. and for example inside <code>pro/services/bastion/main.tf</code> you'd call it with:</p> <pre><code>provider \"aws\" {\nregion = \"eu-west-1\"\n}\n\nmodule \"bastion\" {\nsource = \"../../../modules/services/bastion/\"\n}\n</code></pre>"}, {"location": "terraform/#outputs", "title": "Outputs", "text": "<p>Modules encapsulate their resources. A resource in one module cannot directly depend on resources or attributes in other modules, unless those are exported through outputs. These outputs can be referenced in other places in your configuration, for example:</p> <pre><code>resource \"aws_instance\" \"client\" {\nami               = \"ami-408c7f28\"\ninstance_type     = \"t1.micro\"\navailability_zone = \"${module.consul.server_availability_zone}\"\n}\n</code></pre>"}, {"location": "terraform/#import", "title": "Import", "text": "<p>You can import the different parts with <code>terraform import {{resource_type}}.{{resource_name}} {{ resource_id }}</code></p> <p>For examples see the documentation of the desired resource.</p>"}, {"location": "terraform/#bulk-import", "title": "Bulk import", "text": "<p>But if you want to bulk import sources, I suggest using <code>terraforming</code>.</p>"}, {"location": "terraform/#bad-points", "title": "Bad points", "text": "<ul> <li>Manually added resources wont be managed by terraform, therefore you can't use   it to enforce as shown in this   bug.</li> <li>If you modify the LC of an ASG, the instances don't get rolling updated, you   have to do it manually.</li> <li>They call the dictionaries <code>map</code>... (/\uff9f\u0414\uff9f)/</li> <li>The conditionals are really ugly. You need to use <code>count</code>.</li> <li>You can't split long strings xD</li> </ul>"}, {"location": "terraform/#best-practices", "title": "Best practices", "text": "<p>Name the resources with <code>_</code> instead of <code>-</code> so the editor's completion work :)</p>"}, {"location": "terraform/#vpc", "title": "VPC", "text": "<p>Don't use the default vpc</p>"}, {"location": "terraform/#security-groups", "title": "Security groups", "text": "<p>Instead of using <code>aws_security_group</code> to define the ingress and egress rules, use it only to create the empty security group and use <code>aws_security_group_rule</code> to add the rules, otherwise you'll get into a cycle loop</p> <p>The sintaxis of an egress security group must be <code>egress_from_{{source}}_to_destination</code>. The sintaxis of an ingress security group must be <code>ingress_to_{{destination}}_from_{{source}}</code></p> <p>Also set the order of the arguments, so they look like the name.</p> <p>For ingress rule:</p> <pre><code>security_group_id = ...\ncidr_blocks = ...\n</code></pre> <p>And in egress should look like:</p> <pre><code>security_group_id = ...\ncidr_blocks = ...\n</code></pre> <p>Imagine you want to filter the traffic from A -&gt; B, the egress rule from A to B should go besides the ingress rule from B to A.</p>"}, {"location": "terraform/#default-security-group", "title": "Default security group", "text": "<p>You can't manage the default security group of an vpc, therefore you have to adopt it and set it to no rules at all with <code>aws_default_security_group</code> resource</p>"}, {"location": "terraform/#iam", "title": "IAM", "text": "<p>You have to generate an gpg key and export it in base64</p> <pre><code>gpg --export {{ gpg_id }} | base64\n</code></pre> <p>To see the secrets you have to decrypt it <pre><code>terraform output secret | base64 --decode | gpg -d\n</code></pre></p>"}, {"location": "terraform/#sensitive-information", "title": "Sensitive information", "text": "<p>One of the most common questions we get about using Terraform to manage infrastructure as code is how to handle secrets such as passwords, API keys, and other sensitive data.</p> <p>Your secrets live in two places in a terraform environment:</p> <ul> <li>The Terraform state</li> <li>The Terraform source code.</li> </ul>"}, {"location": "terraform/#sensitive-information-in-the-terraform-state", "title": "Sensitive information in the Terraform State", "text": "<p>Every time you deploy infrastructure with Terraform, it stores lots of data about that infrastructure, including all the parameters you passed in, in a state file. By default, this is a terraform.tfstate file that is automatically generated in the folder where you ran terraform apply. </p> <p>That means that the secrets will end up in terraform.tfstate in plain text! This has been an open issue for more than 6 years now, with no clear plans for a first-class solution. There are some workarounds out there that can scrub secrets from your state files, but these are brittle and likely to break with each new Terraform release, so I don\u2019t recommend them.</p> <p>For the time being, you can:</p> <ul> <li>Store Terraform state in a backend that supports encryption: Instead of storing your state in a local <code>terraform.tfstate</code> file, Terraform natively supports a variety of backends, such as S3, GCS, and Azure Blob Storage. Many of these backends support encryption, so that instead of your state files being in plain text, they will always be encrypted, both in transit (e.g., via TLS) and on disk (e.g., via AES-256). Most backends also support collaboration features (e.g., automatically pushing and pulling state; locking), so using a backend is a must-have both from a security and teamwork perspective.</li> <li>Strictly control who can access your Terraform backend: Since Terraform state files may contain secrets, you\u2019ll want to carefully control who has access to the backend you\u2019re using to store your state files. For example, if you\u2019re using S3 as a backend, you\u2019ll want to configure an IAM policy that solely grants access to the S3 bucket for production to a small handful of trusted devs (or perhaps solely just the CI server you use to deploy to prod).</li> </ul> <p>There are several approaches here.</p> <p>First rely on the S3 encryption to protect the information in your state file.</p> <p>Second, use Vault provider to protect the state file.</p> <p>Third (but I won't use it) would be to use terrahelp</p>"}, {"location": "terraform/#sensitive-information-in-the-terraform-source-code", "title": "Sensitive information in the Terraform source code", "text": "<p>To store secrets in your source code you can:</p> <ul> <li>Use Secret Stores</li> <li>Use environment variables</li> <li>Use encrypted files</li> </ul> <p>Using Secret Stores is the best solution, but for that you'd need access and trust in a Secret Store provider which I don't have at the moment (if you want to follow this path check out Hashicorp Vault). Using environment variables is the worst solution because this technique helps you avoid storing secrets in plain text in your code, but it leaves the question of how to actually securely store and manage the secrets unanswered. So in a sense, this technique just kicks the can down the road, whereas the other techniques described later are more prescriptive. Although you could use a password manager such as <code>pass</code>. Using encrypted files is the solution that remains.</p> <p>If you don't want to install a secret store and are used to work with GPG, you can encrypt the secrets, store the cipher text in a file, and checking that file into the version control system. To encrypt some data, such as some secrets in a file, you need an encryption key. This key is itself a secret! This creates a bit of a conundrum: how do you securely store that key? You can\u2019t check the key into version control as plain text, as then there\u2019s no point of encrypting anything with it. You could encrypt the key with another key, but then you then have to figure out where to store that second key. So you\u2019re back to the \u201ckick the can down the road problem,\u201d as you still have to find a secure way to store your encryption key. Although you can use external solutions such as AWS KMS or GCP KMS we don't want to store that kind of information on big companies servers. A local and more beautiful way is to rely on PGP to do the encryption.</p> <p>We'll use then <code>sops</code> a Mozilla tool for managing secrets that can use PGP behind the scenes. <code>sops</code> can automatically decrypt a file when you open it in your text editor, so you can edit the file in plain text, and when you go to save those files, it automatically encrypts the contents again. </p> <p>Terraform does not yet have native support for decrypting files in the format used by <code>sops</code>. One solution is to install and use the custom provider for sops, <code>terraform-provider-sops</code>. Another option, is to use Terragrunt. To avoid installing more tools, it's better to use the terraform provider.</p> <p>First of all you may need to install <code>sops</code>, you can grab the latest release from their downloads page.</p> <p>Then in your terraform code you need to select the <code>sops</code> provider:</p> <pre><code>terraform {\n  required_providers {\n    sops = {\n      source = \"carlpett/sops\"\n      version = \"~&gt; 0.5\"\n    }\n  }\n}\n</code></pre> <p>Configure <code>sops</code> by defining the gpg keys in a <code>.sops.yaml</code> file at the top of your repository:</p> <pre><code>---\ncreation_rules:\n- pgp: &gt;-\n2829BASDFHWEGWG23WDSLKGL323534J35LKWERQS,\n2GEFDBW349YHEDOH2T0GE9RH0NEORIG342RFSLHH\n</code></pre> <p>Then create the secrets file with the command <code>sops secrets.enc.json</code> somewhere in your terraform repository. For example:</p> <pre><code>{\n\"password\": \"foo\",\n\"db\": {\"password\": \"bar\"}\n}\n</code></pre> <p>You'll be able to use these secrets in your terraform code. For example:</p> <pre><code>data \"sops_file\" \"secrets\" {\n  source_file = \"secrets.enc.json\"\n}\n\noutput \"root-value-password\" {\n  # Access the password variable from the map\n  value = data.sops_file.secrets.data[\"password\"]\n}\n\noutput \"mapped-nested-value\" {\n  # Access the password variable that is under db via the terraform map of data\n  value = data.sops_file.secrets.data[\"db.password\"]\n}\n\noutput \"nested-json-value\" {\n  # Access the password variable that is under db via the terraform object\n  value = jsondecode(data.sops_file.secrets.raw).db.password\n}\n</code></pre> <p>Sops also supports encrypting the entire file when in other formats. Such files can also be used by specifying <code>input_type = \"raw\"</code>:</p> <pre><code>data \"sops_file\" \"some-file\" {\n  source_file = \"secret-data.txt\"\n  input_type = \"raw\"\n}\n\noutput \"do-something\" {\n  value = data.sops_file.some-file.raw\n}\n</code></pre>"}, {"location": "terraform/#rds-credentials", "title": "RDS credentials", "text": "<p>The RDS credentials are saved in plaintext both in the definition and in the state file, see this bug for more information. The value of <code>password</code> is not compared against the value of the password in the cloud, so as long as the string in the code and the state file remains the same, it won't try to change it.</p> <p>As a workaround, you can create the RDS with a fake password <code>changeme</code>, and once the resource is created, run an <code>aws</code> command to change it. That way, the value in your code and the state is not the real one, but it won't try to change it.</p> <p>Inspired in this gist and the <code>local-exec</code> docs, you could do:</p> <pre><code>resource \"aws_db_instance\" \"main\" {\n    username = \"postgres\"\n    password = \"changeme\"\n    ...\n}\n\nresource \"null_resource\" \"master_password\" {\n    triggers {\n        db_host = aws_db_instance.main.address\n    }\n    provisioner \"local-exec\" {\n        command = \"pass generate rds_main_password; aws rds modify-db-instance --db-instance-identifier $INSTANCE --master-user-password $(pass show rds_main_password)\"\n        environment = {\n            INSTANCE = aws_db_instance.main.identifier\n        }\n    }\n}\n</code></pre> <p>Where the password is stored in your <code>pass</code> repository that can be shared with the team.</p> <p>If you're wondering why I added such a long line, well it's because of HCL! as you can't split long strings, marvelous isn't it? xD</p>"}, {"location": "terraform/#loops", "title": "Loops", "text": "<p>You can't use nested lists or dictionaries, see this 2015 bug</p>"}, {"location": "terraform/#loop-over-a-variable", "title": "Loop over a variable", "text": "<pre><code>variable \"vpn_egress_tcp_ports\" {\n  description = \"VPN egress tcp ports \"\n  type = \"list\"\n  default = [50, 51, 500, 4500]\n}\n\nresource \"aws_security_group_rule\" \"ingress_tcp_from_ops_to_vpn_instance\"{\n  count = \"${length(var.vpn_egress_tcp_ports)}\"\n  type = \"ingress\"\n  from_port   = \"${element(var.vpn_egress_tcp_ports, count.index)}\"\n  to_port     = \"${element(var.vpn_egress_tcp_ports, count.index)}\"\n  protocol    = \"tcp\"\n  cidr_blocks = [ \"${var.cidr}\"]\n  security_group_id = \"${aws_security_group.pro_ins_vpn.id}\"\n}\n</code></pre>"}, {"location": "terraform/#refactoring", "title": "Refactoring", "text": "<p>Refactoring in terraform is ugly business</p>"}, {"location": "terraform/#refactoring-in-modules", "title": "Refactoring in modules", "text": "<p>If you try to refactor your terraform state into modules it will try to destroy and recreate all the elements of the module...</p>"}, {"location": "terraform/#refactoring-the-state-file", "title": "Refactoring the state file", "text": "<pre><code>terraform state mv -state-out=other.tfstate module.web module.web\n</code></pre>"}, {"location": "terraform/#google-cloud-integration", "title": "Google cloud integration", "text": "<p>You configure it in the terraform directory <pre><code>// Configure the Google Cloud provider\nprovider \"google\" {\ncredentials = \"${file(\"account.json\")}\"\nproject     = \"my-gce-project\"\nregion      = \"us-central1\"\n}\n</code></pre></p> <p>To download the json go to the Google Developers Console. Go to <code>Credentials</code> then <code>Create credentials</code> and finally <code>Service account key</code>.</p> <p>Select <code>Compute engine default service account</code> and select <code>JSON</code> as the key type.</p>"}, {"location": "terraform/#ignore-the-change-of-an-attribute", "title": "Ignore the change of an attribute", "text": "<p>Sometimes you don't care whether some attributes of a resource change, if that's the case use the <code>lifecycle</code> statement:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  # ...\n\n  lifecycle {\n    ignore_changes = [\n      # Ignore changes to tags, e.g. because a management agent\n      # updates these based on some ruleset managed elsewhere.\n      tags,\n    ]\n  }\n}\n</code></pre>"}, {"location": "terraform/#define-the-default-value-of-an-variable-that-contains-an-object-as-empty", "title": "Define the default value of an variable that contains an object as empty", "text": "<pre><code>variable \"database\" {\n  type = object({\n    size                 = number\n    instance_type        = string\n    storage_type         = string\n    engine               = string\n    engine_version       = string\n    parameter_group_name = string\n    multi_az             = bool\n  })\n  default     = null\n</code></pre>"}, {"location": "terraform/#conditionals", "title": "Conditionals", "text": ""}, {"location": "terraform/#elif", "title": "Elif", "text": "<pre><code>locals {\ntest = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\"\n}\n</code></pre>"}, {"location": "terraform/#do-a-conditional-if-a-variable-is-not-null", "title": "Do a conditional if a variable is not null", "text": "<pre><code>resource \"aws_db_instance\" \"instance\" {\n  count                = var.database == null ? 0 : 1\n  ...\n</code></pre>"}, {"location": "terraform/#debugging", "title": "Debugging", "text": "<p>You can set the <code>TF_LOG</code> environmental variable to one of the log levels <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code> or <code>ERROR</code> to change the verbosity of the logs.</p> <p>To remove the debug traces run <code>unset TF_LOG</code>.</p>"}, {"location": "terraform/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Modules registry</li> <li>terraform-aws-modules</li> <li>AWS providers</li> <li>AWS examples</li> <li>GCloud examples</li> <li>Good and bad sides of terraform</li> <li>Awesome Terraform</li> </ul>"}, {"location": "use_warnings/", "title": "Using warnings to evolve your package", "text": "<p>Regardless of the versioning system you're using, once you reach your first stable version, the commitment to your end users must be that you give them time to adapt to the changes in your program. So whenever you want to introduce a breaking change release it under a new interface, and in parallel, start emitting <code>DeprecationWarning</code> or <code>UserWarning</code> messages whenever someone invokes the old one. Maintain this state for a defined period (for example six months), and communicate explicitly in the warning message the timeline for when users have to migrate.</p> <p>This gives everyone time to move to the new interface without breaking their system, and then the library may remove the change and get rid of the old design chains forever. As an added benefit, only people using the old interface will ever see the warning, as opposed to affecting everyone (as seen with the semantic versioning major version bump).</p> <p>If you're following semantic versioning you'd do this change in a <code>minor</code> release, and you'll finally remove the functionality in another <code>minor</code> release. As you've given your users enough time to adequate to the new version of the code, it's not understood as a breaking change.</p> <p>This allows too for your users to be less afraid and stop upper-pinning you in their dependencies.</p> <p>Another benefit of using warnings is that if you configure your test runner to capture the warnings (which you should!) you can use your test suite to see the real impact of the deprecation, you may even realize why was that feature there and that you can't deprecate it at all.</p>"}, {"location": "use_warnings/#using-warnings", "title": "Using warnings", "text": "<p>Even though there are many warnings, I usually use <code>UserWarning</code> or <code>DeprecationWarning</code>. The full list is:</p> Class Description Warning This is the base class of all warning category classes. UserWarning The default category for warn(). DeprecationWarning Warn other developers about deprecated features. FutureWarning Warn other end users of applications about deprecated features. SyntaxWarning Warn about dubious syntactic features. RuntimeWarning Warn about dubious runtime features. PendingDeprecationWarning Warn about features that will be deprecated in the future (ignored by default). ImportWarning Warn triggered during the process of importing a module (ignored by default). UnicodeWarning Warn related to Unicode. BytesWarning Warn related to bytes and bytearray. ResourceWarning Warn related to resource usage (ignored by default)."}, {"location": "use_warnings/#how-to-raise-a-warning", "title": "How to raise a warning", "text": "<p>Warning messages are typically issued in situations where it is useful to alert the user of some condition in a program, where that condition doesn\u2019t warrant raising an exception and terminating the program.</p> <pre><code>import warnings\n\ndef f():\n    warnings.warn('Message', DeprecationWarning)\n</code></pre>"}, {"location": "use_warnings/#suppressing-a-warning", "title": "Suppressing a warning", "text": "<p>To disable in the whole file, add to the top:</p> <pre><code>import warnings\nwarnings.filterwarnings(\"ignore\", message=\"divide by zero encountered in divide\")\n</code></pre> <p>If you want this to apply to only one section of code, then use the warnings context manager:</p> <pre><code>import warnings\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", message=\"divide by zero encountered in divide\")\n    # .. your divide-by-zero code ..\n</code></pre> <p>And if you want to disable it for the whole code base configure pytest accordingly.</p>"}, {"location": "use_warnings/#how-to-evolve-your-code", "title": "How to evolve your code", "text": "<p>To ensure that the transition is smooth you need to tweak your code so that the user can switch a flag and make sure that their code keeps on working with the new changes. For example imagine that we have a class <code>MyClass</code> with a method <code>my_method</code>.</p> <pre><code>class MyClass:\n    def my_method(self, argument):\n        # my_method code goes here\n</code></pre> <p>You can add an argument <code>deprecate_my_method</code> that defaults to <code>False</code>, or you can take the chance to change the signature of the function, so that if the user is using the old argument, it uses the old behaviour and gets the warning, and if it's using the new argument, it uses the new. The advantage of changing the signature is that you don't need to do another deprecation for the temporal argument flag.</p> <p>!!! note \"Or you can use environmental variables\"</p> <pre><code>class MyClass:\n    def __init__(self, deprecate_my_method = False):\n        self.deprecate_my_method = deprecate_my_method\n\n    def my_method(self, argument):\n        if self.deprecate_my_method:\n            # my_method new functionality\n        else:\n            warnings.warn(\"Use my_new_method instead\", UserWarning)\n            # my_method old code goes here\n</code></pre> <p>That way when users get the new version of your code, if they are not using <code>my_method</code> they won't get the exception, and if they are, they can change how they initialize their classes with <code>MyClass(deprecate_my_method=True)</code>, run their tests tweaking their code to meet the new functionality and make sure that they are ready for the method to be deprecated. Once removed, another UserWarning will be raised to stop using <code>deprecate_my_method</code> as an argument to initialize the class as it is no longer needed.</p> <p>Until you remove the old code, you need to keep both functionalities and make sure all your test suite works with both cases. To do that, create the warning, run the tests and see what tests are raising the exception. For each of them you need to think if this test will make sense with the new code:</p> <ul> <li>If it doesn't, make sure that the warning is raised.</li> <li>If it is, make sure that the warning is raised and create     another test with the <code>deprecate_my_method</code> enabled.</li> </ul> <p>Once the deprecation date arrives you'll need to search for the date in your code to see where the warning is raised and used, remove the old functionality and update the tests. If you used a temporal argument to let the users try the new behaviour, issue the warning to deprecate it.</p>"}, {"location": "use_warnings/#use-environmental-variables", "title": "Use environmental variables", "text": "<p>A cleaner way to handle it is with environmental variables, that way you don't need to change the signature of the function twice. I've learned this from boto where they informed their users this way:</p> <ul> <li>If you wish to test the new feature we have created a new environment variable     <code>BOTO_DISABLE_COMMONNAME</code>. Setting this to <code>true</code> will suppress the warning and     use the new functionality.</li> <li>If you are concerned about this change causing disruptions, you can pin your     version of <code>botocore</code> to <code>&lt;1.28.0</code> until you are ready to migrate.</li> <li> <p>If you are only concerned about silencing the warning in your logs, use     <code>warnings.filterwarnings</code> when instantiating a new service client.</p> <pre><code>import warnings\nwarnings.filterwarnings('ignore', category=FutureWarning, module='botocore.client')\n</code></pre> </li> </ul>"}, {"location": "use_warnings/#testing-warnings", "title": "Testing warnings", "text": "<p>To test the function with pytest you can use <code>pytest.warns</code>:</p> <pre><code>import warnings\nimport pytest\n\n\ndef test_warning():\n    with pytest.warns(UserWarning, match='my warning'):\n        warnings.warn(\"my warning\", UserWarning)\n</code></pre> <p>For the <code>DeprecationWarnings</code> you can use <code>deprecated_call</code>:</p> <p>Or you can use <code>deprecated</code>:</p> <pre><code>def test_myfunction_deprecated():\n    with pytest.deprecated_call():\n        f()\n</code></pre> <pre><code>@deprecated(version='1.2.0', reason=\"You should use another function\")\ndef some_old_function(x, y):\n    return x + y\n</code></pre> <p>But it adds a dependency to your program, although they don't have any downstream dependencies.</p>"}, {"location": "use_warnings/#references", "title": "References", "text": "<ul> <li>Bernat post on versioning</li> </ul>"}, {"location": "velero/", "title": "Velero", "text": "<p>Velero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes.</p>"}, {"location": "velero/#installation", "title": "Installation", "text": ""}, {"location": "velero/#client-instalation", "title": "Client instalation", "text": "<p>You interact with <code>velero</code> through a  client command line.</p> <ul> <li>Download the latest release\u2019s   tarball for your client platform.</li> <li>Extract the tarball:</li> </ul> <p><pre><code>tar -xvf &lt;RELEASE-TARBALL-NAME&gt;.tar.gz\n</code></pre> * Move the extracted velero binary to somewhere in your <code>$PATH</code>.</p>"}, {"location": "velero/#server-configuration", "title": "Server configuration", "text": "<p>Instead of configuring the server through the <code>velero</code> command line it's better to use the vmware-tanzu/velero chart.</p> <p>To configure backup policy use the <code>extraObjects</code> chart field. For example to backup everything with the next policy do:</p> <ul> <li>Hourly backups for a day.</li> <li>Daily backups for a month.</li> <li>Montly backups for a year.</li> <li>Yearly backups forever.</li> </ul> <pre><code>extraObjects:\n- apiVersion: velero.io/v1\nkind: Schedule\nmetadata:\ncreationTimestamp: null\nname: backup-1h\nnamespace: velero\nspec:\nschedule: '@every 1h'\ntemplate:\ncsiSnapshotTimeout: 0s\nhooks: {}\nincludedNamespaces:\n- '*'\nmetadata: {}\nttl: 25h0m0s\nuseOwnerReferencesInBackup: false\nstatus: {}\n- apiVersion: velero.io/v1\nkind: Schedule\nmetadata:\ncreationTimestamp: null\nname: backup-1d\nnamespace: velero\nspec:\nschedule: '@every 24h'\ntemplate:\ncsiSnapshotTimeout: 0s\nhooks: {}\nincludedNamespaces:\n- '*'\nmetadata: {}\nttl: 730h0m0s\nuseOwnerReferencesInBackup: false\nstatus: {}\n- apiVersion: velero.io/v1\nkind: Schedule\nmetadata:\ncreationTimestamp: null\nname: backup-1m\nnamespace: velero\nspec:\nschedule: '@every 730h'\ntemplate:\ncsiSnapshotTimeout: 0s\nhooks: {}\nincludedNamespaces:\n- '*'\nmetadata: {}\nttl: 8760h0m0s\nuseOwnerReferencesInBackup: false\nstatus: {}\n- apiVersion: velero.io/v1\nkind: Schedule\nmetadata:\ncreationTimestamp: null\nname: backup-1y\nnamespace: velero\nspec:\nschedule: '@every 8760h'\ntemplate:\ncsiSnapshotTimeout: 0s\nhooks: {}\nincludedNamespaces:\n- '*'\nmetadata: {}\nttl: 0s\nuseOwnerReferencesInBackup: false\nstatus: {}\n</code></pre>"}, {"location": "velero/#monitorization", "title": "Monitorization", "text": "<p>Assuming you're using prometheus you can add the next prometheus rules in the chart:</p> <pre><code># -------------------------------------------------------------\n# --   Monitor the failures when doing backups or restores   --\n# -------------------------------------------------------------\n- alert: VeleroBackupPartialFailures\nannotations:\nmessage: Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} partialy failed backups.\nexpr: increase(velero_backup_partial_failure_total{schedule!=\"\"}[1h]) &gt; 0\nfor: 15m\nlabels:\nseverity: warning\n- alert: VeleroBackupFailures\nannotations:\nmessage: Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} failed backups.\nexpr: increase(velero_backup_failure_total{schedule!=\"\"}[1h]) &gt; 0\nfor: 15m\nlabels:\nseverity: warning\n- alert: VeleroBackupSnapshotFailures\nannotations:\nmessage: Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} failed snapshot backups.\nexpr: increase(velero_volume_snapshot_failure_total{schedule!=\"\"}[1h]) &gt; 0\nfor: 15m\nlabels:\nseverity: warning\n- alert: VeleroRestorePartialFailures\nannotations:\nmessage: Velero restore {{ $labels.schedule }} has {{ $value | humanizePercentage }} partialy failed restores.\nexpr: increase(velero_restore_partial_failure_total{schedule!=\"\"}[1h]) &gt; 0\nfor: 15m\nlabels:\nseverity: warning\n- alert: VeleroRestoreFailures\nannotations:\nmessage: Velero restore {{ $labels.schedule }} has {{ $value | humanizePercentage }} failed restores.\nexpr: increase(velero_restore_failure_total{schedule!=\"\"}[1h]) &gt; 0\nfor: 15m\nlabels:\nseverity: warning\n\n# ------------------------------------------------\n# --   Monitor the backup rate of each policy   --\n# ------------------------------------------------\n- alert: VeleroHourlyBackupFailure\nannotations:\nmessage: There are no new hourly velero backups in the last hour.\nexpr: time() - velero_backup_last_successful_timestamp{schedule=\"backup-1h\"} &gt; 3600 for: 10m\nlabels:\nseverity: warning\n- alert: VeleroDailyBackupFailure\nannotations:\nmessage: There are no new daily velero backups in the last day.\nexpr: time() - velero_backup_last_successful_timestamp{schedule=\"backup-1d\"} &gt; 3600 * 24 for: 15m\nlabels:\nseverity: warning\n- alert: VeleroMonthlyBackupFailure\nannotations:\nmessage: There are no new montly velero backups in the last month.\nexpr: time() - velero_backup_last_successful_timestamp{schedule=\"backup-1m\"} &gt; 3600 * 24 * 30\nfor: 15m\nlabels:\nseverity: warning\n- alert: VeleroYearlyBackupFailure\nannotations:\nmessage: There are no new yearly velero backups in the last year.\nexpr: time() - velero_backup_last_successful_timestamp{schedule=\"backup-1y\"} &gt; 3600 * 24 * 365\nfor: 15m\nlabels:\nseverity: warning\n\n# -------------------------------------\n# --   Monitor the backup cleaning   --\n# -------------------------------------\n- alert: VeleroBackupCleaningFailure\nannotations:\nmessage: There are more backups than the expected, deletion policy may not be working well.\n# Desired backups:\n# - 25 hourlys\n# - 31 dailys\n# - 12 monthlys\n# - 10 yearlys\nexpr: velero_backup_total &gt; (25 + 31 + 12 + 10)\nfor: 15m\nlabels:\nseverity: warning\n</code></pre>"}, {"location": "velero/#usage", "title": "Usage", "text": ""}, {"location": "velero/#restore-backups", "title": "Restore backups", "text": "<p>To get the available backups you can use:</p> <pre><code>velero get backups\n</code></pre> <p>Imagine we want to restore the backup <code>backup-1h-20230109125511</code>.</p> <p>We first need to update your backup storage location to read-only mode. This prevents backup objects from being created or deleted in the backup storage location during the restore process.</p> <pre><code>kubectl patch backupstoragelocation default \\\n--namespace velero \\\n--type merge \\\n--patch '{\"spec\":{\"accessMode\":\"ReadOnly\"}}'\n</code></pre> <p>Where <code>default</code> is the only backup storage location I have, check what's the name of your's by running <code>kubectl get backupstoragelocation -n velero</code>.</p> <p>You'd now run the <code>velero restore create</code> commands (we'll see more examples in next sections). For example:</p> <ul> <li>Create a restore with a default name <code>backup-1-&lt;timestamp&gt;</code> from backup <code>backup-1</code>.</li> </ul> <pre><code>velero restore create --from-backup backup-1\n</code></pre> <ul> <li>Create a restore with name <code>restore-1</code> from a backup called <code>backup-1</code> .</li> </ul> <pre><code>velero restore create restore-1 --from-backup backup-1\n</code></pre> <ul> <li>Create a restore from the latest successful backup triggered by schedule <code>schedule-1</code>.</li> </ul> <pre><code>velero restore create --from-schedule schedule-1\n</code></pre> <p>When you run the command they'll will show you how to monitor the evolution of the restore with commands similar to:</p> <pre><code>velero restore describe backup-1h-20230109125511-20230110145314\nvelero restore logs backup-1h-20230109125511-20230110145314\n</code></pre> <p>Once they're done, remember to reset the ReadWrite permissions on the backup location.</p> <pre><code>kubectl patch backupstoragelocation default \\\n--namespace velero \\\n--type merge \\\n--patch '{\"spec\":{\"accessMode\":\"ReadWrite\"}}'\n</code></pre>"}, {"location": "velero/#overwrite-existing-resources", "title": "Overwrite existing resources", "text": "<p>The only way I've found to do this is by removing the namespace and then restore it with <code>velero</code>. If you want to try to do it in a cleaner way keep on reading (although for me it didn't work!).</p> <p>By default, Velero is configured to be non-destructive during a restore. This means that it will never overwrite data that already exists in your cluster. When Velero attempts to create a resource during a restore, the resource being restored is compared to the existing resources on the target cluster by the Kubernetes API Server. If the resource already exists in the target cluster, Velero skips restoring the current resource and moves onto the next resource to restore, without making any changes to the target cluster.</p> <p>You can change this policy for a restore by using the <code>--existing-resource-policy</code> restore flag. The available options are <code>none</code> (default) and <code>update</code>. If you choose to update existing resources during a restore (<code>--existing-resource-policy=update</code>), Velero will attempt to update an existing resource to match the resource being restored:</p> <ul> <li> <p>If the existing resource in the target cluster is the same as the resource Velero is attempting to restore, Velero will add a <code>velero.io/backup-name</code> label with the backup name and a <code>velero.io/restore-name</code> label with the restore name to the existing resource. If patching the labels fails, Velero adds a restore error and continues restoring the next resource.</p> </li> <li> <p>If the existing resource in the target cluster is different from the backup, Velero will first try to patch the existing resource to match the backup resource. If the patch is successful, Velero will add a <code>velero.io/backup-name</code> label with the backup name and a <code>velero.io/restore-name</code> label with the restore name to the existing resource. If the patch fails, Velero adds a restore warning and tries to add the <code>velero.io/backup-name</code> and <code>velero.io/restore-name</code> labels on the resource. If the labels patch also fails, then Velero logs a restore error and continues restoring the next resource.</p> </li> </ul> <p>Even with these flags the restore of PersistentVolumeClaim doesn't work.</p>"}, {"location": "velero/#restore-only-a-namespace", "title": "Restore only a namespace", "text": "<pre><code>velero restore create --include-namespaces monitoring\n</code></pre>"}, {"location": "velero/#restore-only-a-subsection-of-the-backup", "title": "Restore only a subsection of the backup", "text": "<p>You can list the backed up resources with:</p> <pre><code>velero describe backups backup-1h-20230109125511\n</code></pre> <p>Then you can create a restore for only <code>persistentvolumeclaims</code> and <code>persistentvolumes</code> within a backup.</p> <pre><code>velero restore create --from-backup backup-2 --include-resources persistentvolumeclaims,persistentvolumes\n</code></pre>"}, {"location": "velero/#restore-from-a-snapshot-not-done-by-velero", "title": "Restore from a snapshot not done by velero", "text": "<p>If you want to use an EBS snapshot that is not managed by <code>velero</code> you need to:</p> <ul> <li>Locate a <code>velero</code> backup which has done a backup of the resources you want to restore. Imagine it's <code>backup-1</code>. </li> <li>Go to the <code>backup-1</code> directory in the S3 bucket where velero stores the data.</li> <li>Download and decompress the <code>backup-1-volumesnapshots.json.gz</code> file.</li> <li>Edit the result <code>json</code> and restore the <code>snap-.*</code> strings of the <code>pvc</code> that you want to restore for the ones that are not managed by velero.</li> <li>Compress the file and upload it to the S3 bucket.</li> <li>Restore the backup</li> </ul> <p>Keep in mind that if you are trying to restore a backup created by an EBS lifecycle hook you'll receive an error when restoring because these snapshots have a tag that starts with <code>aws:</code> which is reserved for AWS only. The solution is to copy the snapshot into a new one, assign a tag, for example <code>Name</code>, and use that snapshot instead. If you don't define any tag you'll get another error :/.</p>"}, {"location": "velero/#overview-of-velero", "title": "Overview of Velero", "text": "<p>Each Velero operation \u2013 on-demand backup, scheduled backup, restore \u2013 is a custom resource, defined with a Kubernetes Custom Resource Definition (CRD) and stored in etcd. Velero also includes controllers that process the custom resources to perform backups, restores, and all related operations.</p> <p>You can back up or restore all objects in your cluster, or you can filter objects by type, namespace, and/or label.</p>"}, {"location": "velero/#backups", "title": "Backups", "text": ""}, {"location": "velero/#on-demand-backups", "title": "On demand backups", "text": "<p>The backup operation:</p> <ul> <li>Uploads a tarball of copied Kubernetes objects into cloud object storage.</li> <li>Calls the cloud provider API to make disk snapshots of persistent volumes, if specified.</li> </ul> <p>You can optionally specify backup hooks to be executed during the backup. For example, you might need to tell a database to flush its in-memory buffers to disk before taking a snapshot. </p> <p>Note that cluster backups are not strictly atomic. If Kubernetes objects are being created or edited at the time of backup, they might not be included in the backup. The odds of capturing inconsistent information are low, but it is possible.</p>"}, {"location": "velero/#scheduled-backups", "title": "Scheduled backups", "text": "<p>The schedule operation allows you to back up your data at recurring intervals. You can create a scheduled backup at any time, and the first backup is then performed at the schedule\u2019s specified interval. These intervals are specified by a Cron expression.</p> <p>Velero saves backups created from a schedule with the name <code>&lt;SCHEDULE NAME&gt;-&lt;TIMESTAMP&gt;</code>, where <code>&lt;TIMESTAMP&gt;</code> is formatted as <code>YYYYMMDDhhmmss</code>.</p>"}, {"location": "velero/#backup-workflow", "title": "Backup workflow", "text": "<p>When you run <code>velero backup create test-backup</code>:</p> <ul> <li>The Velero client makes a call to the Kubernetes API server to create a <code>Backup</code> object.</li> <li>The <code>BackupController</code> notices the new <code>Backup</code> object and performs validation.</li> <li>The <code>BackupController</code> begins the backup process. It collects the data to back up by querying the API server for resources.</li> <li>The <code>BackupController</code> makes a call to the object storage service \u2013 for example, AWS S3 \u2013 to upload the backup file.</li> </ul>"}, {"location": "velero/#set-expiration-date-of-a-backup", "title": "Set expiration date of a backup", "text": "<p>When you create a backup, you can specify a TTL (time to live) by adding the flag <code>--ttl &lt;DURATION&gt;</code>. If Velero sees that an existing backup resource is expired, it removes:</p> <ul> <li>The backup resource</li> <li>The backup file from cloud object storage</li> <li>All <code>PersistentVolume</code> snapshots</li> <li>All associated Restores</li> </ul> <p>The TTL flag allows the user to specify the backup retention period with the value specified in hours, minutes and seconds in the form <code>--ttl 24h0m0s</code>. If not specified, a default TTL value of 30 days will be applied.</p> <p>If backup fails to delete, a label <code>velero.io/gc-failure=&lt;Reason&gt;</code> will be added to the backup custom resource.</p> <p>You can use this label to filter and select backups that failed to delete.</p>"}, {"location": "velero/#restores", "title": "Restores", "text": "<p>The restore operation allows you to restore all of the objects and persistent volumes from a previously created backup. You can also restore only a filtered subset of objects and persistent volumes. </p> <p>By default, backup storage locations are created in read-write mode. However, during a restore, you can configure a backup storage location to be in read-only mode, which disables backup creation and deletion for the storage location. This is useful to ensure that no backups are inadvertently created or deleted during a restore scenario.</p> <p>A restored object includes a label with key velero.io/restore-name and value . <p>You can optionally specify restore hooks to be executed during a restore or after resources are restored. For example, you might need to perform a custom database restore operation before the database application containers start.</p>"}, {"location": "velero/#restore-workflow", "title": "Restore workflow", "text": "<p>When you run <code>velero restore create</code>:</p> <ul> <li>The Velero client makes a call to the Kubernetes API server to create a <code>Restore</code> object.</li> <li>The <code>RestoreController</code> notices the new <code>Restore</code> object and performs validation.</li> <li>The <code>RestoreController</code> fetches the backup information from the object storage service. It then runs some preprocessing on the backed up resources to make sure the resources will work on the new cluster. For example, using the backed-up API versions to verify that the restore resource will work on the target cluster.</li> <li>The <code>RestoreController</code> starts the restore process, restoring each eligible resource one at a time.</li> </ul> <p>By default, Velero performs a non-destructive restore, meaning that it won\u2019t delete any data on the target cluster. If a resource in the backup already exists in the target cluster, Velero will skip that resource. You can configure Velero to use an update policy instead using the <code>--existing-resource-policy</code> restore flag. When this flag is set to update, Velero will attempt to update an existing resource in the target cluster to match the resource from the backup.</p>"}, {"location": "velero/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> <li>Home</li> </ul>"}, {"location": "versioning/", "title": "Program versioning", "text": "<p>The Don't Repeat Yourself principle encourages developers to abstract code into a separate components and reuse them rather than write it over and over again. If this happens across the system, the best practice is to put it inside a package that lives on its own (a library) and then pull it in from the applications when required.</p> <p>!!! note \"This article is heavily based on the posts in the references sections, the main credit goes to them, I've just refactored all together under my personal opinion.\"</p> <p>As most of us can\u2019t think of every feature that the library might offer or what bugs it might contain, these packages tend to evolve. Therefore, we need some mechanism to encode these evolutions of the library, so that downstream users can understand how big the change is. Most commonly, developers use three methods:</p> <ul> <li>A version number.</li> <li>A changelog.</li> <li>The git history.</li> </ul> <p>The version change is used as a concise way for the project to communicate this evolution, and it's what we're going to analyze in this article. However, encoding all the information of a change into a number switch has proven to be far from perfect.</p> <p>That's why keeping a good and detailed changelog makes a lot of sense, as it will better transmit that intent, and what will be the impact of upgrading. Once again, this falls into the same problem as before, while a change log is more descriptive, it still only tells you what changes (or breakages) a project intended to make, it doesn\u2019t go into any detail about unintended consequences of changes made. Ultimately, a change log\u2019s accuracy is no different than that of the version itself, it's just (hopefully!) more detailed.</p> <p>Fundamentally, any indicator of change that isn\u2019t a full diff is just a lossy encoding of that change. You can't expect though to read all the diffs of the libraries that you use, that's why version numbers and changelogs make a lot of sense. We just need to be aware of the limits of each system.</p> <p>That being said, you'll use version numbers in two ways:</p> <ul> <li> <p>As a producer of applications and libraries where you\u2019ll have to decide what     versioning system to use.</p> </li> <li> <p>As a consumer of dependencies, you\u2019ll have to express what versions of a given library your     application/library is compatible.</p> </li> </ul>"}, {"location": "versioning/#deciding-what-version-system-to-use-for-your-programs", "title": "Deciding what version system to use for your programs", "text": "<p>The two most popular versioning systems are:</p> <ul> <li> <p>Semantic Versioning: A way to define your program's     version based on the type of changes you've introduced.</p> </li> <li> <p>Calendar Versioning: A versioning convention based     on your project's release calendar, instead of arbitrary numbers.</p> </li> </ul> <p>Each has it's advantages and disadvantages. From a consumer perspective, I think that projects should generally default to SemVer-ish, following the spirit of the documentation rather than the letter of the specification because:</p> <ul> <li>Your version number becomes a means of communicating your changes intents to     your end users.</li> <li>If you use the semantic versioning commit message     guidelines, you are more     likely to have a useful git history and can automatically maintain the     project's changelog.</li> </ul> <p>There are however, corner cases where CalVer makes more sense:</p> <ul> <li> <p>You\u2019re tracking something that is already versioned using dates or for which     the version number can only really be described as a point in time release.     The <code>pytz</code> is a good example of both of these cases, the Olson TZ database is     versioned using a date based scheme and the information that it is providing     is best represented as a snapshot of the state of what timezones were like     at a particular point in time.</p> </li> <li> <p>Your project is going to be breaking compatibility in every release and you do     not want to make any promises of compatibility. You should still document     this fact in your README, but if there\u2019s no promise of compatibility between     releases, then there\u2019s no information to be communicated in the version     number.</p> </li> <li> <p>Your project is never going to intentionally breaking compatibility in     a release, and you strive to always maintain compatibility. Projects can     always just use the latest version of your software. Your changes will     only ever be additive, and if you need to change your API, you\u2019ll do     something like leave the old API intact, and add a new API with the new     semantics. An example of this case would be the Ubuntu versions.</p> </li> </ul>"}, {"location": "versioning/#how-to-evolve-your-code-version", "title": "How to evolve your code version", "text": "<p>Assuming you're using Semantic Versioning you can improve your code evolution by:</p> <ul> <li>Avoid becoming a ZeroVer package.</li> <li>Use Warnings to avoid major changes</li> </ul>"}, {"location": "versioning/#avoid-becoming-a-zerover-package", "title": "Avoid becoming a ZeroVer package", "text": "<p>Once your project reach it's first level of maturity you should release <code>1.0.0</code> to avoid falling into ZeroVer. For example you can use one of the next indicators:</p> <ul> <li>If you're frequently using it and haven't done any breaking change in 3 months.</li> <li>If 30 users are depending on it. For example counting the project stars.</li> </ul>"}, {"location": "versioning/#use-warnings-to-avoid-major-changes", "title": "Use Warnings to avoid major changes", "text": "<p>Semantic versioning uses the major version to defend against breaking changes, and at the same offers maintainers the freedom to evolve the library without breaking users. Nevertheless, this does not seem to work that well.</p> <p>So it's better to use Warnings to avoid major changes.</p>"}, {"location": "versioning/#communicate-with-your-users", "title": "Communicate with your users", "text": "<p>You should warn your users not to blindly trust that any version change is not going to break their code and that you assume that they are actively testing the package updates.</p>"}, {"location": "versioning/#keep-the-requires-python-metadata-updated", "title": "Keep the <code>Requires-Python</code> metadata updated", "text": "<p>It's important not to upper cap the Python version and to maintain the <code>Requires-Python</code> package metadata updated. Dependency solvers will use this information to fetch the correct versions of the packages for the users.</p>"}, {"location": "versioning/#deciding-how-to-manage-the-versions-of-your-dependencies", "title": "Deciding how to manage the versions of your dependencies", "text": "<p>As a consumer of other dependencies, you need to specify in your package what versions does your code support. The traditional way to do it is by pinning those versions in your package definition. For example in python it lives either in the <code>setup.py</code> or in the <code>pyproject.toml</code>.</p>"}, {"location": "versioning/#lower-version-pinning", "title": "Lower version pinning", "text": "<p>When you're developing a program that uses a dependency, you usually don't know if a previous version of that dependency is compatible with your code, so in theory it makes sense to specify that you don't support any version smaller than the actual with something like <code>&gt;=1.2</code>. If you follow this train of thought, each time you update your dependencies, you should update your lower pins, because you're only running your test suite on those versions. If the libraries didn't do upper version pinning, then there would be no problem as you wouldn't be risking to get into version conflicts.</p> <p>A more relaxed approach would be not to update the pins when you update, in that case, you should run your tests both against the oldest possible values and the newest to ensure that everything works as expected. This way you'll be more kind to your users as you'll reduce possible version conflicts, but it'll add work to the maintainers.</p> <p>The most relaxed approach would be not to use pins at all, it will suppress most of the version conflicts but you won't be sure that the dependencies that your users are using are compatible with your code.</p> <p>Think about how much work you want to invest in maintaining your package and how much stability you want to offer before you choose one or the other method. Once you've made your choice, it would be nice if you communicate it to your users through your documentation.</p>"}, {"location": "versioning/#upper-version-pinning", "title": "Upper version pinning", "text": "<p>Program maintainers often rely on upper version pinning to guarantee that their code is not going to be broken due to a dependency update.</p> <p>We\u2019ll cover the valid use cases for capping after this section. But, just to be clear, if you know you do not support a new release of a library, then absolutely, go ahead and cap it as soon as you know this to be true. If something does not work, you should cap (or maybe restrict a single version if the upstream library has a temporary bug rather than a design direction that\u2019s causing the failure). You should also do as much as you can to quickly remove the cap, as all the downsides of capping in the next sections still apply.</p> <p>The following will assume you are capping before knowing that something does not work, but just out of general principle, like Poetry recommends and defaults to with <code>poetry add</code>. In most cases, the answer will be don\u2019t. For simplicity, I will also assume you are being tempted to cap to major releases (<code>^1.0.0</code> in Poetry or <code>~=1.0</code> in all other tooling that follows Python standards via PEP 440) following the false security that only <code>major</code> changes can to break your code. If you cap to minor versions <code>~=1.0.0</code>, this is much worse, and the arguments below apply even more strongly.</p>"}, {"location": "versioning/#version-limits-break-code-too", "title": "Version limits break code too", "text": "<p>Following this path will effectively opt you out of bug fixes and security updates, as most of the projects only maintain the latest version of their program and what worse, you'll be preventing everyone using your library not to use the latest version of those libraries. All in exchange to defend yourself against a change that in practice will rarely impact you. Sure, you can move on to the next version of each of your pins each time they increase a major via something like <code>Click&gt;=8, &lt;9</code>. However, this involves manual intervention on their code, and you might not have the time to do this for every one of your projects.</p> <p>If we add the fact that not only major but any other version change may break your code due to unintended changes and the difference in the change categorization, then you can treat all changes equally, so it makes no sense on pinning the major version either.</p> <p>This is specially useless when you add dependencies that follow CalVer. <code>poetry add</code> packaging will still do <code>^21</code> for the version it adds. You shouldn\u2019t be capping versions, but you really shouldn\u2019t be capping CalVer.</p>"}, {"location": "versioning/#semver-never-promises-to-break-your-code", "title": "SemVer never promises to break your code", "text": "<p>A really easy but incorrect generalization of the SemVer rules is \u201ca major version will break my code\u201d. Even if the library follows true SemVer perfectly, a major version bump does not promise to break downstream code. It promises that some downstream code may break. If you use <code>pytest</code> to test your code, for example, the next major version will be very unlikely to break. If you write a <code>pytest</code> extension, however, then the chances of something breaking are much higher (but not 100%, maybe not even 50%). Quite ironically, the better a package follows SemVer, the smaller the change will trigger a major version, and therefore the less likely a major version will break a particular downstream code.</p> <p>As a general rule, if you have a reasonably stable dependency, and you only use the documented API, especially if your usage is pretty light/general, then a major update is extremely unlikely to break your code. It\u2019s quite rare for light usage of a library to break on a major update. It can happen, of course, but is unlikely. If you are using something very heavily, if you are working on a framework extension, or if you use internals that are not publicly documented, then your chances of breaking on a major release are much higher. Python has a culture of producing <code>FutureWarnings</code>, <code>DeprecationWarnings</code>, or <code>PendingDeprecationWarnings</code> (make sure they are on in your testing, and turn into errors), good libraries will use them.</p>"}, {"location": "versioning/#version-conflicts", "title": "Version conflicts", "text": "<p>And then there\u2019s another aspect version pinning will introduce: version conflicts.</p> <p>An application or library will have a set of libraries it depends on directly. These are libraries you\u2019re directly importing within the application/library you\u2019re maintaining, but then the libraries themselves may rely on other libraries. This is known as a transitive dependency. Very soon, you\u2019ll get to a point where two different components use the same library, and both of them might express version constraints on it.</p> <p>For example, consider the case of <code>tenacity</code>: a general-purpose retrying library. Imagine you were using this in your application, and being a religious follower of semantic versioning, you\u2019ve pinned it to the version that was out when you created the app in early 2018: <code>4.11</code>. The constraint would specify version <code>4.11</code> or later, but less than the next major version <code>5</code>.</p> <p>At the same time, you also connect to an HTTP service. This connection is handled by another library, and the maintainer of that decided to also use <code>tenacity</code> to offer automatic retry functionality. They pinned it similarly following the semantic versioning convention. Back in 2018, this caused no issues. But then August comes, and version <code>5.0</code> is released.</p> <p>The service and its library maintainers have a lot more time on their hands (perhaps because they are paid to do so), so they quickly move to version <code>5.0</code>. Or perhaps they want to use a feature from the new major version. Now they introduce the pin greater than five but less than six on tenacity. Their public interface does not change at all at this point, so they do not bump their major version. It\u2019s just a patch release.</p> <p>Python can only have one version of a library installed at a given time. At this point, there is a version conflict. You\u2019re requesting a version between four and five, while the service library is requesting a version between five and six. Both constraints cannot be satisfied.</p> <p>If you use a version of pip older than <code>20.2</code> (the release in which it added a dependency resolver) it will just install a version matching the first constraint it finds and ignore any subsequent constraints. Versions of pip after <code>20.2</code> would fail with an error indicating that the constraint cannot be satisfied.</p> <p>Either way, your application no longer works. The only way to make it work is to either pin the service library down to the last working patch number, or upgrade your version pinning of <code>tenacity</code>. This is generating extra work for you with minimal benefit. Often it might not be even possible to use two conflicting libraries until one of them relaxes their requirements. It also means you must support a wide version range; ironically. If you update to requiring `tenacity</p> <p>5<code>, your update can\u2019t be installed with another library still on</code>4.11<code>. So you have to support</code>tenacity&gt;=4.11,&lt;6` for a while until most libraries have similarly updated.</p> <p>And for those who might think this doesn\u2019t happen often, let me say that <code>tenacity</code> released another major version a year later in November 2019. Thus, the cycle starts all over again. In both cases, your code most likely did not need to change at all, as just a small part of their public API changed. In my experience, this happens a lot more often than when a major version bump breaks you. I've found myself investing most of my project maintenance time opening issues in third party dependencies to update their pins.</p>"}, {"location": "versioning/#it-doesnt-scale", "title": "It doesn\u2019t scale", "text": "<p>If you have a single library that doesn\u2019t play well, then you probably will get a working solve easily (this is one reason that this practice doesn\u2019t seem so bad at first). If more packages start following this tight capping, however, you end up with a situation where things simply cannot solve. A moderately sized application can have a hundred or more dependencies when expanded, so such issues in my experience start to appear every few months. You need only 5-6 of such cases for every 100 libraries for this issue to pop up every two months on your plate. And potentially for a multiple of your applications.</p> <p>The entire point of packaging is to allow you to get lots of packages that each do some job for you. We should be trying to make it easy to be able to add dependencies, not harder.</p> <p>The implication of this is you should be very careful when you see tight requirements in packages and you have any upper bound caps anywhere in the dependency chain. If something caps dependencies, there\u2019s a very good chance adding two such packages will break your solve, so you should pick just one, or just avoid them altogether, so you can add one in the future. This is a good rule, actually: Never add a library to your dependencies that has excessive upper bound capping. When I have failed to follow this rule for a larger package, I have usually come to regret it.</p> <p>If you are doing the capping and are providing a library, you now have a commitment to quickly release an update, ideally right before any capped dependency comes out with a new version. Though if you cap, how to you install development versions or even know when a major version is released? This makes it harder for downstream packages to update, because they have to wait for all the caps to be moved for all upstream.</p>"}, {"location": "versioning/#it-conflicts-with-tight-lower-bounds", "title": "It conflicts with tight lower bounds", "text": "<p>A tight lower bound is only bad if packages cap upper bounds. If you can avoid upper-cap packages, you can accept tight lower bound packages, which are much better; better features, better security, better compatibility with new hardware and OS\u2019s. A good packaging system should allow you to require modern packages; why develop for really old versions of things if the packaging system can upgrade them? But a upper bound cap breaks this. Hopefully anyone who is writing software and pushing versions will agree that tight lower limits are much better than tight upper limits, so if one has to go, it\u2019s the upper limits.</p> <p>It is also rather rare that packages solve for lower bounds in CI (I would love to see such a solver become an option, by the way!), so setting a tight lower bound is one way to avoid rare errors when old packages are cached that you don\u2019t actually support. CI almost never has a cache of old packages, but users do.</p>"}, {"location": "versioning/#capping-dependencies-hides-incompatibilities", "title": "Capping dependencies hides incompatibilities", "text": "<p>Another serious side effect of capping dependencies is that you are not notified properly of incoming incompatibilities, and you have to be extra proactive in monitoring your dependencies for updates. If you don\u2019t cap your dependencies, you are immediately notified when a dependency releases a new version, probably by your CI, the first time you build with that new version. If you are running your CI with the <code>--dev</code> flag on your <code>pip install</code> (uncommon, but probably a good idea), then you might even catch and fix the issue before a release is even made. If you don\u2019t do this, however, then you don\u2019t know about the incompatibility until (much) later.</p> <p>If you are not following all of your dependencies, you might not notice that you are out of date until it\u2019s both a serious problem for users and it\u2019s really hard for you to tell what change broke your usage because several versions have been released. While I\u2019m not a huge fan of Google\u2019s live-at-head philosophy (primarily because it has heavy requirements not applicable for most open-source projects), I appreciate and love catching a dependency incompatibility as soon as you possibly can; the smaller the change set, the easier it is to identify and fix the issue.</p>"}, {"location": "versioning/#capping-all-dependencies-hides-real-incompatibilities", "title": "Capping all dependencies hides real incompatibilities", "text": "<p>If you see <code>X&gt;=1.1</code>, that tells you that the package is using features from <code>1.1</code> and do not support <code>1.0</code>. If you see <code>X&lt;1.2</code>, this should tell you that there\u2019s a problem with <code>1.2</code> and the current software, specifically something they know the dependency will not fix/revert. Not that you just capped all your dependencies and have no idea if that will or won\u2019t work at all. A cap should be like a TODO; it\u2019s a known issue that needs to be worked on soon. As in yesterday.</p>"}, {"location": "versioning/#pinning-the-python-version-is-special", "title": "Pinning the Python version is special", "text": "<p>Another practice pushed by Poetry is adding an upper cap to the Python version. This is misusing a feature designed to help with dropping old Python versions to instead stop new Python versions from being used. \u201cScrolling back\u201d through older releases to find the newest version that does not restrict the version of Python being used is exactly the wrong behavior for an upper cap, and that is what the purpose of this field is. Current versions of pip do seem to fail when this is capped, rather than scrolling back to find an older uncapped version, but I haven\u2019t found many libraries that have \u201cadded\u201d this after releasing to be sure of that.</p> <p>To be clear, this is very different from a library: specifically, you can\u2019t downgrade your Python version if this is capped to something below your current version. You can only fail. So this does not \u201cfix\u201d something by getting an older, working version, it only causes hard failures if it works the way you might hope it does. This means instead of seeing the real failure and possibly helping to fix it, users just see a <code>Python doesn\u2019t match</code> error. And, most of the time, it\u2019s not even a real error; if you support Python <code>3.x</code> without warnings, you should support Python <code>3.x+1</code> (and <code>3.x+2</code>, too).</p> <p>Capping to <code>&lt;4</code> (something like <code>^3.6</code> in Poetry) is also directly in conflict with the Python developer\u2019s own statements; they promise the <code>3-&gt;4</code> transition will be more like the <code>1-&gt;2</code> transition than the <code>2-&gt;3</code> transition. When Python 4 does come out, it will be really hard to even run your CI on 4 until all your dependencies uncap. And you won\u2019t actually see the real failures, you\u2019ll just see incompatibility errors, so you won\u2019t even know what to report to those libraries. And this practice makes it hard to test development versions of Python.</p> <p>And, if you use Poetry, as soon as someone caps the Python version, every Poetry project that uses it must also cap, even if you believe it is a detestable practice and confusing to users. It is also wrong unless you fully pin the dependency that forced the cap. If the dependency drops it in a patch release or something else you support, you no longer would need the cap.</p>"}, {"location": "versioning/#applications-are-slightly-different", "title": "Applications are slightly different", "text": "<p>If you have a true application (that is, if you are not intending your package to be used as a library), upper version constraints are much less problematic, and some of the reasons above don't apply. This due to two reasons.</p> <p>First, if you are writing a library, your \u201cusers\u201d are specifying your package in their dependencies; if an update breaks them, they can always add the necessary exclusion or cap for you to help end users. It\u2019s a leaky abstraction, they shouldn\u2019t have to care about what your dependencies are, but when capping interferes with what they can use, that\u2019s also a leaky and unfixable abstraction. For an application, the \u201cusers\u201d are more likely to be installing your package directly, where the users are generally other developers adding to requirements for libraries.</p> <p>Second, for an app that is installed from PyPI, you are less likely to have to worry about what else is installed (the other issues are still true). Many (most?) users will not be using <code>pipx</code> or a fresh virtual environment each time, so in practice, you\u2019ll still run into problems with tight constraints, but there is a workaround (use <code>pipx</code>, for example). You still are still affected by most of the arguments above, though, so personally I\u2019d still not recommend adding untested caps.</p>"}, {"location": "versioning/#when-is-it-ok-to-set-an-upper-limit", "title": "When is it ok to set an upper limit?", "text": "<p>Valid reasons to add an upper limit are:</p> <ul> <li> <p>If a dependency is known to be broken, block out the broken version. Try very     hard to fix this problem quickly, then remove the block if it\u2019s fixable on     your end. If the fix happens upstream, excluding just the broken version is     fine (or they can \u201cyank\u201d the bad release to help everyone).</p> </li> <li> <p>If you know upstream is about to make a major change that is very likely to     break your usage, you can cap. But try to fix this as quickly as possible so     you can remove the cap by the time they release. Possibly add development     branch/release testing until this is resolved.</p> </li> <li> <p>If upstream asks users to cap, then I still don\u2019t like it, but it is okay if     you want to follow the upstream recommendation. You should ask yourself: do     you want to use a library that may intentionally break you and require     changes on your part without help via deprecation periods? A one-time major     rewrite might be an acceptable reason. Also, if you are upstream, it is very     un-Pythonic to break users without deprecation warnings first. Don\u2019t do it     if possible.</p> </li> <li> <p>If you are writing an extension for an ecosystem/framework (pytest extension,     Sphinx extension, Jupyter extension, etc), then capping on the major version     of that library is acceptable. Note this happens once - you have a single     library that can be capped. You must release as soon as you possibly can     after a new major release, and you should be closely following upstream</p> <ul> <li>probably using development releases for testing, etc. But doing this for one library is probably manageable.</li> </ul> </li> <li> <p>You are releasing two or more libraries in sync with each other. You control     the release cadence for both libraries. This is likely the \u201cbest\u201d reason to     cap. Some of the above issues don\u2019t apply in this case - since you control     the release cadence and can keep them in sync.</p> </li> <li> <p>You depend on private internal details of a library. You should also rethink     your choices - this can be broken in a minor or patch release, and often is.</p> </li> </ul> <p>If you cap in these situations, I wouldn\u2019t complain, but I wouldn\u2019t really recommend it either:</p> <ul> <li> <p>If you have a heavy dependency on a library, maybe cap. A really large API     surface is more likely to be hit by the possible breakage.</p> </li> <li> <p>If a library is very new, say on version 1 or a ZeroVer library, and has very     few users, maybe cap if it seems rather unstable. See if the library authors     recommend capping - they might plan to make a large change if it\u2019s early in     development. This is not blanket permission to cap ZeroVer libraries!</p> </li> <li> <p>If a library looks really unstable, such as having a history of making big     changes, then cap. Or use a different library. Even better, contact the     authors, and make sure that your usage is safe for the near future.</p> </li> </ul>"}, {"location": "versioning/#summary", "title": "Summary", "text": "<p>No more than 1-2 of your dependencies should fall into the categories of acceptable upper pinning. In every other case, do not cap your dependences, specially if you are writing a library! You could probably summarize it like this: if there\u2019s a high chance (say <code>75%+</code>) that a dependency will break for you when it updates, you can add a cap. But if there\u2019s no reason to believe it will break, do not add the cap; you will cause more severe (unfixable) pain than the breakage would.</p> <p>If you have an app instead of a library, you can be cautiously more relaxed, but not much. Apps do not have to live in shared environments, though they might.</p> <p>Notice many of the above instances are due to very close/special interaction with a small number of libraries (either a plugin for a framework, synchronized releases, or very heavy usage). Most libraries you use do not fall into this category. Remember, library authors don\u2019t want to break users who follow their public API and documentation. If they do, it\u2019s for a special and good reason (or it is a bad library to depend on). They will probably have a deprecation period, produce warnings, etc.</p> <p>If you do version cap anything, you are promising to closely follow that dependency, update the cap as soon as possible, follow beta or RC releases or the development branch, etc. When a new version of a library comes out, end users should be able to start trying it out. If they can\u2019t, your library\u2019s dependencies are a leaky abstraction (users shouldn\u2019t have to care about what dependencies libraries use).</p>"}, {"location": "versioning/#automatically-upgrade-and-test-your-dependencies", "title": "Automatically upgrade and test your dependencies", "text": "<p>Now that you have minimized the upper bound pins and defined the lower bound pins you need to ensure that your code works with the latest version of your dependencies.</p> <p>One way to do it is running a periodic cronjob (daily probably) that updates your requirements lock, optionally your lower bounds, and checks that the tests keep on passing.</p>"}, {"location": "versioning/#monitor-your-dependencies-evolution", "title": "Monitor your dependencies evolution", "text": "<p>You rely on your dependencies to fulfill critical parts of your package, therefore it makes sense to know how they are changing in order to:</p> <ul> <li>Change your package to use new features.</li> <li>Be aware of the new possibilities to solve future problems.</li> <li>Get an idea of the dependency stability and future.</li> </ul> <p>Depending on how much you rely on the dependency, different levels of monitorization can be used, ordered from least to most you could check:</p> <ul> <li> <p>Release messages: Some projects post them in their blogs, you can use     their RSS feed to keep updated. If the project uses Github to create the     release messages, you can get notifications on just those release messages.</p> <p>If the project uses Semantic Versioning, it can help you dismiss all changes that are <code>micro</code>, review without urgency the <code>minor</code> and prioritize the <code>major</code> ones. If all you're given is a CalVer style version then you're forced to dedicate the same time to each of the changes.</p> </li> <li> <p>Changelog: if you get a notification of a new release, head to the changelog     to get a better detail of what has changed.</p> </li> <li> <p>Pull requests: Depending on the project release workflow, it may     take some time from a change to be accepted until it's published under a new     release, if you monitor the pull requests, you get an early idea of what     will be included in the new version.</p> </li> <li> <p>Issues: Most of changes introduced in a project are created from the outcome     of a repository issue, where a user expresses their desire to introduce the     change. If you monitor them you'll get the idea of how the project will     evolve in the future.</p> </li> </ul>"}, {"location": "versioning/#summary_1", "title": "Summary", "text": "<p>Is semantic versioning irrevocably broken? Should it never be used? I don\u2019t think so. It still makes a lot of sense where there are ample resources to maintain multiple versions in parallel. A great example of this is Django. However, it feels less practical for projects that have just a few maintainers.</p> <p>In this case, it often leads to opting people out of bug fixes and security updates. It also encourages version conflicts in environments that can\u2019t have multiple versions of the same library, as is the case with Python. Furthermore, it makes it a lot harder for developers to learn from their mistakes and evolve the API to a better place. Rotten old design decisions will pull down the library for years to come.</p> <p>A better solution at hand can be using CalVer and a time-window based warning system to evolve the API and remove old interfaces. Does it solve all problems? Absolutely not.</p> <p>One thing it makes harder is library rewrites. For example, consider virtualenv's recent rewrite. Version 20 introduced a completely new API and changed some behaviours to new defaults. For such use cases in a CalVer world, you would likely need to release the rewritten project under a new name, such as virtualenv2. Then again, such complete rewrites are extremely rare (in the case of virtualenv, it involved twelve years passing).</p> <p>No version scheme will allow you to predict with any certainty how compatible your software will be with potential future versions of your dependencies. The only reasonable choices are for libraries to choose minimum versions/excluded versions only, never maximum versions. For applications, do the same thing, but also add in a lock file of known, good versions with exact pins (this is the fundamental difference between install_requires and requirements.txt).</p>"}, {"location": "versioning/#this-doesnt-necessarily-apply-to-other-ecosystems", "title": "This doesn't necessarily apply to other ecosystems", "text": "<p>All of  this advice coming from me does not necessarily apply to all other packaging ecosystems. Python's flat dependency management has its pros and cons, hence why some other ecosystems do things differently.</p>"}, {"location": "versioning/#references", "title": "References", "text": "<ul> <li>Bernat post on versioning</li> <li>Should You Use Upper Bound Version Constraints? by Henry Schreiner</li> <li>Why I don't like SemVer anymore by Snarky</li> <li>Versioning Software by donald stufft</li> </ul>"}, {"location": "vim/", "title": "Vim", "text": "<p>Vim is a lightweight keyboard driven editor. It's the road to fly over the keyboard as it increases productivity and usability.</p> <p>If you doubt between learning emacs or vim, go with emacs with spacemacs</p> <p>I am a power vim user for more than 10 years, and seeing what my friends do with emacs, I suggest you to learn it while keeping the vim movement.</p> <p>Spacemacs is a preconfigured Emacs with those bindings and a lot of more stuff, but it's a good way to start.</p>"}, {"location": "vim/#vi-vs-vim-vs-neovim", "title": "Vi vs Vim vs Neovim", "text": "<p>TL;DR: Use Neovim</p> <p>Small comparison:</p> <ul> <li>Vi</li> <li>Follows the Single Unix Specification and POSIX.</li> <li>Original code written by Bill Joy in 1976.</li> <li>BSD license.</li> <li> <p>Doesn't even have a git repository <code>-.-</code>.</p> </li> <li> <p>Vim</p> </li> <li>Written by Bram Moolenaar in 1991.</li> <li>Vim is free and open source software, license is compatible with the GNU General Public License.</li> <li>C: 47.6 %, Vim Script: 44.8%, Roff 1.9%, Makefile 1.7%, C++ 1.2%</li> <li>Commits: 7120, Branch: 1, Releases: 5639, Contributor: 1</li> <li> <p>Lines: 1.295.837</p> </li> <li> <p>Neovim</p> </li> <li>Written by the community from 2014</li> <li>Published under the Apache 2.0 license</li> <li>Commits: 7994, Branch 1, Releases: 9, Contributors: 303</li> <li>Vim script: 46.9%, C:38.9%, Lua 11.3%, Python 0.9%, C++ 0.6%</li> <li>Lines: 937.508 (27.65% less code than vim)</li> <li>Refactor: Simplify maintenance and encourage contributions</li> <li>Easy update, just symlinks</li> <li>Ahead of vim, new features inserted in Vim 8.0 (async)</li> </ul> <p>Neovim is a refactor of Vim to make it viable for another 30 years of hacking.</p> <p>Neovim very intentionally builds on the long history of Vim community knowledge and user habits. That means \u201cswitching\u201d from Vim to Neovim is just an \u201cupgrade\u201d.</p> <p>From the start, one of Neovim\u2019s explicit goals has been simplify maintenance and encourage contributions.  By building a codebase and community that enables experimentation and low-cost trials of new features..</p> <p>And there\u2019s evidence of real progress towards that ambition. We\u2019ve successfully executed non-trivial \u201coff-the-roadmap\u201d patches: features which are important to their authors, but not the highest priority for the project.</p> <p>These patches were included because they:</p> <ul> <li>Fit into existing conventions/design.</li> <li>Included robust test coverage (enabled by an advanced test framework and CI).</li> <li>Received thoughtful review by other contributors.</li> </ul> <p>One downside though is that it's not able to work with \"big\" files for me 110kb file broke it. Although after some debugging it worked.</p>"}, {"location": "vim/#installation", "title": "Installation", "text": "<p>The version of <code>nvim</code> released by debian is too old, use the latest by downloading it directly from the releases page and unpacking it somewhere in your home and doing a link to the <code>bin/nvim</code> file somewhere in your <code>$PATH</code>.</p>"}, {"location": "vim/#configuration", "title": "Configuration", "text": "<p>Nvim moved away from vimscript and now needs to be configured in lua. You can access the config file in <code>~/.config/nvim/init.lua</code>. It's not created by default so you need to do it yourself.</p> <p>To access the editor's setting we need to use the global variable <code>vim</code>. Okay, more than a variable this thing is a module. It has an <code>opt</code> property to change the program options.  This is the syntax you should follow.</p> <pre><code>vim.opt.option_name = value\n</code></pre> <p>Where <code>option_name</code> can be anything in this list. And value must be whatever that option expects. You can also see the list with <code>:help option-list</code>.</p>"}, {"location": "vim/#key-bindings", "title": "Key bindings", "text": "<p>We need to learn about <code>vim.keymap.set</code>. Here is a basic usage example.</p> <pre><code>vim.keymap.set('n', '&lt;space&gt;w', '&lt;cmd&gt;write&lt;cr&gt;', {desc = 'Save'})\n</code></pre> <p>After executing this, the sequence <code>Space + w</code> will call the <code>write</code> command. Basically, we can save changes made to a file with <code>Space + w</code>.</p> <p>Let's dive into what does the  <code>vim.keymap.set</code> parameters mean.</p> <pre><code>vim.keymap.set({mode}, {lhs}, {rhs}, {opts})\n</code></pre> <ul> <li><code>{mode}</code>:  mode where the keybinding should execute. It can be a list of modes. We need to specify the mode's short name. Here are some of the most common.</li> <li><code>n</code>: Normal mode.</li> <li><code>i</code>: Insert mode.</li> <li><code>x</code>: Visual mode.</li> <li><code>s</code>: Selection mode.</li> <li><code>v</code>: Visual + Selection.</li> <li><code>t</code>: Terminal mode.</li> <li><code>o</code>: Operator-pending.</li> <li> <p><code>''</code>: Yes, an empty string. Is the equivalent of <code>n + v + o</code>.</p> </li> <li> <p><code>{lhs}</code>: is the key we want to bind.</p> </li> <li><code>{rhs}</code> is the action we want to execute. It can be a string with a command or an expression. You can also provide a lua function.</li> <li> <p><code>{opts}</code> this must be a lua table. If you don't know what is a \"lua table\" just think is a way of storing several values in one place. Anyway, it can have these properties.</p> </li> <li> <p><code>desc</code>: A string that describes what the keybinding does. You can write anything you want.</p> </li> <li><code>remap</code>: A boolean that determines if our keybinding can be recursive. The default value is <code>false</code>. Recursive keybindings can cause some conflicts if used incorrectly. Don't enable it unless you know what you're doing.</li> <li><code>buffer</code>: It can be a boolean or a number. If we assign the boolean <code>true</code> it means the keybinding will only be effective in the current file. If we assign a number, it needs to be the \"id\" of an open buffer.</li> <li><code>silent</code>: A boolean. Determines whether or not the keybindings can show a message. The default value is <code>false</code>.</li> <li><code>expr</code>: A boolean. If enabled it gives the chance to use vimscript or lua to calculate the value of <code>{rhs}</code>. The default value is <code>false</code>.</li> </ul>"}, {"location": "vim/#the-leader-key", "title": "The leader key", "text": "<p>When creating keybindings we can use the special sequence <code>&lt;leader&gt;</code> in the <code>{lhs}</code> parameter, it'll take the value of the global variable mapleader.</p> <p>So mapleader is a global variable in vimscript that can be string. For example.</p> <pre><code>vim.g.mapleader = ' '\n</code></pre> <p>After defining it we can use it as a prefix in our keybindings.</p> <pre><code>vim.keymap.set('n', '&lt;leader&gt;w', '&lt;cmd&gt;write&lt;cr&gt;')\n</code></pre> <p>This will make <code>&lt;space key&gt;</code> + <code>w</code> save the current file.</p> <p>There are different opinions on what key to use as the <code>&lt;leader&gt;</code> key. The <code>&lt;space&gt;</code> is the most comfortable as it's always close to your thumbs, and it works well with both hands. Nevertheless, you can only use it in normal mode, because in insert <code>&lt;space&gt;&lt;whatever&gt;</code> will be triggered as you write. An alternative is to use <code>;</code> which is also comfortable (if you use the english key distribution) and you can use it in insert mode. </p> <p>If you want to define more than one leader key you can either:</p> <ul> <li>Change the <code>mapleader</code> many times in your file: As the value of <code>mapleader</code> is used at the moment the mapping is defined, you can indeed change that while plugins are loading. For that, you have to explicitly <code>:runtime</code> the plugins in your <code>~/.vimrc</code> (and count on the canonical include guard to prevent redefinition later):</li> </ul> <p><pre><code>let mapleader = ','\nruntime! plugin/NERD_commenter.vim\nruntime! ...\nlet mapleader = '\\'\nrunime! plugin/mark.vim\n...\n</code></pre> * Use the keys directly instead of using <code>&lt;leader&gt;</code> </p> <pre><code>\" editing mappings\nnnoremap ,a &lt;something&gt;\nnnoremap ,k &lt;something else&gt;\nnnoremap ,d &lt;and something else&gt;\n\n\" window management mappings\nnnoremap gw &lt;something&gt;\nnnoremap gb &lt;something else&gt;\n</code></pre> <p>Defining <code>mapleader</code> and/or using <code>&lt;leader&gt;</code> may be useful if you change your mind often on what key to use a leader but it won't be of any use if your mappings are stable.</p>"}, {"location": "vim/#spelling", "title": "Spelling", "text": "<pre><code>set.spell = true\nset.spelllang = 'en_us'\nset.spellfile = '/home/your_user/.config/nvim/spell/my_dictionary.add'\n</code></pre>"}, {"location": "vim/#testing", "title": "Testing", "text": "<p>The <code>vim-test</code> alternatives for neovim are:</p> <ul> <li><code>neotest</code></li> <li><code>nvim-test</code></li> </ul> <p>The first one is the most popular so it's the first to try.</p>"}, {"location": "vim/#neotest", "title": "neotest", "text": ""}, {"location": "vim/#installation_1", "title": "Installation", "text": "<p>Add to your <code>packer</code> configuration:</p> <pre><code>use {\n  \"nvim-neotest/neotest\",\n  requires = {\n    \"nvim-lua/plenary.nvim\",\n    \"nvim-treesitter/nvim-treesitter\",\n    \"antoinemadec/FixCursorHold.nvim\"\n  }\n}\n</code></pre> <p>To get started you will also need to install an adapter for your test runner. For example for python add also:</p> <pre><code>use  \"nvim-neotest/neotest-python\"\n</code></pre> <p>Then configure the plugin with:</p> <pre><code>require(\"neotest\").setup({ -- https://github.com/nvim-neotest/neotest\n  adapters = {\n    require(\"neotest-python\")({ -- https://github.com/nvim-neotest/neotest-python\n      dap = { justMyCode = false },\n    }),\n  }\n})\n</code></pre> <p>It also needs a font that supports icons. If you don't see them install one of these.</p>"}, {"location": "vim/#plugin-managers", "title": "Plugin managers", "text": "<p>Neovim has builtin support for installing plugins. You can manually download the plugins in any directory shown in <code>:set packpath?</code>, for example <code>~/.local/share/nvim/site</code>. In one of those directories we have to create a directory called <code>pack</code> and inside <code>pack</code> we must create a \"package\". A package is a directory that contains several plugins. It must have this structure.</p> <pre><code>package-directory\n\u251c\u2500\u2500 opt\n\u2502   \u251c\u2500\u2500 [plugin 1]\n\u2502   \u2514\u2500\u2500 [plugin 2]\n\u2514\u2500\u2500 start\n    \u251c\u2500\u2500 [plugin 3]\n    \u2514\u2500\u2500 [plugin 4]\n</code></pre> <p>In this example we are creating a directory with two other directory inside: opt and start. Plugins in opt will only be loaded if we execute the command packadd. The plugins in start will be loaded automatically during the startup process.</p> <p>So to install a plugin like <code>lualine</code> and have it load automatically, we should place it for example here <code>~/.local/share/nvim/site/pack/github/start/lualine.nvim</code></p> <p>As I'm using <code>chezmoi</code> to handle the plugins of <code>zsh</code> and other stuff I tried to work with that. It was a little cumbersome to add the plugins but it did the job until I had to install <code>telescope</code> which needs to run a command after each install, and that was not easy with <code>chezmoi</code>. Then I analyzed the  most popular plugin managers in the Neovim ecosystem right now:</p> <ul> <li><code>packer</code></li> <li><code>paq</code></li> </ul> <p>If you prefer minimalism take a look at <code>paq</code>. If you want something full of features use <code>packer</code>. I went with <code>packer</code>.</p>"}, {"location": "vim/#packer", "title": "Packer", "text": ""}, {"location": "vim/#installation_2", "title": "Installation", "text": "<p>Create the <code>~/.config/nvim/lua/plugins.lua</code> file with the contents:</p> <pre><code>vim.cmd [[packadd packer.nvim]]\n\nreturn require('packer').startup(function(use)\n  -- Packer can manage itself\n  use 'wbthomason/packer.nvim'\n\n  -- Example of another plugin. Nice buffer closing \n  use 'moll/vim-bbye'\n\nend)\n</code></pre> <p>And load the file in your <code>~/.config/nvim/init.lua</code>:</p> <pre><code>-- -------------------\n-- --    Plugins    --\n-- -------------------\nrequire('plugins')\n</code></pre> <p>You can now run the <code>packer</code> commands.</p>"}, {"location": "vim/#usage", "title": "Usage", "text": "<p>Whenever you make changes to your plugin configuration you need to:</p> <ul> <li>Regenerate the compiled loader file:</li> </ul> <pre><code>:PackerCompile\n</code></pre> <ul> <li>Remove any disabled or unused plugins</li> </ul> <pre><code>:PackerClean\n</code></pre> <ul> <li>Clean, then install missing plugins</li> </ul> <pre><code>:PackerInstall\n</code></pre> <p>To update the packages to the latest version you can run:</p> <pre><code>:PackerUpdate\n</code></pre> <p>To show the list of installed plugins run:</p> <pre><code>:PackerStatus\n</code></pre>"}, {"location": "vim/#buffer-and-file-management", "title": "Buffer and file management", "text": "<p>In the past I used ctrlp as a remaining of the migration from vim to nvim. Today I've seen that there are <code>nvim</code> native plugins to do the same. I'm going to start with <code>Telescope</code>, a popular plugin (8.4k stars)</p>"}, {"location": "vim/#telescope", "title": "Telescope", "text": ""}, {"location": "vim/#install", "title": "Install", "text": "<p>It is suggested to either use the latest release tag or their release branch (which will get consistent updates) 0.1.x. If you're  using <code>packer</code> you can add this to your <code>plugins.lua</code>:</p> <pre><code>use {\n  'nvim-telescope/telescope.nvim', tag = '0.1.x',\n  requires = { {'nvim-lua/plenary.nvim'} }\n}\n</code></pre> <p>You may need to have installed <code>treesitter</code> look for those instructions to install it.</p> <p><code>telescope</code> uses <code>ripgrep</code> to do <code>live-grep</code>. I've tried using <code>ag</code> instead with this config, but it didn't work.</p> <pre><code>require('telescope').setup{\n  defaults = {\n     vimgrep_arguments = {\n        \"ag\",\n        \"--nocolor\",\n        \"--noheading\",\n        \"--numbers\",\n        \"--column\",\n        \"--smart-case\",\n        \"--silent\",\n        \"--vimgrep\",\n    }\n  }\n}\n</code></pre> <p>It's a good idea also to have <code>fzf</code> fuzzy finder, to do that we need to install the <code>telescope-fzf-native</code> plugin. To do that add to your <code>plugins.lua</code> config file:</p> <pre><code>  use {\n    'nvim-telescope/telescope-fzf-native.nvim', \n    run = 'make' \n  }\n</code></pre> <p>Run <code>:PackerInstall</code> and then configure it in your <code>init.lua</code>:</p> <pre><code>-- You dont need to set any of these options. These are the default ones. Only\n-- the loading is important\nrequire('telescope').setup {\n  extensions = {\n    fzf = {\n      fuzzy = true,                    -- false will only do exact matching\n      override_generic_sorter = true,  -- override the generic sorter\n      override_file_sorter = true,     -- override the file sorter\n      case_mode = \"smart_case\",        -- or \"ignore_case\" or \"respect_case\"\n                                       -- the default case_mode is \"smart_case\"\n    }\n  }\n}\n-- To get fzf loaded and working with telescope, you need to call\n-- load_extension, somewhere after setup function:\nrequire('telescope').load_extension('fzf')\n</code></pre> <p>It also needs <code>fd</code> for further features. You should be using it too for your terminal.</p> <p>To check that everything is fine run <code>:checkhealth telescope</code>.</p>"}, {"location": "vim/#usage_1", "title": "Usage", "text": "<p><code>telescope</code> has different ways to find files:</p> <ul> <li><code>find_files</code>: Uses <code>fd</code> to find a string in the file names.</li> <li><code>live_grep</code>: Uses <code>rg</code> to find a string in the file's content.</li> <li><code>buffers</code>: Searches strings in the buffer names.</li> </ul> <p>You can configure each of these commands with the next bindings:</p> <pre><code>local builtin = require('telescope.builtin')\nlocal key = vim.keymap\nkey.set('n', '&lt;leader&gt;f', builtin.find_files, {})\nkey.set('n', '&lt;leader&gt;a', builtin.live_grep, {})\nkey.set('n', '&lt;leader&gt;b', builtin.buffers, {})\n</code></pre> <p>By default it searches on all files. You can ignore some of them with:</p> <pre><code>require('telescope').setup{\n  defaults = {\n    -- Default configuration for telescope goes here:\n    -- config_key = value,\n    file_ignore_patterns = {\n      \"%.svg\",\n      \"%.bmp\",\n      \"%.jpg\",\n      \"%.jpeg\",\n      \"%.gif\",\n      \"%.png\",\n    },\n  }\n}\n</code></pre> <p>You can also replace some other default <code>vim</code> commands like history browsing, spell checker suggestions or searching in the current buffer with:</p> <pre><code>key.set('n', '&lt;C-r&gt;', builtin.command_history, {})\nkey.set('n', 'z=', builtin.spell_suggest, {})\nkey.set('n', '/', builtin.current_buffer_fuzzy_find, {})\n</code></pre>"}, {"location": "vim/#heading-navigation", "title": "Heading navigation", "text": "<p>It's a <code>telescope</code> plugin to navigate through your markdown headers</p>"}, {"location": "vim/#installation_3", "title": "Installation", "text": "<p>Install with your favorite package manager:</p> <pre><code>use('nvim-telescope/telescope.nvim')\nuse('crispgm/telescope-heading.nvim')\n</code></pre> <p><code>telescope-heading</code> supports Tree-sitter for parsing documents and finding headings.</p> <pre><code>-- make sure you have already installed treesitter modules\nrequire('nvim-treesitter.configs').setup({\n    ensure_installed = {\n        -- ..\n        'markdown',\n        'rst',\n        -- ..\n    },\n})\n\n-- enable treesitter parsing\nlocal telescope = require('telescope')\ntelescope.setup({\n    -- ...\n    extensions = {\n        heading = {\n            treesitter = true,\n        },\n    },\n})\n\n-- `load_extension` must be after `telescope.setup`\ntelescope.load_extension('heading')\n\n-- Set the key binding\n\nlocal key = vim.keymap\nkey.set('n', '&lt;leader&gt;h', ':Telescope heading&lt;cr&gt;')\n</code></pre>"}, {"location": "vim/#treesitter", "title": "Treesitter", "text": "<p><code>treesitter</code> it's a neovim parser generator tool and an incremental parsing library. It can build a concrete syntax tree for a source file and efficiently update the syntax tree as the source file is edited. With it you can do nice things like:</p> <ul> <li>Highlight code</li> <li>Incremental selection of the code</li> <li>Indentation</li> <li>Folding</li> </ul>"}, {"location": "vim/#installation_4", "title": "Installation", "text": "<p>Add these lines to your <code>plugins.lua</code> file:</p> <pre><code>  use {\n    'nvim-treesitter/nvim-treesitter',\n    run = function()\n        local ts_update = require('nvim-treesitter.install').update({ with_sync = true })\n        ts_update()\n    end,\n  }\n</code></pre> <p>Install it with <code>:PackerInstall</code>.</p> <p>The base configuration is:</p> <pre><code>require('nvim-treesitter.configs').setup({\n  ensure_installed = {\n    'bash',\n    'beancount',\n    'dockerfile',\n    'make',\n    'terraform',\n    'toml',\n    'vue',\n    'lua',\n    'markdown',\n    'python',\n    'css',\n    'html',\n    'javascript',\n    'json',\n    'yaml',\n  },\n})\n</code></pre> <p>Select the languages you want to install from the available ones, close and reopen the vim window to install them.</p>"}, {"location": "vim/#usage_2", "title": "Usage", "text": "<p>By default it doesn't enable any feature, you need to enable them yourself.</p>"}, {"location": "vim/#highlight-code", "title": "Highlight code", "text": "<p>Enable the feature with:</p> <pre><code>require('nvim-treesitter.configs').setup({\n  highlight = {\n    enable = true,\n  },\n})\n</code></pre> <p>Improves the default syntax for the supported languages.</p>"}, {"location": "vim/#incremental-selection", "title": "Incremental selection", "text": "<p>It lets you select pieces of your code by the function they serve. For example imagine that we have the next snippet:</p> <pre><code>def function():\n  if bool is True:\n    print('this is a Test')\n</code></pre> <p>And your cursor is in the <code>T</code> of the <code>print</code> statement. If you were to press the <code>Enter</code> key it will enter in visual mode selecting the <code>Test</code> word, if you were to press <code>Enter</code> key again it will increment the scope of the search, so it will select all the contents of the print statement <code>'this is a Test'</code>, if you press <code>Enter</code> again it will increase the scope. </p> <p>If you went too far, you can use the <code>Return</code> key to reduce the scope. For these keybindings to work you need to set:</p> <pre><code>require('nvim-treesitter.configs').setup({\n  incremental_selection = {\n    enable = true,\n    keymaps = {\n      init_selection = \"&lt;cr&gt;\", -- set to `false` to disable one of the mappings\n      node_incremental = \"&lt;cr&gt;\",\n      node_decremental = \"&lt;bs&gt;\",\n      -- scope_incremental = \"grc\",\n    },\n  },\n})\n</code></pre>"}, {"location": "vim/#indentation", "title": "Indentation", "text": "<pre><code>require'nvim-treesitter.configs'.setup {\n  indent = {\n    enable = true\n  }\n}\n</code></pre>"}, {"location": "vim/#folding", "title": "Folding", "text": "<p>Tree-sitter based folding</p> <pre><code>set.foldmethod = 'expr'\nset.foldexpr = 'nvim_treesitter#foldexpr()'\nset.foldenable = true                   \nset.foldminlines = 3\n</code></pre> <p>It won't fold code sections that have have less than 3 lines.</p> <p>If you add files through <code>telescope</code> you may see an <code>E490: No fold found</code> error when trying to access the folds, there's an open issue that tracks this, the workaround for me was to add this snippet in the <code>telescope</code> configuration::</p> <pre><code>require('telescope').setup {\n    defaults = {\n        mappings = {\n            i = {\n                [\"&lt;CR&gt;\"] = function()\n                    vim.cmd [[:stopinsert]]\n                    vim.cmd [[call feedkeys(\"\\&lt;CR&gt;\")]]\n                end\n            }\n        }\n    }\n}\n</code></pre> <p>To save the foldings when you save a file use the next snippet. Sorry but I don't know how to translate that into lua.</p> <pre><code>vim.cmd[[\n  augroup remember_folds\n    autocmd!\n    autocmd BufWinLeave * silent! mkview\n    autocmd BufWinEnter * silent! loadview\n  augroup END\n]]\n</code></pre>"}, {"location": "vim/#git", "title": "Git", "text": "<p>There are many plugins to work with git in neovim the most interesting ones are:</p> <ul> <li>vim-fugitive</li> <li>neogit</li> <li>lazygit</li> <li>vgit</li> </ul> <p>I've been using <code>vim-fugitive</code> for some years now and it works very well but is built for <code>vim</code>. Now that I'm refurbishing all the neovim configuration I want to try some neovim native plugins.</p> <p><code>neogit</code> looks interesting as it's a magit clone for <code>neovim</code>. <code>lazygit</code> is the most popular one as it's a command line tool not specific to <code>neovim</code>. As such you'd need to launch a terminal inside neovim or use a plugin like lazygit.nvim. I'm not able to understand how to use <code>vgit</code> by looking at their readme, there's not more documentation and there is no videos showing it's usage. It's also the least popular although it looks active.</p> <p>At a first look <code>lazygit</code> is too much and <code>neogit</code> a little more verbose than <code>vim-fugitive</code> but it looks closer to my current workflow. I'm going to try <code>neogit</code> then.</p>"}, {"location": "vim/#neogit", "title": "Neogit", "text": ""}, {"location": "vim/#installation_5", "title": "Installation", "text": "<pre><code>use { 'TimUntersberger/neogit', requires = 'nvim-lua/plenary.nvim' }\n</code></pre> <p>Now you have to add the following lines to your <code>init.lua</code></p> <pre><code>local neogit = require('neogit')\n\nneogit.setup()\n</code></pre> <p>That uses the default configuration, but there are many options that can be set. For example to disable the commit confirmation use:</p> <pre><code>neogit.setup({\n  disable_commit_confirmation = true\n})\n\n### Improve the commit message window\n\n\n\n\n\n\n[create custom keymaps with lua](https://blog.devgenius.io/create-custom-keymaps-in-neovim-with-lua-d1167de0f2c2)\n[create specific bindings for a file type](https://stackoverflow.com/questions/72984648/neovim-lua-how-to-use-different-mappings-depending-on-file-type)\nhttps://neovim.discourse.group/t/how-to-create-an-auto-command-for-a-specific-filetype-in-neovim-0-7/2404\n[create autocmd in neovim](https://alpha2phi.medium.com/neovim-for-beginners-lua-autocmd-and-keymap-functions-3bdfe0bebe42)\n[autocmd events](https://neovim.io/doc/user/autocmd.html#autocmd-events)\n\n\n\n\n\n\n\n\n# [Abbreviations](https://davidxmoody.com/2014/better-vim-abbreviations/)\n\nIn order to reduce the amount of typing and fix common typos, I use the Vim\nabbreviations support. Those are split into two files,\n`~/.vim/abbreviations.vim` for abbreviations that can be used in every type of\nformat and `~/.vim/markdown-abbreviations.vim` for the ones that can interfere\nwith programming typing.\n\nThose files are sourced in my `.vimrc`\n\n```vim\n\" Abbreviations\nsource ~/.vim/abbreviations.vim\nautocmd BufNewFile,BufReadPost *.md source ~/.vim/markdown-abbreviations.vim\n</code></pre> <p>To avoid getting worse in grammar, I don't add abbreviations for words that I doubt their spelling or that I usually mistake. Instead I use it for common typos such as <code>teh</code>.</p> <p>The process has it's inconveniences:</p> <ul> <li>You need different abbreviations for the capitalized versions, so you'd need     two abbreviations for <code>iab cant can't</code> and <code>iab Cant Can't</code></li> <li>It's not user friendly to add new words, as you need to open a file.</li> </ul> <p>The Vim Abolish plugin solves that. For example:</p> <pre><code>\" Typing the following:\nAbolish seperate separate\n\n\" Is equivalent to:\niabbrev seperate separate\niabbrev Seperate Separate\niabbrev SEPERATE SEPARATE\n</code></pre> <p>Or create more complex rules, were each <code>{}</code> gets captured and expanded with different caps</p> <pre><code>:Abolish {despa,sepe}rat{e,es,ed,ing,ely,ion,ions,or}  {despe,sepa}rat{}\n</code></pre> <p>With a bang (<code>:Abolish!</code>) the abbreviation is also appended to the file in <code>g:abolish_save_file</code>. By default <code>after/plugin/abolish.vim</code> which is loaded by default.</p> <p>Typing <code>:Abolish! im I'm</code> will append the following to the end of this file:</p> <pre><code>Abolish im I'm\n</code></pre> <p>To make it quicker I've added a mapping for <code>&lt;leader&gt;s</code>.</p> <pre><code>nnoremap &lt;leader&gt;s :Abolish!&lt;Space&gt;\n</code></pre> <p>Check the README for more details.</p>"}, {"location": "vim/#troubleshooting", "title": "Troubleshooting", "text": "<p>Abbreviations with dashes or if you only want the first letter in capital need to be specified with the first letter in capital letters as stated in this issue.</p> <pre><code>Abolish knobas knowledge-based\nAbolish w what\n</code></pre> <p>Will yield <code>KnowledgeBased</code> if invoked with <code>Knobas</code>, and <code>WHAT</code> if invoked with <code>W</code>. Therefore the following definitions are preferred:</p> <pre><code>Abolish Knobas Knowledge-based\nAbolish W What\n</code></pre>"}, {"location": "vim/#auto-complete-prose-text", "title": "Auto complete prose text", "text": "<p>Tools like YouCompleteMe allow you to auto complete variables and functions. If you want the same functionality for prose, you need to enable it for markdown and text, as it's disabled by default.</p> <pre><code>let g:ycm_filetype_blacklist = {\n      \\ 'tagbar' : 1,\n      \\ 'qf' : 1,\n      \\ 'notes' : 1,\n      \\ 'unite' : 1,\n      \\ 'vimwiki' : 1,\n      \\ 'pandoc' : 1,\n      \\ 'infolog' : 1\n  \\}\n</code></pre> <p>When writing prose you don't need all possible suggestions, as navigating the options is slower than keep on typing. So I'm limiting the results just to one, to avoid unnecessary distractions.</p> <pre><code>\" Limit the results for markdown files to 1\nau FileType markdown let g:ycm_max_num_candidates = 1\nau FileType markdown let g:ycm_max_num_identifier_candidates = 1\n</code></pre>"}, {"location": "vim/#find-synonyms", "title": "Find synonyms", "text": "<p>Sometimes the prose linters tell you that a word is wordy or too complex, or you may be repeating a word too much. The thesaurus query plugin allows you to search synonyms of the word under the cursor. Assuming you use Vundle, add the following lines to your config.</p> <p>File: ~/.vimrc</p> <pre><code>Plugin 'ron89/thesaurus_query.vim'\n\n\" Thesaurus\nlet g:tq_enabled_backends=[\"mthesaur_txt\"]\nlet g:tq_mthesaur_file=\"~/.vim/thesaurus\"\nnnoremap &lt;leader&gt;r :ThesaurusQueryReplaceCurrentWord&lt;CR&gt;\ninoremap &lt;leader&gt;r &lt;esc&gt;:ThesaurusQueryReplaceCurrentWord&lt;CR&gt;\n</code></pre> <p>Run <code>:PluginInstall</code> and download the thesaurus text from gutenberg.org</p> <p>Next time you find a word like <code>therefore</code> you can press <code>:ThesaurusQueryReplaceCurrentWord</code> and you'll get a window with the following:</p> <pre><code>In line: ... therefore ...\nCandidates for therefore, found by backend: mthesaur_txt\nSynonyms: (0)accordingly (1)according to circumstances (2)and so (3)appropriately (4)as a consequence\n          (5)as a result (6)as it is (7)as matters stand (8)at that rate (9)because of that (10)because of this\n          (11)compliantly (12)conformably (13)consequently (14)equally (15)ergo (16)finally (17)for that\n          (18)for that cause (19)for that reason (20)for this cause (21)for this reason (22)for which reason\n          (23)hence (24)hereat (25)in that case (26)in that event (27)inconsequence (28)inevitably\n          (29)it follows that (30)naturally (31)naturellement (32)necessarily (33)of course (34)of necessity\n          (35)on that account (36)on that ground (37)on this account (38)propter hoc (39)suitably\n          (40)that being so (41)then (42)thence (43)thereat (44)therefor (45)thus (46)thusly (47)thuswise\n          (48)under the circumstances (49)whence (50)wherefore (51)wherefrom\nType number and &lt;Enter&gt; (empty cancels; 'n': use next backend; 'p' use previous backend):\n</code></pre> <p>If for example you type <code>45</code> and hit enter, it will change it for <code>thus</code>.</p>"}, {"location": "vim/#keep-foldings", "title": "Keep foldings", "text": "<p>When running fixers usually the foldings go to hell. To keep the foldings add the following snippet to your vimrc file</p> <pre><code>augroup remember_folds\n  autocmd!\n  autocmd BufLeave * mkview\n  autocmd BufEnter * silent! loadview\naugroup END\n</code></pre>"}, {"location": "vim/#python-folding-done-right", "title": "Python folding done right", "text": "<p>Folding Python in Vim is not easy, the python-mode plugin doesn't do it for me by default and after fighting with it for 2 hours...</p> <p>SimpylFold does the trick just fine.</p>"}, {"location": "vim/#delete-a-file-inside-vim", "title": "Delete a file inside vim", "text": "<pre><code>:call delete(expand('%')) | bdelete!\n</code></pre> <p>You can make a function so it's easier to remember</p> <pre><code>function! Rm()\n  call delete(expand('%')) | bdelete!\nendfunction\n</code></pre> <p>Now you need to run <code>:call Rm()</code>.</p>"}, {"location": "vim/#task-management", "title": "Task management", "text": "<p>Check the <code>nvim-orgmode</code> file.</p>"}, {"location": "vim/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Getting started guide</li> <li>Docs</li> </ul>"}, {"location": "vim/#troubleshooting_1", "title": "Troubleshooting", "text": ""}, {"location": "vim/#deal-with-big-files", "title": "Deal with big files", "text": "<p>Sometimes <code>neovim</code> freezes when opening big files, one way to deal with it is to disable some functionality when loading them</p> <pre><code>local aug = vim.api.nvim_create_augroup(\"buf_large\", { clear = true })\n\nvim.api.nvim_create_autocmd({ \"BufReadPre\" }, {\n  callback = function()\n    local ok, stats = pcall(vim.loop.fs_stat, vim.api.nvim_buf_get_name(vim.api.nvim_get_current_buf()))\n    if ok and stats and (stats.size &gt; 100000) then\n      vim.b.large_buf = true\n      -- vim.cmd(\"syntax off\") I don't yet need to turn the syntax off\n      vim.opt_local.foldmethod = \"manual\"\n      vim.opt_local.spell = false\n      set.foldexpr = 'nvim_treesitter#foldexpr()' -- Disable fold expression with treesitter, it freezes the loading of files\n    else\n      vim.b.large_buf = false\n    end\n  end,\n  group = aug,\n  pattern = \"*\",\n})\n</code></pre> <p>When it opens a file it will decide if it's a big file. If it is, it will unset the <code>foldexpr</code> which made it break for me.</p> <p>Telescope's preview also froze the terminal. To deal with it I had to disable treesitter for the preview</p> <pre><code>require('telescope').setup{\n  defaults = {\n    preview = {\n      enable = true,\n      treesitter = false,\n    },\n  ...\n</code></pre>"}, {"location": "vim/#tips", "title": "Tips", "text": ""}, {"location": "vim/#run-lua-snippets", "title": "Run lua snippets", "text": "<p>Run lua snippet within neovim with <code>:lua &lt;your snippet&gt;</code>. Useful to test the commands before binding it to keys.</p>"}, {"location": "vim/#bind-a-lua-function-to-a-key-binding", "title": "Bind a lua function to a key binding", "text": "<pre><code>key.set({'n'}, 't', \":lua require('neotest').run.run()&lt;cr&gt;\", {desc = 'Run the closest test'})\n</code></pre>"}, {"location": "vim/#use-relativenumber", "title": "Use relativenumber", "text": "<p>If you enable the <code>relativenumber</code> configuration you'll see how to move around with <code>10j</code> or <code>10k</code>.</p>"}, {"location": "vim/#resources", "title": "Resources", "text": "<ul> <li>Nvim news</li> <li>spacevim</li> <li>awesome-neovim</li> <li>awesome-vim: a list of vim       resources maintained by the community</li> </ul>"}, {"location": "vim/#vimrc-tweaking", "title": "Vimrc tweaking", "text": "<ul> <li>List of nvim configs</li> <li>jessfraz vimrc</li> </ul>"}, {"location": "vim/#learning", "title": "Learning", "text": "<ul> <li>vim golf</li> <li>Vim game tutorial: very funny and challenging,       buuuuut at lvl 3 you have to pay :(.</li> <li>PacVim:       Pacman like vim game to learn.</li> <li>Vimgenius: Increase your speed and improve your       muscle memory with Vim Genius, a timed flashcard-style game designed to       make you faster in Vim. It\u2019s free and you don\u2019t need to sign up. What are       you waiting for?</li> <li>Openvim: Interactive tutorial for vim.</li> </ul>"}, {"location": "wireshark/", "title": "Wireshark", "text": "<p>Wireshark is the world\u2019s foremost and widely-used network protocol analyzer. It lets you see what\u2019s happening on your network at a microscopic level and is the de facto (and often de jure) standard across many commercial and non-profit enterprises, government agencies, and educational institutions.</p>"}, {"location": "wireshark/#installation", "title": "Installation", "text": "<pre><code>apt-get install wireshark\n</code></pre> <p>If the version delivered by your distribution is not high enough, use Jezz's Docker</p> <pre><code>docker run -d \\\n-v /etc/localtime:/etc/localtime:ro \\\n-v /tmp/.X11-unix:/tmp/.X11-unix \\\n-e DISPLAY=unix$DISPLAY \\\n-v /tmp/wireshark:/data \\\njess/wireshark\n</code></pre>"}, {"location": "wireshark/#usage", "title": "Usage", "text": ""}, {"location": "wireshark/#filter", "title": "Filter", "text": "<p>You can filter by traffic type with <code>tcp and tcp.port == 80</code>, <code>http or ftp</code> or <code>not ftp</code>.</p> <p>It's also possible to nest many operators with <code>(http or ftp) and ip.addr == 192.168.1.14</code></p> <p>The most common filters are:</p> Item Description ip.addr IP address (check both source and destination) tcp.port TCP Layer 4 port (check both source and destination) udp.port UDP Layer 4 port (check both source and destination) ip.src IP source address ip.dst IP destination address tcp.srcport TCP source port tcp.dstport TCP destination port udp.srcport UDP source port udp.dstport UDP destination port icmp.type ICMP numeric type ip.tos.precedence IP precedence eth.addr MAC address ip.ttl IP Time to Live (TTL)"}, {"location": "wireshark/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "yamlfix/", "title": "Yamlfix", "text": "<p>Yamlfix is a simple opinionated yaml formatter that keeps your comments.</p>"}, {"location": "yamlfix/#install", "title": "Install", "text": "<pre><code>pip install yamlfix\n</code></pre>"}, {"location": "yamlfix/#usage", "title": "Usage", "text": "<p>Imagine we've got the following source code:</p> <pre><code>book_library:\n- title: Why we sleep\nauthor: Matthew Walker\n- title: Harry Potter and the Methods of Rationality\nauthor: Eliezer Yudkowsky\n</code></pre> <p>It has the following errors:</p> <ul> <li>There is no <code>---</code> at the top.</li> <li>The indentation is all wrong.</li> </ul> <p>After running <code>yamlfix</code> the resulting source code will be:</p> <pre><code>---\nbook_library:\n- title: Why we sleep\nauthor: Matthew Walker\n- title: Harry Potter and the Methods of Rationality\nauthor: Eliezer Yudkowsky\n</code></pre> <p><code>yamlfix</code> can be used both as command line tool and as a library.</p> <ul> <li> <p>As a command line tool:</p> <pre><code>$: yamlfix file.yaml\n</code></pre> </li> <li> <p>As a library:</p> <pre><code>from yamlfix import fix_files\n\nfix_files(['file.py'])\n</code></pre> </li> </ul>"}, {"location": "yamlfix/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "zfs_storage_planning/", "title": "OpenZFS storage planning", "text": "<p>When you build a ZFS storage system you need to invest some time doing the storage planning.</p> <p>There are many variables that affect the number and type of disks, you first need to have an idea of what kind of data you want to store and what use are you going to give to that data.</p>"}, {"location": "zfs_storage_planning/#robustness", "title": "Robustness", "text": "<p>ZFS is designed to survive disk failures, so it stores each block of data redundantly. This feature complicates capacity planning because your total usable storage is not just the sum of each disk\u2019s capacity.</p> <p>ZFS creates filesystems out of \u201cpools\u201d of disks. The more disks in the pool, the more efficiently ZFS can use their storage capacity. For example, if you give ZFS two 10 TB drives, you can only use half of your total disk capacity. If you instead use five 4 TB drives, ZFS gives you 14 TB of usable storage. Even though your total disk space is the same in either scenario, the five smaller drives give you 40% more usable space.</p> <p>When you\u2019re building a NAS server, you need to decide whether to use a smaller quantity of large disks or a larger quantity of small disks. Smaller drives are usually cheaper in terms of $/TB, but they\u2019re more expensive to operate. Two 4 TB drives require twice the electricity of a single 8 TB drive.</p> <p>Also keep in mind that so far ZFS doesn't let you add a new drive to an existing vdev, but that feature is under active development. If you want to be safe, plan your vdev definition so that they don't need to change the disk numbers.</p>"}, {"location": "zfs_storage_planning/#preventing-concurrent-disk-failures", "title": "Preventing concurrent disk failures", "text": "<p>Naively, the probability of two disks failing at once seems vanishingly small. Based on Backblaze\u2019s stats, high-quality disk drives fail at 0.5-4% per year. A 4% risk per year is a 0.08% chance in any given week. Two simultaneous failures would happen once every 30,000 years, so you should be fine, right?</p> <p>The problem is that disks aren\u2019t statistically independent. If one disk fails, its neighbor has a substantially higher risk of dying. This is especially true if the disks are the same model, from the same manufacturing batch, and processed the same workloads.</p> <p>Further, rebuilding a ZFS pool puts an unusual amount of strain on all of the surviving disks. A disk that would have lasted a few more months under normal usage might die under the additional load of a pool rebuild.</p> <p>Given these risks, you can reduce the risk of concurrent disk failures by choosing two different models of disk from two different manufacturers. To reduce the chances of getting disks from the same manufacturing batch, you can buy them from different vendors.</p>"}, {"location": "zfs_storage_planning/#choosing-the-disks", "title": "Choosing the disks", "text": "<p>There are many things to take into account when choosing the different disks for your pool.</p>"}, {"location": "zfs_storage_planning/#choosing-the-disks-to-hold-data", "title": "Choosing the disks to hold data", "text": "<p>Check diskprices.com to get an idea of the cost of disks in the market. If you can, try to avoid buying to Amazon as it's the devil. Try to buy them from a local store instead, that way you interact with a human and promote a local commerce.</p> <p>Note: If you want a TL;DR you can jump to the conclusion.</p> <p>To choose your disks take into account:</p> <ul> <li>Disk speed</li> <li>Disk load</li> <li>Disk type</li> <li>Disk homogeneity</li> <li>Disk Warranty</li> <li>Disk Brands</li> </ul>"}, {"location": "zfs_storage_planning/#data-disk-speed", "title": "Data disk speed", "text": "<p>When comes to disk speed there are three kinds, the slow (5400 RPM), normal (7200 RPM) and fast (10k RPM).</p> <p>The higher the RPM, the louder the disk is, the more heat it creates and the more power it will consume. In exchange they will have higher writing and reading speeds. Slower disks expand the lifecycle of the device, but in the case of a failed disk in a RAID scenario, the rebuild time will be higher than on faster ones therefore increasing the risk on concurrent failing disks.</p> <p>Before choosing a high number of RPM make sure that it's your bottleneck, which usually is the network if you're using a 1Gbps network. In this case a 10k RPM disk won't offer better performance than a 7200 RPM, even a 7200 one won't be better than a 5400.</p> <p>The need of higher speeds can be fixed by using an SSD as a cache for reading and writing.</p>"}, {"location": "zfs_storage_planning/#data-disk-load", "title": "Data disk load", "text": "<p>Disk specifications tell you the amount of TB/year they support, it gives you an idea of the fault tolerance. Some examples</p> Disk Fault tolerance (TB/year) WD RED 8TB 180"}, {"location": "zfs_storage_planning/#data-disk-type", "title": "Data disk type", "text": "<p>It\u2019s easy to think that all hard drives are equal, save for the form factor and connection type. However, there\u2019s a difference between the work your hard drive does in your computer versus the workload of a NAS hard drive. A drive in your computer may only read and write data for a couple hours at a time, while a NAS drive may read and write data for weeks on end, or even longer.</p> <p>The environment inside of a NAS box is much different than a typical desktop or laptop computer. When you pack in a handful of hard drives close together, several things happen: there\u2019s more vibration, more heat, and a lot more action going on in general.</p> <p>To cope with this, NAS hard drives usually have better vibration tolerance and produce less heat than regular hard drives, thanks to slightly-slower spindle speeds and reduced seek noise.</p> <p>Most popular brands are Western Digital Red and Seagate IronWolf which use 5400 RPM, if you want to go on the 7200 RPM speeds you can buy the Pro version of each. I initially tried checking Backblaze\u2019s hard drive stats to avoid failure-prone disks, but they use drives on the pricier side.</p> <p>The last pitfall to avoid is shingled magnetic recording (SMR) technology. ZFS performs poorly on SMR drives, so if you\u2019re building a NAS, avoid known SMR drives. If the drive is labeled as CMR, that\u2019s conventional magnetic recording, which is fine for ZFS.</p> <p>SMR is well suited for high-capacity, low-cost use where writes are few and reads are many. It has worse sustained write performance than CMR, which can cause severe issues during resilvering or other write-intensive operations.</p> <p>There are three types of SMR:</p> <ul> <li>Drive Managed, DM-SMR: It's opaque to the OS. This means ZFS cannot \"target\"   writes, and is the worst type for ZFS use. As a rule of thumb, avoid DM-SMR   drives, unless you have a specific use case where the increased resilver time   (a week or longer) is acceptable, and you know the drive will function for ZFS   during resilver.</li> <li>Host Aware, HA-SMR: It's designed to give ZFS insight into the SMR process.   Note that ZFS code to use HA-SMR does not appear to exist. Without that code,   a HA-SMR drive behaves like a DM-SMR drive where ZFS is concerned.</li> <li>Host Managed, HM-SMR: It's not backwards compatible and requires ZFS to manage   the SMR process.</li> </ul>"}, {"location": "zfs_storage_planning/#data-disk-homogeneity", "title": "Data disk homogeneity", "text": "<p>It's recommended that all the disks in your pool (or is it by vdev?) have the same RPM and size.</p>"}, {"location": "zfs_storage_planning/#data-disk-warranty", "title": "Data disk warranty", "text": "<p>Disks are going to fail, so it's good to have a good warranty to return them.</p>"}, {"location": "zfs_storage_planning/#data-disk-brands", "title": "Data disk brands", "text": ""}, {"location": "zfs_storage_planning/#western-digital", "title": "Western Digital", "text": "<p>The Western Digital Red series of NAS drives are very similar to Seagate\u2019s offering and you should consider these if you can find them at more affordable prices. WD splits its NAS drives into three sub-categories, normal, Plus, and Pro.</p> Specs WD Red WD Red Plus WD Red Pro Technology SMR CMR CMR Bays 1-8 1-8 1-24 Capacity 2-6TB 1-14TB 2-18TB Speed 5,400 RPM 5,400 RPM (1-4TB) 7200 RPM Speed 5,400 RPM 5,640 RPM (6-8TB) 7200 RPM Speed 5,400 RPM 7,200 RPM (8-14TB) 7200 RPM Speed ? 210MB/s 235MB/s Cache 256MB 16MB (1TB) Cache 256MB 64MB (1TB) 64MB (2TB) Cache 256MB 128MB (2-8TB) 256MB (4-12TB) Cache 256MB 256MB (8-12TB) 512MB (14-18TB) Cache 256MB 512MB (14TB) Workload 180TB/yr 180TB/yr 300TB/yr MTBF 1 million 1 million 1 million Warranty 3 years 3 years 5 years Power Consumption ? ? 8.8 W Power Consumption Rest ? ? 4.6 W Price From $50 From $45 From $78"}, {"location": "zfs_storage_planning/#seagate", "title": "Seagate", "text": "<p>Seagate's \"cheap\" NAS disks are the IronWolf gama, there are two variations IronWolf and IronWolf Pro. Seagate Exos is a premium series of drives from the company. They\u2019re even more advanced than IronWolf Pro and are best suited for server environments. They sport incredible levels of performance and reliability, including a workload rate of 550TB per year.</p> Specs IronWolf IronWolf Pro Exos 7E8 8TB Exos 7E10 8TB Technology CMR CMR CMR SMR Bays 1-8 1-24 ? ? Capacity 1-12TB 2-20TB 8TB 8TB RPM 5,400 RPM (3-6TB) 7200 RPM 7200 RPM 7200 RPM RPM 5,900 RPM (1-3TB) 7200 RPM 7200 RPM 7200 RPM RPM 7,200 RPM (8-12TB) 7200 RPM 7200 RPM 7200 RPM Speed 180MB/s (1-12TB) 214-260MB/s (4-18TB) 249 MB/s 255 MB/s Cache 64MB (1-4TB) 256 MB 256 MB 256 MB Cache 256MB (3-12TB) 256 MB 256 MB 256 MB Power Consumption (8TB) 10.1 W 10.1 W 12.81 W 11.03 W Power Consumption Rest (8TB) 7.8 W 7.8 W 7.64 W 7.06 W Workload 180TB/yr 300TB/yr 550TB/yr 550TB/yr MTBF 1 million 1 million 2 millions 2 millions Warranty 3 years 5 years 5 years 5 years Price From $60 From $83 249$ 249$ <p>Exos 7E10 is SMR so it's ruled out.</p> <p>Where MTBF stands for Medium Time Between Failures in hours</p>"}, {"location": "zfs_storage_planning/#data-disk-conclusion", "title": "Data disk conclusion", "text": "<p>I'm more interested on the 5400 RPM drives, but of all the NAS disks available to purchase only the WD RED of 8TB use it, and they use the SMR technology, so they aren't a choice.</p> <p>The disk prices offered by my cheapest provider are:</p> Disk Size Price Seagate IronWolf 8TB 225$ Seagate IronWolf Pro 8TB 254$ WD Red Plus 8TB 265$ Seagate Exos 7E8 8TB 277$ WD Red Pro 8TB 278$ <p>WD Red Plus has 5,640 RPM which is different than the rest, so it's ruled out. Between the IronWolf and IronWolf Pro, they offer 180MB/s and 214MB/s respectively. The Seagate Exos 7E8 provides much better performance than the WD Red Pro so I'm afraid that WD is out of the question.</p> <p>There are three possibilities in order to have two different brands. Imagining we want 4 disks:</p> Combination Total Price IronWolf + IronWolf Pro 958$ IronWolf + Exos 7E8 1004$ (+46$ +4.5%) IronWolf Pro + Exos 7E8 1062$ (+54$ +5.4%) <p>In terms of:</p> <ul> <li>Consumption: both IronWolfs are equal, the Exos uses 2.7W more on normal use   and uses 0.2W less on rest.</li> <li>Warranty: IronWolf has only 3 years, the others 5.</li> <li>Speed: Ironwolf has 210MB/s, much less than the Pro (255MB/s) and Exos   (249MB/s), which are more similar.</li> <li>Sostenibility: The Exos disks are much more robust (more workload, MTBF and   Warranty).</li> </ul> <p>I'd say that for 104$ it makes sense to go with the IronWolf Pro + Exos 7E8 combination.</p>"}, {"location": "zfs_storage_planning/#choosing-the-disks-for-the-cache", "title": "Choosing the disks for the cache", "text": "<p>Using a ZLOG greatly improves the writing speed, equally using an SSD disk for the L2ARC cache improves the read speeds and improves the health of the rotational disks.</p> <p>The best M.2 NVMe SSD for NAS caching are the ones that have enough capacity to actually make a difference to overall system performance. It also requires a good endurance rating for better reliability and longer lifespan, and you should look for a drive with a specific NAND technology if possible.</p> <p>Note: If you want a TL;DR you can jump to the conclusion.</p> <p>To choose your disks take into account:</p> <ul> <li>Cache disk NAND technology</li> <li>DWPD</li> </ul>"}, {"location": "zfs_storage_planning/#cache-disk-nand-technology", "title": "Cache disk NAND technology", "text": "<p>Not all flash-based storage drives are the same. NAND flash cells are usually categorised based on the number of bits that can be stored per cell. Watch out for the following terms when shopping around for an SSD:</p> <ul> <li>Single-Level Cell (SLC): one bit per cell.</li> <li>Multi-Level Cell (MLC): two bits per cell.</li> <li>Triple-Level Cell (TLC): three bits per cell.</li> <li>Quad-Level Cell (QLC): four bits per cell.</li> </ul> <p>When looking for the best M.2 NVMe SSD for NAS data caching, it\u2019s important to bear the NAND technology in mind.</p> <p>SLC is the best technology for SSDs that will be used for NAS caching. This does mean you\u2019re paying out more per GB and won\u2019t be able to select high-capacity drives, but reliability and the protection of stored data is the most important factor here.</p> <p>Another benefit of SLC is the lower impact of write amplification, which can quickly creep up and chomp through a drive\u2019s DWPD endurance rating. It\u2019s important to configure an SSD for caching correctly too regardless of which technology you pick.</p> <p>Doing so will lessen the likelihood of losing data through a drive hanging and causing the system to crash. Anything stored on the cache drive that has yet to be written to the main drive array would be lost. This is mostly a reported issue for NVMe drives, as opposed to SATA.</p>"}, {"location": "zfs_storage_planning/#dwpd", "title": "DWPD", "text": "<p>DWPD stands for drive writes per day. This is often used as a measurement of a drive\u2019s endurance. The higher this number, the more writes the drive can perform on a daily basis, as is rated by the manufacturer. For caching, especially which involves writing data, you\u2019ll want to aim for as high a DWPD rating as possible.</p>"}, {"location": "zfs_storage_planning/#cache-disk-conclusion", "title": "Cache disk conclusion", "text": "<p>Overall, I\u2019d recommend the Western Digital Red SN700, which has a good 1 DWPD endurance rating, is available in sizes up to 4TB, and is using SLC NAND technology, which is great for enhancing reliability through heavy caching workloads. A close second place goes to the Seagate IronWolf 525, which has similar specifications to the SN700 but utilizes TLC.</p> Disk Size Speed Endurance Warranty Tech Price WD Red SN700 500 GB 3430MB/s 1 DWPD 5 years SLC 73$ SG IronWolf 525 500 GB 5000MB/s 0.8 DWPD 5 years TLC ? WD Red SN700 1 TB 3430MB/s 1 DWPD 5 years SLC 127$ SG IronWolf 525 1 TB 5000MB/s 0.8 DWPD 5 years TLC ?"}, {"location": "zfs_storage_planning/#choosing-the-cold-spare-disks", "title": "Choosing the cold spare disks", "text": "<p>It's good to think how much time you want to have your raids to be inconsistent once a drive has failed.</p> <p>In my case, for the data I want to restore the raid as soon as I can, therefore I'll buy another rotational disk. For the SSDs I have more confidence that they won't break so I don't feel like having a spare one.</p>"}, {"location": "zfs_storage_planning/#design-your-pools", "title": "Design your pools", "text": ""}, {"location": "zfs_storage_planning/#pool-configuration", "title": "Pool configuration", "text": "<ul> <li>Use <code>ashift=12</code> or <code>ashift=13</code> when creating the pool if applicable (though ZFS can detect correctly for most cases). Value of <code>ashift</code> is exponent of 2, which should be aligned to the physical sector size of disks, for example <code>2^9=512</code>, <code>2^12=4096</code>, <code>2^13=8192</code>. Some disks are reporting a logical sector size of 512 bytes while having 4KiB physical sector size , and some SSDs have 8KiB physical sector size. </li> </ul> <p>Consider using <code>ashift=12</code> or <code>ashift=13</code> even if currently using only disks with 512 bytes sectors. Adding devices with bigger sectors to the same VDEV can severely impact performance due to wrong alignment, while a device with 512 sectors will work also with a higher <code>ashift</code>. </p> <ul> <li>Set \"autoexpand\" to on, so you can expand the storage pool automatically after all disks in the pool have been replaced with larger ones. Default is off.</li> </ul>"}, {"location": "zfs_storage_planning/#zil-or-slog", "title": "ZIL or SLOG", "text": "<p>Before we can begin, we need to get a few terms out of the way that seem to be confusing:</p> <ul> <li>ZFS Intent Log, or ZIL is a logging mechanism where all of the data to be the written is stored, then later flushed as a transactional write. Similar in function to a journal for journaled filesystems, like <code>ext3</code> or <code>ext4</code>. Typically stored on platter disk. Consists of a ZIL header, which points to a list of records, ZIL blocks and a ZIL trailer. The ZIL behaves differently for different writes. For writes smaller than 64KB (by default), the ZIL stores the write data. For writes larger, the write is not stored in the ZIL, and the ZIL maintains pointers to the synched data that is stored in the log record.</li> <li>Separate Intent Log, or SLOG, is a separate logging device that caches the synchronous parts of the ZIL before flushing them to slower disk. This would either be a battery-backed DRAM drive or a fast SSD. The SLOG only caches synchronous data, and does not cache asynchronous data. Asynchronous data will flush directly to spinning disk. Further, blocks are written a block-at-a-time, rather than as simultaneous transactions to the SLOG. If the SLOG exists, the ZIL will be moved to it rather than residing on platter disk. Everything in the SLOG will always be in system memory.</li> </ul> <p>When you read online about people referring to \"adding an SSD ZIL to the pool\", they are meaning adding an SSD SLOG, of where the ZIL will reside. The ZIL is a subset of the SLOG in this case. The SLOG is the device, the ZIL is data on the device. Further, not all applications take advantage of the ZIL. Applications such as databases (MySQL, PostgreSQL, Oracle), NFS and iSCSI targets do use the ZIL. Typical copying of data around the filesystem will not use it. Lastly, the ZIL is generally never read, except at boot to see if there is a missing transaction. The ZIL is basically \"write-only\", and is very write-intensive.</p> <p>It's important to use devices that can maintain data persistence during a power outage. The SLOG and the ZIL are critical in getting your data to spinning platter. If a power outage occurs, and you have a volatile SLOG, the worst thing that will happen is the new data is not flushed, and you are left with old data. However, it's important to note, that in the case of a power outage, you won't have corrupted data, just lost data. Your data will still be consistent on disk.</p> <p>If you use a SLOG you will see improved disk latencies, disk utilization and system load. What you won't see is improved throughput. Remember that the SLOG device is still flushing data to platter every 5 seconds. As a result, benchmarking disk after adding a SLOG device doesn't make much sense, unless the goal of the benchmark is to test synchronous disk write latencies. Check this article if you want to know more.</p>"}, {"location": "zfs_storage_planning/#adding-a-slog", "title": "Adding a SLOG", "text": "<p>WARNING: Some motherboards will not present disks in a consistent manner to the Linux kernel across reboots. As such, a disk identified as <code>/dev/sda</code> on one boot might be <code>/dev/sdb</code> on the next. For the main pool where your data is stored, this is not a problem as ZFS can reconstruct the VDEVs based on the metadata geometry. For your L2ARC and SLOG devices, however, no such metadata exists. So, rather than adding them to the pool by their <code>/dev/sd?</code> names, you should use the <code>/dev/disk/by-id/*</code> names, as these are symbolic pointers to the ever-changing <code>/dev/sd?</code> files. If you don't heed this warning, your SLOG device may not be added to your hybrid pool at all, and you will need to re-add it later. This could drastically affect the performance of the applications depending on the existence of a fast SLOG.</p> <p>Adding a SLOG to your existing zpool is not difficult. However, it is considered best practice to mirror the SLOG. Suppose that there are 4 platter disks in the pool, and two NVME. </p> <p>First you need to create a partition of 5 GB on each the nvme drive:</p> <pre><code>fdisk -l\nfdisk /dev/nvme0n1\nfdisk /dev/nvme1n1\n</code></pre> <p>Then mirror the partitions as SLOG</p> <pre><code>zpool add tank log mirror \\\n/dev/disk/by-id/nvme0n1-part1 \\\n/dev/disk/by-id/nvme1n1-part1 </code></pre> <p>Check that it worked with <pre><code># zpool status\npool: tank\n state: ONLINE\n scan: scrub repaired 0 in 1h8m with 0 errors on Sun Dec  2 01:08:26 2012\nconfig:\n\n        NAME               STATE     READ WRITE CKSUM\n        pool               ONLINE       0     0     0\nraidz1-0         ONLINE       0     0     0\nsdd            ONLINE       0     0     0\nsde            ONLINE       0     0     0\nsdf            ONLINE       0     0     0\nsdg            ONLINE       0     0     0\nlogs\n          mirror-1         ONLINE       0     0     0\nnvme0n1-part1  ONLINE       0     0     0\nnvme0n1-part2  ONLINE       0     0     0\n</code></pre></p> <p>You will likely not need a large ZIL, take into account that zfs dumps it's contents quite often.</p>"}, {"location": "zfs_storage_planning/#adjustable-replacement-cache", "title": "Adjustable Replacement Cache", "text": "<p>The ZFS adjustable replacement cache (ARC) is one such caching mechanism that caches both recent block requests as well as frequent block requests.  It will occupy \u00bd of available RAM. However, this isn't static. If you have 32 GB of RAM in your server, this doesn't mean the cache will always be 16 GB. Rather, the total cache will adjust its size based on kernel decisions. If the kernel needs more RAM for a scheduled process, the ZFS ARC will be adjusted to make room for whatever the kernel needs. However, if there is space that the ZFS ARC can occupy, it will take it up.</p> <p>The ARC can be extended using the level 2 ARC or L2ARC. This means that as the MRU (the most recently requested blocks from the filesystem) or MFU (the most frequently requested blocks from the filesystem) grow, they don't both simultaneously share the ARC in RAM and the L2ARC on your SSD. Instead, when a page is about to be evicted, a walking algorithm will evict the MRU and MFU pages into an 8 MB buffer, which is later set as an atomic write transaction to the L2ARC. The advantage is that the latency of evicting pages from the cache is not impacted. Further, if a large read of data blocks is sent to the cache, the blocks are evicted before the L2ARC walk, rather than sent to the L2ARC. This minimizes polluting the L2ARC with massive sequential reads. Filling the L2ARC can also be very slow, or very fast, depending on the access to your data.</p> <p>Persistence in the L2ARC is not needed, as the cache will be wiped on boot. To learn more about the ZFS ARC read 1.</p>"}, {"location": "zfs_storage_planning/#adding-an-l2arc", "title": "Adding an L2ARC", "text": "<p>First you need to create the partitions on each the nvme drive:</p> <pre><code>fdisk -l\nfdisk /dev/nvme0n1\nfdisk /dev/nvme1n1\n</code></pre> <p>It is recommended that you stripe the L2ARC to maximize both size and speed.</p> <pre><code>zpool add tank cache \\\n/dev/disk/by-id/nvme0n1-part2 \\\n/dev/disk/by-id/nvme1n1-part2 </code></pre> <p>Check that it worked with <pre><code># zpool status\npool: tank\n state: ONLINE\n scan: scrub repaired 0 in 1h8m with 0 errors on Sun Dec  2 01:08:26 2012\nconfig:\n\n        NAME               STATE     READ WRITE CKSUM\n        pool               ONLINE       0     0     0\nraidz1-0         ONLINE       0     0     0\nsdd            ONLINE       0     0     0\nsde            ONLINE       0     0     0\nsdf            ONLINE       0     0     0\nsdg            ONLINE       0     0     0\ncache\n          nvme0n1-part1    ONLINE       0     0     0\nnvme0n1-part2    ONLINE       0     0     0\n</code></pre></p> <p>To check hte size of the L2ARC use <code>zpool iostat -v</code>.</p>"}, {"location": "zfs_storage_planning/#follow-the-best-practices", "title": "Follow the best practices", "text": "<p>As with all recommendations, some of these guidelines carry a great amount of weight, while others might not. You may not even be able to follow them as rigidly as you would like. Regardless, you should be aware of them. The idea of \"best practices\" is to optimize space efficiency, performance and ensure maximum data integrity.</p> <ul> <li>Keep pool capacity under 80% for best performance. Due to the copy-on-write nature of ZFS, the filesystem gets heavily fragmented.</li> <li>Only run ZFS on 64-bit kernels. It has 64-bit specific code that 32-bit kernels cannot do anything with.</li> <li>Install ZFS only on a system with lots of RAM. 1 GB is a bare minimum, 2 GB is better, 4 GB would be preferred to start. Remember, ZFS will use \u00bd of the available RAM for the ARC.</li> <li>Use ECC RAM when possible for scrubbing data in registers and maintaining data consistency. The ARC is an actual read-only data cache of valuable data in RAM.</li> <li>Use whole disks rather than partitions. ZFS can make better use of the on-disk cache as a result. If you must use partitions, backup the partition table, and take care when reinstalling data into the other partitions, so you don't corrupt the data in your pool.</li> <li>Keep each VDEV in a storage pool the same size. If VDEVs vary in size, ZFS will favor the larger VDEV, which could lead to performance bottlenecks.</li> <li>Use redundancy when possible, as ZFS can and will want to correct data errors that exist in the pool. You cannot fix these errors if you do not have a redundant good copy elsewhere in the pool. Mirrors and RAID-Z levels accomplish this.</li> <li>Do not use raidz1 for disks 1TB or greater in size.</li> <li>For raidz1, do not use less than 3 disks, nor more than 7 disks in each vdev </li> <li>For raidz2, do not use less than 6 disks, nor more than 10 disks in each vdev (8 is a typical average).</li> <li>For raidz3, do not use less than 7 disks, nor more than 15 disks in each vdev (13 &amp; 15 are typical average).</li> <li>Consider using RAIDZ-2 or RAIDZ-3 over RAIDZ-1. You've heard the phrase \"when it rains, it pours\". This is true for disk failures. If a disk fails in a RAIDZ-1, and the hot spare is getting resilvered, until the data is fully copied, you cannot afford another disk failure during the resilver, or you will suffer data loss. With RAIDZ-2, you can suffer two disk failures, instead of one, increasing the probability you have fully resilvered the necessary data before the second, and even third disk fails.</li> <li>Perform regular (at least weekly) backups of the full storage pool. It's not a backup, unless you have multiple copies. Just because you have redundant disk, does not ensure live running data in the event of a power failure, hardware failure or disconnected cables.</li> <li>Use hot spares to quickly recover from a damaged device. Set the \"autoreplace\" property to on for the pool.</li> <li>Consider using a hybrid storage pool with fast SSDs or NVRAM drives. Using a fast SLOG and L2ARC can greatly improve performance.</li> <li>If using a hybrid storage pool with multiple devices, mirror the SLOG and stripe the L2ARC.</li> <li>If using a hybrid storage pool, and partitioning the fast SSD or NVRAM drive, unless you know you will need it, 1 GB is likely sufficient for your SLOG. Use the rest of the SSD or NVRAM drive for the L2ARC. The more storage for the L2ARC, the better.</li> <li>If possible, scrub consumer-grade SATA and SCSI disks weekly and enterprise-grade SAS and FC disks monthly. Depending on a lot factors, this might not be possible, so your mileage may vary. But, you should scrub as frequently as possible, basically.</li> <li>Email reports of the storage pool health weekly for redundant arrays, and bi-weekly for non-redundant arrays.</li> <li>When using advanced format disks that read and write data in 4 KB sectors, set the \"ashift\" value to 12 on pool creation for maximum performance. Default is 9 for 512-byte sectors.</li> <li>Set \"autoexpand\" to on, so you can expand the storage pool automatically after all disks in the pool have been replaced with larger ones. Default is off.</li> <li>Always export your storage pool when moving the disks from one physical system to another.</li> <li>When considering performance, know that for sequential writes, mirrors will always outperform RAID-Z levels. For sequential reads, RAID-Z levels will perform more slowly than mirrors on smaller data blocks and faster on larger data blocks. For random reads and writes, mirrors and RAID-Z seem to perform in similar manners. Striped mirrors will outperform mirrors and RAID-Z in both sequential, and random reads and writes.</li> <li>Compression is disabled by default. This doesn't make much sense with today's hardware. ZFS compression is extremely cheap, extremely fast, and barely adds any latency to the reads and writes. In fact, in some scenarios, your disks will respond faster with compression enabled than disabled. A further benefit is the massive space benefits.</li> <li>Unless you have the RAM, avoid using deduplication. Unlike compression, deduplication is very costly on the system. The deduplication table consumes massive amounts of RAM.</li> <li>Avoid running a ZFS root filesystem on GNU/Linux for the time being. It's a bit too experimental for /boot and GRUB. However, do create datasets for /home/, /var/log/ and /var/cache/.</li> <li>Snapshot frequently and regularly. Snapshots are cheap, and can keep a plethora of file versions over time.</li> <li>Snapshots are not a backup. Use \"zfs send\" and \"zfs receive\" to send your ZFS snapshots to an external storage.</li> <li>If using NFS, use ZFS NFS rather than your native exports. This can ensure that the dataset is mounted and online before NFS clients begin sending data to the mountpoint.</li> <li>Don't mix NFS kernel exports and ZFS NFS exports. This is difficult to administer and maintain.</li> <li>For /home/ ZFS installations, setting up nested datasets for each user. For example, pool/home/atoponce and pool/home/dobbs. Consider using quotas on the datasets.</li> <li>When using \"zfs send\" and \"zfs receive\", send incremental streams with the \"zfs send -i\" switch. This can be an exceptional time saver.</li> <li>Consider using \"zfs send\" over \"rsync\", as the \"zfs send\" command can preserve dataset properties.</li> </ul> <p>There are some caveats though. The point of the caveat list is by no means to discourage you from using ZFS. Instead, as a storage administrator planning out your ZFS storage server, these are things that you should be aware of. If you don't head these warnings, you could end up with corrupted data. The line may be blurred with the \"best practices\" list above.</p> <ul> <li>Your VDEVs determine the IOPS of the storage, and the slowest disk in that VDEV will determine the IOPS for the entire VDEV.</li> <li>ZFS uses 1/64 of the available raw storage for metadata. So, if you purchased a 1 TB drive, the actual raw size is 976 GiB. After ZFS uses it, you will have 961 GiB of available space. The \"zfs list\" command will show an accurate representation of your available storage. Plan your storage keeping this in mind.</li> <li>ZFS wants to control the whole block stack. It checksums, resilvers live data instead of full disks, self-heals corrupted blocks, and a number of other unique features. If using a RAID card, make sure to configure it as a true JBOD (or \"passthrough mode\"), so ZFS can control the disks. If you can't do this with your RAID card, don't use it. Best to use a real HBA.</li> <li>Do not use other volume management software beneath ZFS. ZFS will perform better, and ensure greater data integrity, if it has control of the whole block device stack. As such, avoid using dm-crypt, mdadm or LVM beneath ZFS.</li> <li>Do not share a SLOG or L2ARC DEVICE across pools. Each pool should have its own physical DEVICE, not logical drive, as is the case with some PCI-Express SSD cards. Use the full card for one pool, and a different physical card for another pool. If you share a physical device, you will create race conditions, and could end up with corrupted data.</li> <li>Do not share a single storage pool across different servers. ZFS is not a clustered filesystem. Use GlusterFS, Ceph, Lustre or some other clustered filesystem on top of the pool if you wish to have a shared storage backend.</li> <li>Other than a spare, SLOG and L2ARC in your hybrid pool, do not mix VDEVs in a single pool. If one VDEV is a mirror, all VDEVs should be mirrors. If one VDEV is a RAIDZ-1, all VDEVs should be RAIDZ-1. Unless of course, you know what you are doing, and are willing to accept the consequences. ZFS attempts to balance the data across VDEVs. Having a VDEV of a different redundancy can lead to performance issues and space efficiency concerns, and make it very difficult to recover in the event of a failure.</li> <li>Do not mix disk sizes or speeds in a single VDEV. Do mix fabrication dates, however, to prevent mass drive failure.</li> <li>In fact, do not mix disk sizes or speeds in your storage pool at all.</li> <li>Do not mix disk counts across VDEVs. If one VDEV uses 4 drives, all VDEVs should use 4 drives.</li> <li>Do not put all the drives from a single controller in one VDEV. Plan your storage, such that if a controller fails, it affects only the number of disks necessary to keep the data online.</li> <li>When using advanced format disks, you must set the ashift value to 12 at pool creation. It cannot be changed after the fact. Use \"zpool create -o ashift=12 tank mirror sda sdb\" as an example.</li> <li>Hot spare disks will not be added to the VDEV to replace a failed drive by default. You MUST enable this feature. Set the autoreplace feature to on. Use \"zpool set autoreplace=on tank\" as an example.</li> <li>The storage pool will not auto resize itself when all smaller drives in the pool have been replaced by larger ones. You MUST enable this feature, and you MUST enable it before replacing the first disk. Use \"zpool set autoexpand=on tank\" as an example.</li> <li>ZFS does not restripe data in a VDEV nor across multiple VDEVs. Typically, when adding a new device to a RAID array, the RAID controller will rebuild the data, by creating a new stripe width. This will free up some space on the drives in the pool, as it copies data to the new disk. ZFS has no such mechanism. Eventually, over time, the disks will balance out due to the writes, but even a scrub will not rebuild the stripe width.</li> <li>You cannot shrink a zpool, only grow it. This means you cannot remove VDEVs from a storage pool.</li> <li>You can only remove drives from mirrored VDEV using the \"zpool detach\" command. You can replace drives with another drive in RAIDZ and mirror VDEVs however.</li> <li>Do not create a storage pool of files or ZVOLs from an existing zpool. Race conditions will be present, and you will end up with corrupted data. Always keep multiple pools separate.</li> <li>The Linux kernel may not assign a drive the same drive letter at every boot. Thus, you should use the /dev/disk/by-id/ convention for your SLOG and L2ARC. If you don't, your zpool devices could end up as a SLOG device, which would in turn clobber your ZFS data.</li> <li>Don't create massive storage pools \"just because you can\". Even though ZFS can create 78-bit storage pool sizes, that doesn't mean you need to create one.</li> <li>Don't put production directly into the zpool. Use ZFS datasets instead.</li> <li>Don't commit production data to file VDEVs. Only use file VDEVs for testing scripts or learning the ins and outs of ZFS.</li> <li>A \"zfs destroy\" can cause downtime for other datasets. A \"zfs destroy\" will touch every file in the dataset that resides in the storage pool. The larger the dataset, the longer this will take, and it will use all the possible IOPS out of your drives to make it happen. Thus, if it take 2 hours to destroy the dataset, that's 2 hours of potential downtime for the other datasets in the pool.</li> <li>Debian and Ubuntu will not start the NFS daemon without a valid export in the /etc/exports file. You must either modify the /etc/init.d/nfs init script to start without an export, or create a local dummy export.</li> <li>When creating ZVOLs, make sure to set the block size as the same, or a multiple, of the block size that you will be formatting the ZVOL with. If the block sizes do not align, performance issues could arise.</li> <li>When loading the \"zfs\" kernel module, make sure to set a maximum number for the ARC. Doing a lot of \"zfs send\" or snapshot operations will cache the data. If not set, RAM will slowly fill until the kernel invokes OOM killer, and the system becomes responsive. For example set in the <code>/etc/modprobe.d/zfs.conf</code> file \"options zfs zfs_arc_max=2147483648\", which is a 2 GB limit for the ARC.</li> </ul>"}, {"location": "devops/alex/", "title": "Alex", "text": "<p>Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text.</p> <p>For example, <code>when We\u2019ve confirmed his identity is given</code>, <code>alex</code> will warn you and suggest using <code>their</code> instead of <code>his</code>.</p> <p>Give alex a spin on the Online demo.</p>"}, {"location": "devops/alex/#installation", "title": "Installation", "text": "<pre><code>npm install alex --global\n</code></pre> <p>You can use it with Vim through the ALE plugin.</p> <p>As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter.</p>"}, {"location": "devops/alex/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/api_management/", "title": "API Management", "text": "<p>API management is the process of creating and publishing web application programming interfaces (APIs) under a service that:</p> <ul> <li>Enforces the usage of policies.</li> <li>Controls access.</li> <li>Collects and analyzes usage statistics.</li> <li>Reports on performance.</li> </ul>"}, {"location": "devops/api_management/#components", "title": "Components", "text": "<p>While solutions vary, components that provide the following functionality are typically found in API management products:</p> <ul> <li>Gateway: a server that acts as an API front-end, receives API requests,     enforces throttling and security policies, passes requests to the back-end     service and then passes the response back to the requester. A gateway often     includes a transformation engine to orchestrate and modify the requests and     responses on the fly. A gateway can also provide functionality such as     collecting analytics data and providing caching. The gateway can provide     functionality to support authentication, authorization, security, audit and     regulatory compliance.</li> <li>Publishing tools: a collection of tools that API providers use to define     APIs, for instance using the OpenAPI or RAML specifications, generate API     documentation, manage access and usage policies for APIs, test and debug the     execution of API, including security testing and automated generation of     tests and test suites, deploy APIs into production, staging, and quality     assurance environments, and coordinate the overall API lifecycle.</li> <li>Developer portal/API store: community site, typically branded by an API     provider, that can encapsulate for API users in a single convenient source     information and functionality including documentation, tutorials, sample     code, software development kits, an interactive API console and sandbox to     trial APIs, the ability to subscribe to the APIs and manage subscription     keys such as OAuth2 Client ID and Client Secret, and obtain support from the     API provider and user and community.</li> <li>Reporting and analytics: functionality to monitor API usage and load     (overall hits, completed transactions, number of data objects returned,     amount of compute time and other internal resources consumed, volume of data     transferred). This can include real-time monitoring of the API with alerts     being raised directly or via a higher-level network management system, for     instance, if the load on an API has become too great, as well as     functionality to analyze historical data, such as transaction logs, to     detect usage trends.  Functionality can also be provided to create synthetic     transactions that can be used to test the performance and behavior of API     endpoints. The information gathered by the reporting and analytics     functionality can be used by the API provider to optimize the API offering     within an organization's overall continuous improvement process and for     defining software Service-Level Agreements for APIs.</li> <li>Monetization: functionality to support charging for access to commercial     APIs. This functionality can include support for setting up pricing rules,     based on usage, load and functionality, issuing invoices and collecting     payments including multiple types of credit card payments.</li> </ul>"}, {"location": "devops/bandit/", "title": "Bandit", "text": "<p>Bandit finds common security issues in Python code. To do this, Bandit processes each file, builds an AST from it, and runs appropriate plugins against the AST nodes. Once Bandit has finished scanning all the files, it generates a report.</p> <p>You can use this cookiecutter template to create a python project with <code>bandit</code> already configured.</p>"}, {"location": "devops/bandit/#installation", "title": "Installation", "text": "<pre><code>pip install bandit\n</code></pre>"}, {"location": "devops/bandit/#usage", "title": "Usage", "text": ""}, {"location": "devops/bandit/#ignore-an-error", "title": "Ignore an error.", "text": "<p>Add the <code># nosec</code> comment in the line.</p>"}, {"location": "devops/bandit/#configuration", "title": "Configuration", "text": "<p>You can run bandit through:</p> <ul> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>repos:\n- repo: https://github.com/Lucas-C/pre-commit-hooks-bandit\nrev: v1.0.4\nhooks:\n- id: python-bandit-vulnerability-check\n</code></pre> <p>bandit takes a lot of time to run, so it slows down too much the commiting, therefore it should be run only in the CI.</p> </li> <li> <p>Github Actions: Make sure to check that the correct python version is applied.</p> <p>File: .github/workflows/security.yml</p> <pre><code>name: Security\n\non: [push, pull_request]\n\njobs:\nbandit:\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout\nuses: actions/checkout@v2\n- uses: actions/setup-python@v2\nwith:\npython-version: 3.7\n- name: Install dependencies\nrun: pip install bandit\n- name: Execute bandit\nrun: bandit -r project\n</code></pre> </li> </ul>"}, {"location": "devops/bandit/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "devops/black/", "title": "black", "text": "<p>Black is a style guide enforcement tool.</p> <p>You can use this cookiecutter template to create a python project with <code>black</code> already configured.</p>"}, {"location": "devops/black/#installation", "title": "Installation", "text": "<pre><code>pip install black\n</code></pre>"}, {"location": "devops/black/#configuration", "title": "Configuration", "text": "<p>Its configuration is stored in <code>pyproject.toml</code>.</p> <p>File: pyproject.toml</p> <pre><code># Example configuration for Black.\n\n# NOTE: you have to use single-quoted strings in TOML for regular expressions.\n# It's the equivalent of r-strings in Python.  Multiline strings are treated as\n# verbose regular expressions by Black.  Use [ ] to denote a significant space\n# character.\n\n[tool.black]\nline-length = 88\ntarget-version = ['py36', 'py37', 'py38']\ninclude = '\\.pyi?$'\nexclude = '''\n/(\n\\.eggs\n| \\.git\n| \\.hg\n| \\.mypy_cache\n| \\.tox\n| \\.venv\n| _build\n| buck-out\n| build\n| dist\n# The following are specific to Black, you probably don't want those.\n| blib2to3\n| tests/data\n| profiling\n)/\n'''\n</code></pre> <p>You can use it both with:</p> <ul> <li> <p>The Vim plugin</p> </li> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>repos:\n- repo: https://github.com/ambv/black\nrev: stable\nhooks:\n- id: black\nlanguage_version: python3.7\n</code></pre> </li> <li> <p>Github Actions:</p> <p>File: .github/workflows/lint.yml</p> <pre><code>---\nname: Lint\n\non: [push, pull_request]\n\njobs:\nBlack:\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v2\n- uses: actions/setup-python@v2\n- name: Black\nuses: psf/black@stable\n</code></pre> </li> </ul>"}, {"location": "devops/black/#split-long-lines", "title": "Split long lines", "text": "<p>If you want to split long lines, you need to use the <code>--experimental-string-processing</code> flag. I haven't found how to set that option in the config file.</p>"}, {"location": "devops/black/#disable-the-formatting-of-some-lines", "title": "Disable the formatting of some lines", "text": "<p>You can use the comments <code># fmt: off</code> and <code># fmt: on</code></p>"}, {"location": "devops/black/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "devops/ci/", "title": "CI", "text": "<p>Continuous Integration (CI) allows to automatically run processes on the code each time a commit is pushed.  For example it can be used to run the tests, build the documentation, build a package or maintain dependencies updated.</p> <p>I've automated the configuration of CI/CD pipelines for python projects in this cookiecutter template.</p> <p>There are three non exclusive ways to run the tests:</p> <ul> <li>Integrate them in your editor, so it's executed each time you save the file.</li> <li>Through a pre-commit hook to     make it easy for the collaborator to submit correctly formatted code. pre-commit     is a framework for managing and maintaining multi-language     pre-commit hooks.</li> <li>Through a CI server (like Drone or Github Actions) to ensure that the commited     code meets the quality standards. Developers can bypass the pre-commit     filter, so we need to set up the quality gate in an agnostic environment.</li> </ul> <p>Depending on the time the test takes to run and their different implementations, we'll choose from one to three of the choices above.</p>"}, {"location": "devops/ci/#configuring-pre-commit", "title": "Configuring pre-commit", "text": "<p>To adopt <code>pre-commit</code> to our system we have to:</p> <ul> <li>Install pre-commit: <code>pip3 install pre-commit</code> and add it to the development     <code>requirements.txt</code>.</li> <li>Define <code>.pre-commit-config.yaml</code> with the hooks you want to include (they     don't plan to support pyproject.toml).</li> <li>Execute <code>pre-commit install</code> to install git hooks in your <code>.git/</code> directory.</li> <li>Execute <code>pre-commit run --all-files</code> to tests all the files. Usually     <code>pre-commit</code> will only run on the changed files during git hooks.</li> </ul>"}, {"location": "devops/ci/#static-analysis-checkers", "title": "Static analysis checkers", "text": "<p>Static analysis is the analysis of computer software that is performed without actually executing programs.</p>"}, {"location": "devops/ci/#formatters", "title": "Formatters", "text": "<p>Formatters are tools that change your files to meet a linter requirements.</p> <ul> <li>Black: A python style guide formatter tool.</li> </ul>"}, {"location": "devops/ci/#linters", "title": "Linters", "text": "<p>Lint, or a linter, is a static code analysis tool used to flag programming errors, bugs, stylistic errors, and suspicious constructs. The term originates from a Unix utility that examined C language source code.</p> <ul> <li>alex to find gender favoring, polarizing, race related, religion     inconsiderate, or other unequal phrasing in text.</li> <li>Flake8: A python style guide checker tool.</li> <li>markdownlint: A linter for Markdown files.</li> <li>proselint: Is another linter for prose.</li> <li>Yamllint: A linter for YAML files.</li> <li>write-good is a naive linter for English     prose.</li> </ul>"}, {"location": "devops/ci/#type-checkers", "title": "Type checkers", "text": "<p>Type checkers are programs that the code is compliant with a defined type system which is a logical system comprising a set of rules that assigns a property called a type to the various constructs of a computer program, such as variables, expressions, functions or modules. The main purpose of a type system is to reduce possibilities for bugs by defining interfaces between different parts of the program, and then checking that the parts have been connected in a consistent way.</p> <ul> <li>Mypy: A static type checker for Python.</li> </ul>"}, {"location": "devops/ci/#security-vulnerability-checkers", "title": "Security vulnerability checkers", "text": "<p>Tools that check potential vulnerabilities in the code.</p> <ul> <li>Bandit: Finds common security issues in Python code.</li> <li>Safety: Checks your installed dependencies     for known security vulnerabilities.</li> </ul>"}, {"location": "devops/ci/#other-pre-commit-tests", "title": "Other pre-commit tests", "text": "<p>Pre-commit comes with several tests by default. These are the ones I've chosen.</p> <p>File: .pre-commit-config.yaml</p> <pre><code>repos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\nrev: v3.1.0\nhooks:\n- id: trailing-whitespace\n- id: check-added-large-files\n- id: check-docstring-first\n- id: check-merge-conflict\n- id: end-of-file-fixer\n- id: detect-private-key\n</code></pre>"}, {"location": "devops/ci/#update-package-dependencies", "title": "Update package dependencies", "text": "<p>Tools to automatically keep your dependencies updated.</p> <ul> <li>pip-tools</li> </ul>"}, {"location": "devops/ci/#coverage-reports", "title": "Coverage reports", "text": "<p>Coveralls is a service that monitors and writes statistics on the coverage of your repositories. To use them, you'll need to log in with your Github account and enable the repos you want to test.</p> <p>Save the secret in the repository configuration and add this step to your tests job.</p> <pre><code>    - name: Coveralls\nuses: coverallsapp/github-action@master\nwith:\ngithub-token: ${{ secrets.COVERALLS_TOKEN }}\n</code></pre> <p>Add the following badge to your README.md.</p> <p>Variables to substitute:</p> <ul> <li><code>repository_path</code>: Github repository path, like <code>lyz-code/pydo</code>.</li> </ul> <pre><code>[![Coverage Status](https://coveralls.io/repos/github/{{ repository_path\n}}/badge.svg?branch=master)](https://coveralls.io/github/{{ repository_path }}?branch=master)\n</code></pre>"}, {"location": "devops/ci/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "devops/ci/#error-pathspec-master-did-not-match-any-files-known-to-git", "title": "error: pathspec 'master' did not match any file(s) known to git", "text": "<p>If you have this error while making a commit through a pipeline step, it may be the pre-commits stepping in.</p> <p>To fix it, remove all git hooks with <code>rm -r .git/hooks</code>.</p>"}, {"location": "devops/devops/", "title": "Devops", "text": "<p>DevOps is a set of practices that combines software development (Dev) and information-technology operations (Ops) which aims to shorten the systems development life cycle and provide continuous delivery with high software quality.</p> <p>One of the most important goals of the DevOps initiative is to break the silos between the developers and the sysadmins, that lead to ill feelings and unproductivity.</p> <p>It's a relatively new concept, the main ideas emerged in the 1990s and the first conference was in 2009. That means that as of 2021 there is still a lot of debate of what people understand as DevOps.</p>"}, {"location": "devops/devops/#devops-pitfalls", "title": "DevOps pitfalls", "text": "<p>I've found that the DevOps word leads to some pitfalls that we should try to avoid.</p>"}, {"location": "devops/devops/#getting-lost-in-the-label", "title": "Getting lost in the label", "text": "<p>Labels are a language tool used to speed up communication by describing someone or something in a word or short phrase.</p> <p>However, there are times when labels achieve the complete oposite, as it's the case with DevOps, where there are different views on what the label represents and usually one of the communication parties strongly feels they belong to the label while the other doesn't agree. These discussions can fall into an unproductive, agitated semantic debate where each part tries to convince each other.</p> <p>So instead of starting a twitter thread telling people why they aren't a DevOps team, we could invest those energies in creating resources that close the gap between both parties. Similarly, instead of starting an internal discussion of what do we understand as DevOps?, we could discuss how to improve our existent processes so that the team members feel more comfortable contributing to the application or infrastructure code.</p>"}, {"location": "devops/devops/#you-need-to-do-it-all-to-be-awarded-the-devops-pin", "title": "You need to do it all to be awarded the DevOps pin", "text": "<p>I find specially harmful the idea that to be qualified as DevOps you need to develop and maintain the application at the same time as the infrastructure that holds it.</p> <p>To be able to do that in a typical company product you'll need to know (between another thousand more things):</p> <ul> <li>How to operate the cloud infrastructure where the project lives, which can be     AWS, Google Cloud, Azure or/and baremetal servers.</li> <li>Deploy new resources in that infrastructure, which probably would mean knowing     Terraform, Ansible, Docker or/and Kubernetes.</li> <li>How to integrate the new resources with the operations processes, for example:<ul> <li>The monitoring system, so you'll need to know how to use     Prometheus, Nagios, Zabbix or the existent solution.</li> <li>The continuous integration or delivery system, that you'll need to know     how to maintain, so you have to know how it works and how is it built.</li> <li>The backup system.</li> <li>The log centralizer system.</li> </ul> </li> <li>Infrastructure architecture to know what you need to deploy, how and where.</li> <li>To code efficiently in the language that the application is developed in, for     example Java, Python, Rust, Go, PHP or Javascript, in a way that meets the     quality requirements (code style, linters, coverage and documentation).</li> <li>Knowing how to test your code in that language.</li> <li>Software architecture to structure complex code projects in a maintainable     way.</li> <li>The product you're developing to be able to suggest features and fixtures when     the product owner or the stakeholders show their needs.</li> <li>How to make the application user friendly so anyone wants to use it.</li> <li>And don't forget that you also need to do that in a secure way, so you should     also have to know about pentesting, static and dynamic security tools,     common security vulnerabilities...</li> </ul> <p>And I could keep on going forever. Even if you could get there (and you won't), it wouldn't matter, because when you did, the technologies will have changed so much that you will already be outdated and would need to start over.</p> <p>It's the sickness of the fullstack developer. If you make job openings with this mindset you're going to end up with a team of cis white males in their thirties or forties that are used to earn 3 or 4 times the minimum salary. No other kind of people can reach that point and hold it in time.</p> <p>But bare with me a little longer, even if you make there. What happens when the project changes so that you need to:</p> <ul> <li>Change the programming language of your application.</li> <li>Change the cloud provider.</li> <li>Change the deployment system.</li> <li>Change the program architecture.</li> <li>Change the language framework.</li> </ul> <p>Or any other thousand of possible changes? You would need to be able to keep up with them. Noooo way.</p> <p>Luckily you are not alone. You are a cog in a team, that as a whole is able to overcome these changes. That is why we have developers, sysadmins, security, user experience, quality, scrum master and product owners. So each profile can design, create, investigate and learn it's best in their area of expertise. In the merge of all that personal knowledge is where the team thrives.</p> <p>DevOps then, as I understand it, is the philosophy where the developers and sysadmins try their best to break the barriers that separate them. That idea can be brought to earth by for example:</p> <ul> <li>Open discussions on how to:<ul> <li>Improve the development workflow.</li> <li>Make developers or sysadmins life easier.</li> <li>Make it easier to use the sysadmin tools.</li> <li>Make it easier to understand the developers code.</li> </ul> </li> <li>Formations on the technologies or architecture used by either side.</li> <li>A clear documentation that allows either side to catch up with new changes.</li> <li>Periodic meetings to update each other with the changes.</li> <li>Periodic meetings to release the tension that have appeared between     them.</li> <li>Joined design sessions to decide how to solve problems.</li> </ul>"}, {"location": "devops/devops/#learn-path", "title": "Learn path", "text": "<p>DevOps has become a juicy work, if you want to become one, I think you first need to get the basic knowledge of each of them (developing and operating) before being able to unlock the benefits of the combination of both. You can try to learn both at the same time, but I think it can be a daunting task.</p> <p>To get the basic knowledge of the Ops side I would:</p> <ul> <li> <p>Learn basic Linux administration, otherwise you'll be lost.</p> </li> <li> <p>Learn how to be comfortable searching for anything you don't know, most of     your questions are already answered, and even the most senior people spent     a great amount of time searching for solutions in the project's     documentation, Github issues or Stackoverflow.</p> <p>When you start, navigating this knowledge sources is hard and consumes a lot of your life, but it will get easier with the time.</p> </li> <li> <p>Learn how to use Git. If you can, host your own Gitea, if not, use an existing   service such as Gitlab or Github.</p> </li> <li> <p>Learn how to install and maintain services, (that is why I suggested hosting     your own Gitea). If you don't know what to install, take a look at the     awesome     self-hosted list.</p> </li> <li> <p>Learn how to use Ansible, from now on try to deploy every machine with it.</p> </li> <li> <p>Build a small project inside AWS so you can get used to the most   common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier   resources so you don't need to pay anything. You can try also with Google   Cloud or Azure, but I recommend against it.</p> </li> <li> <p>Once you are comfortable with AWS, learn how to use Terraform. You could for   example deploy the previous project. From now on only use Terraform to   provision AWS resources.</p> </li> <li> <p>Get into the CI/CD world hosting your own Drone, if not, use Gitlab   runners or Github   Actions.</p> </li> </ul> <p>To get the basic knowledge of the Dev side I would:</p> <ul> <li> <p>Learn the basics of a programming language, for example Python. There are     thousand sources there on how to do it, books, articles, videos, forums or     courses, choose the one that suits you best.</p> </li> <li> <p>As with the Ops path, get comfortable with git and searching for things you     don't know.</p> </li> <li> <p>As soon as you can, start doing small programming projects that make your life     easier. Coding your stuff is what's going to make you internalize the     learned concepts, by finding solutions to the blocks you encounter.</p> </li> <li> <p>Publish those projects into a public git server, don't be afraid if you code     is good enough, it works for you, you did your best and you should be happy     about it. That's all that matters. By doing so, you'll start collaborating     to the open source world and it will probably force yourself to make your     code better.</p> </li> <li> <p>Step into the TDD world, learn why, how and when to test your code.</p> </li> <li> <p>For those projects that you want to maintain, create CI/CD pipelines     that enhance the quality of your code, by for example running your tests or     some linters.</p> </li> <li> <p>Once you're comfortable, try to collaborate with existent projects (right now     you may not now where to look for projects to collaborate, but when you reach     this point, I promise you will).</p> </li> </ul>"}, {"location": "devops/flake8/", "title": "Flake8", "text": "<p>DEPRECATION: Use Flakehell instead</p> <p>Flake8 doesn't support <code>pyproject.toml</code>, which is becoming the standard, so I suggest using Flakehell instead.</p> <p>Flake8 is a style guide enforcement tool. Its configuration is stored in <code>setup.cfg</code>, <code>tox.ini</code> or <code>.flake8</code>.</p> <p>File: .flake8</p> <pre><code>[flake8]\n# ignore = E203, E266, E501, W503, F403, F401\nmax-line-length = 88\n# max-complexity = 18\n# select = B,C,E,F,W,T4,B9\n</code></pre> <p>You can use it both with:</p> <ul> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>repos:\n- repo: https://gitlab.com/pycqa/flake8\nrev: master\nhooks:\n- id: flake8\n</code></pre> <ul> <li>Github Actions:</li> </ul> <p>File: .github/workflows/lint.yml</p> <pre><code>name: Lint\n\non: [push, pull_request]\n\njobs:\nFlake8:\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v2\n- uses: actions/setup-python@v2\n- name: Flake8\nuses: cclauss/GitHub-Action-for-Flake8@v0.5.0\n</code></pre> </li> </ul>"}, {"location": "devops/flake8/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "devops/helmfile/", "title": "Helmfile", "text": "<p>Helmfile is a declarative spec for deploying Helm charts. It lets you:</p> <ul> <li>Keep a directory of chart value files and maintain changes in version control.</li> <li>Apply CI/CD to configuration changes.</li> <li>Environmental chart promotion.</li> <li>Periodically sync to avoid skew in environments.</li> </ul> <p>To avoid upgrades for each iteration of helm, the helmfile executable delegates to helm - as a result, helm must be installed.</p> <p>All information is saved in the <code>helmfile.yaml</code> file.</p> <p>In case we need custom yamls, we'll use kustomize.</p>"}, {"location": "devops/helmfile/#installation", "title": "Installation", "text": "<p>Helmfile is not yet in the distribution package managers, so you'll need to install it manually.</p> <p>Gather the latest release number.</p> <pre><code>wget {{ bin_url }} -O helmfile_linux_amd64\nchmod +x helmfile_linux_amd64\nmv helmfile_linux_amd64 ~/.local/bin/helmfile\n</code></pre>"}, {"location": "devops/helmfile/#usage", "title": "Usage", "text": ""}, {"location": "devops/helmfile/#how-to-deploy-a-new-chart", "title": "How to deploy a new chart", "text": "<p>When we want to add a new chart, the workflow would be:</p> <ul> <li>Run <code>helmfile deps &amp;&amp; helmfile diff</code> to check that your existing charts are   updated, if they are not, run <code>helmfile apply</code>.</li> <li>Configure the release in <code>helmfile.yaml</code> specifying:   <code>name</code>: Deployment name.</li> <li><code>namespace</code>: K8s namespace to deploy.</li> <li><code>chart</code>: Chart release.</li> <li><code>values</code>: path pointing to the values file created above.</li> <li>Create a directory with the <code>{{ chart_name }}</code>.   <pre><code>mkdir {{ chart_name }}\n</code></pre></li> <li>Get a copy of the chart values inside that directory.   <pre><code>helm inspect values {{ package_name }} &gt; {{ chart_name }}/values.yaml\n</code></pre></li> <li>Edit the <code>values.yaml</code> file according to the chart documentation. Be careful     becase some charts specify the docker image version in the name. Comment out     that line because upgrading the chart version without upgrading the image     tag can break the service.</li> <li>Run <code>helmfile deps</code> to update the lock file.</li> <li>Run <code>helmfile diff</code> to check the changes.</li> <li>Run <code>helmfile apply</code> to apply the changes.</li> </ul>"}, {"location": "devops/helmfile/#keep-charts-updated", "title": "Keep charts updated", "text": "<p>Updating charts with <code>helmfile</code> is easy as long as you don't use environments, you run <code>helmfile deps</code>, then <code>helmfile diff</code> and finally <code>helmfile apply</code>. The tricky business comes when you want to use environments to reuse your helmfile code and don't repeat yourself.</p> <p>This is my suggested workflow, I've opened an issue to see if the developers agree with it:</p> <p>As of today, helmfile doesn't support lock files per environment, that means that the lock file needs to be shared by all of them. At a first sight this is a good idea, because it forces us to have the same versions of the charts in all the environments.</p> <p>The problem comes when you want to upgrade the charts of staging, test that they work and then apply the same changes in production. You'd start the process by running <code>helmfile deps</code>, which will read the helmfiles and update the lock file to the latest version. From this point on you need to be careful on executing the next steps in order so as not to break production.</p> <ul> <li>Tell your team that you're going to do the update operation, so that     they don't try to run <code>helmfile</code> against any environment of the cluster.</li> <li> <p>Run <code>helmfile --environment=staging diff</code> to review the changes to be introduced.</p> <p>To be able to see the differences of long diff files, you can filter it with <code>egrep</code>.</p> <pre><code>helmfile diff | egrep -A20 -B20 \"^.{5}(\\-|\\+)\"\n</code></pre> <p>It will show you all the changed lines with the 20 previous and next ones. * Once you agree on them, run <code>helmfile --environment=staging apply</code> to apply them. * Check that all the helm deployments are well deployed with <code>helm list -A | grep -v deployed</code> * Wait 20 minutes to see if the monitoring system or your fellow partners start yelling at you. * If something breaks up, try to fix it up, if you see it's going to delay you to the point that you're not going to be able to finish the upgrade in your working day, it's better to revert back to the working version of that chart and move on with the next steps. Keep in mind that since you run the <code>apply</code> to the last of the steps of this long process, the team is blocked by you. So prioritize to commit the next stable version to the version control repository. * Once you've checked that all the desired upgrades are working, change the context to the production cluster and run <code>helmfile --environment=production diff</code>. This review should be quick, as it should be the same as the staging one. * Now upgrade the production environment with <code>helmfile --environment=production apply</code>. * Check that all the helm deployments are well deployed with <code>helm list -A | grep -v deployed</code> * Wait another 20 minutes and check that everything is working. * Make a commit with the new lockfile and upload it to the version control repository.</p> </li> </ul> <p>If you want the team to be involved in the review process, you can open a PR with the lock file updated with the <code>WIP</code> state, and upload the relevant diff of staging and production, let the discussion end and then run the apply on staging and then on production if everything goes well.</p> <p>Another ugly solution that I thought was to have a lockfile per environment, and let a Makefile manage them, for example, copying it to <code>helmfile.lock</code> before running any command.</p>"}, {"location": "devops/helmfile/#uninstall-charts", "title": "Uninstall charts", "text": "<p>Helmfile still doesn't remove charts if you remove them from your <code>helmfile.yaml</code>. To remove them you have to either set <code>installed: false</code> in the release candidate and execute <code>helmfile apply</code> or delete the release definition from your helmfile and remove it using standard helm commands.</p>"}, {"location": "devops/helmfile/#force-the-reinstallation-of-everything", "title": "Force the reinstallation of everything", "text": "<p>If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use <code>helmfile sync</code> which will reinstall all the releases.</p>"}, {"location": "devops/helmfile/#multi-environment-project-structure", "title": "Multi-environment project structure", "text": "<p><code>helmfile</code> can handle environments with many different project structures. Such as the next one:</p> <pre><code>\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 helmfile.yaml\n\u251c\u2500\u2500 vars\n\u2502   \u251c\u2500\u2500 production_secrets.yaml\n\u2502   \u251c\u2500\u2500 production_values.yaml\n\u2502   \u251c\u2500\u2500 default_secrets.yaml\n\u2502   \u2514\u2500\u2500 default_values.yaml\n\u251c\u2500\u2500 charts\n\u2502   \u251c\u2500\u2500 local_defined_chart_1\n\u2502   \u2514\u2500\u2500 local_defined_chart_2\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 environments.yaml\n\u2502   \u2514\u2500\u2500 templates.yaml\n\u251c\u2500\u2500 base\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 helmfile.yaml\n\u2502   \u251c\u2500\u2500 helmfile.lock\n\u2502   \u251c\u2500\u2500 repos.yaml\n\u2502   \u251c\u2500\u2500 chart_1\n\u2502   \u2502   \u251c\u2500\u2500 secrets.yaml\n\u2502   \u2502   \u251c\u2500\u2500 values.yaml\n\u2502   \u2502   \u251c\u2500\u2500 production_secrets.yaml\n\u2502   \u2502   \u251c\u2500\u2500 production_values.yaml\n\u2502   \u2502   \u251c\u2500\u2500 default_secrets.yaml\n\u2502   \u2502   \u2514\u2500\u2500 default_values.yaml\n\u2502   \u2514\u2500\u2500 chart_2\n\u2502       \u251c\u2500\u2500 secrets.yaml\n\u2502       \u251c\u2500\u2500 values.yaml\n\u2502       \u251c\u2500\u2500 production_secrets.yaml\n\u2502       \u251c\u2500\u2500 production_values.yaml\n\u2502       \u251c\u2500\u2500 default_secrets.yaml\n\u2502       \u2514\u2500\u2500 default_values.yaml\n\u2514\u2500\u2500 service_1\n    \u251c\u2500\u2500 README.md\n    \u251c\u2500\u2500 helmfile.yaml\n    \u251c\u2500\u2500 helmfile.lock\n    \u251c\u2500\u2500 repos.yaml\n    \u251c\u2500\u2500 chart_1\n    \u2502   \u251c\u2500\u2500 secrets.yaml\n    \u2502   \u251c\u2500\u2500 values.yaml\n    \u2502   \u251c\u2500\u2500 production_secrets.yaml\n    \u2502   \u251c\u2500\u2500 production_values.yaml\n    \u2502   \u251c\u2500\u2500 default_secrets.yaml\n    \u2502   \u2514\u2500\u2500 default_values.yaml\n    \u2514\u2500\u2500 chart_2\n        \u251c\u2500\u2500 secrets.yaml\n        \u251c\u2500\u2500 values.yaml\n        \u251c\u2500\u2500 production_secrets.yaml\n        \u251c\u2500\u2500 production_values.yaml\n        \u251c\u2500\u2500 default_secrets.yaml\n        \u2514\u2500\u2500 default_values.yaml\n</code></pre> <p>Where:</p> <ul> <li>There is a general <code>README.md</code> that introduces the repository.</li> <li> <p>Optionally there could be a <code>helmfile.yaml</code> file at the root with a glob     pattern so that it's easy     to run commands on all children helmfiles.</p> <p><pre><code>helmfiles:\n- ./*/helmfile.yaml\n</code></pre> * There is a <code>vars</code> directory to store the variables and secrets shared by the charts that belong to different services. * There is a <code>templates</code> directory to store the helmfile code to reuse through templates and layering. * The project structure is defined by the services hosted in the Kubernetes cluster. Each service contains:</p> <ul> <li>A <code>README.md</code> to document the service implementation.</li> <li>A <code>helmfile.yaml</code> file to configure the service charts.</li> <li>A <code>helmfile.lock</code> to lock the versions of the service charts.</li> <li>A <code>repos.yaml</code> to define the repositories to fetch the charts from.</li> <li>One or more chart directories that contain the environment specific and     shared chart values and secrets.</li> </ul> </li> <li> <p>There is a <code>base</code> service that manages all the charts required to keep the     cluster running, such as the ingress, csi, cni or the cluster-autoscaler.</p> </li> </ul>"}, {"location": "devops/helmfile/#using-helmfile-environments", "title": "Using helmfile environments", "text": "<p>To customize the contents of a <code>helmfile.yaml</code> or <code>values.yaml</code> file per environment, add them under the <code>environments</code> key in the <code>helmfile.yaml</code>:</p> <pre><code>environments:\ndefault:\nproduction:\n</code></pre> <p>The environment name defaults to <code>default</code>, that is, <code>helmfile sync</code> implies the <code>default</code> environment. So it's a good idea to use staging as <code>default</code> to be more robust against human errors. If you want to specify a non-default environment, provide a <code>--environment NAME</code> flag to helmfile like <code>helmfile --environment production sync</code>.</p> <p>In the <code>environments</code> definition we'll load the values and secrets from the <code>vars</code> directory with the next snippet.</p> <pre><code>environments:\ndefault:\nsecrets:\n- ../vars/default_secrets.yaml\nvalues:\n- ../vars/default_values.yaml\nproduction:\nsecrets:\n- ../vars/production_secrets.yaml\nvalues:\n- ../vars/production_values.yaml\n</code></pre> <p>As this snippet is going to be repeated on every <code>helmfile.yaml</code> we'll use a state layering for it.</p> <p>To install a release only in one environment use:</p> <pre><code>environments:\ndefault:\nproduction:\n\n---\n\nreleases:\n- name: newrelic-agent\ninstalled: {{ eq .Environment.Name \"production\" | toYaml }}\n# snip\n</code></pre>"}, {"location": "devops/helmfile/#using-environment-specific-variables", "title": "Using environment specific variables", "text": "<p>Environment Values allows you to inject a set of values specific to the selected environment, into <code>values.yaml</code> templates or <code>helmfile.yaml</code> files. Use it to inject common values from the environment to multiple values files, to make your configuration DRY.</p> <p>Suppose you have three files helmfile.yaml, production.yaml and values.yaml.gotmpl</p> <p>File: <code>helmfile.yaml</code></p> <pre><code>environments:\nproduction:\nvalues:\n- production.yaml\n\n---\n\nreleases:\n- name: myapp\nvalues:\n- values.yaml.gotmpl\n</code></pre> <p>File: <code>production.yaml</code></p> <pre><code>domain: prod.example.com\n</code></pre> <p>File: <code>values.yaml.gotmpl</code></p> <pre><code>domain: {{ .Values | get \"domain\" \"dev.example.com\" }}\n</code></pre> <p>Sadly you can't use templates in the secrets files, so you'll need to repeat the code.</p>"}, {"location": "devops/helmfile/#loading-the-chart-variables-and-secrets", "title": "Loading the chart variables and secrets", "text": "<p>For each chart definition in the <code>helmfile.yaml</code> we need to load it's secrets and values. We could use the next snippet:</p> <pre><code>  - name: chart_1\nvalues:\n- ./chart_1/values.yaml\n- ./chart_1/{{ Environment.Name }}_values.yaml\nsecrets:\n- ./chart_1/secrets.yaml\n- ./chart_1/{{ Environment.Name }}_secrets.yaml\n</code></pre> <p>This assumes that the <code>environment</code> variable is set, as it's going to be shared by all the <code>helmfiles.yaml</code> you can add it to the <code>vars</code> files:</p> <p>File: <code>vars/production_values.yaml</code></p> <pre><code>environment: production\n</code></pre> <p>File: <code>vars/default_values.yaml</code></p> <pre><code>environment: staging\n</code></pre> <p>Instead of <code>.Environment.Name</code>, in theory you could have used <code>.Vars | get \"environment\"</code>, which could have prevented the variables and secrets of the default environment will need to be called <code>default_values.yaml</code>, and <code>default_secrets.yaml</code>, which is misleading. But you can't use <code>.Values</code> in the <code>helmfile.yaml</code> as it's not loaded when the file is parsed, and you get an error. A solution would be to layer the helmfile state files but I wasn't able to make it work.</p>"}, {"location": "devops/helmfile/#avoiding-code-repetition", "title": "Avoiding code repetition", "text": "<p>Besides environments, <code>helmfile</code> gives other useful tricks to prevent the illness of code repetition.</p>"}, {"location": "devops/helmfile/#using-release-templates", "title": "Using release templates", "text": "<p>For each chart in a <code>helmfile.yaml</code> we're going to repeat the <code>values</code> and <code>secrets</code> sections, to avoid it, we can use release templates:</p> <pre><code>templates:\ndefault: &amp;default\n# This prevents helmfile exiting when it encounters a missing file\n# Valid values are \"Error\", \"Warn\", \"Info\", \"Debug\". The default is \"Error\"\n# Use \"Debug\" to make missing files errors invisible at the default log level(--log-level=INFO)\nmissingFileHandler: Warn\nvalues:\n- {{`{{ .Release.Name }}`}}/values.yaml\n- {{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}.yaml\nsecrets:\n- config/{{`{{ .Release.Name }}`}}/secrets.yaml\n- config/{{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}-secrets.yaml\n\nreleases:\n- name: chart_1\nchart: stable/chart_1\n&lt;&lt;: *default\n- name: chart_2\nchart: stable/chart_2\n&lt;&lt;: *default\n</code></pre> <p>If you're not familiar with YAML anchors, <code>&amp;default</code> names the block, then <code>*default</code> references it. The <code>&lt;&lt;:</code> syntax says to \"extend\" (merge) that reference into the current tree.</p> <p>The <code>missingFileHandler: Warn</code> field is necessary if you don't need all the values and secret files, but want to use the same definition for all charts.</p> <p><code>{{` {{ .Release.Name }} `}}</code> is surrounded by <code>{{`</code> and <code>}}`</code> so as not to be executed on the loading time of <code>helmfile.yaml</code>. We need to defer it until each release is actually processed by the <code>helmfile</code> command, such as <code>diff</code> or <code>apply</code>.</p> <p>For more information see this issue.</p>"}, {"location": "devops/helmfile/#layering-the-state", "title": "Layering the state", "text": "<p>You may occasionally end up with many helmfiles that shares common parts like which repositories to use, and which release to be bundled by default.</p> <p>Use Layering to extract the common parts into a dedicated library helmfiles, so that each helmfile becomes DRY.</p> <p>Let's assume that your code looks like:</p> <p>File: <code>helmfile.yaml</code></p> <pre><code>bases:\n- environments.yaml\n\nreleases:\n- name: metricbeat\nchart: stable/metricbeat\n- name: myapp\nchart: mychart\n</code></pre> <p>File: <code>environments.yaml</code></p> <pre><code>environments:\ndevelopment:\nproduction:\n</code></pre> <p>At run time, <code>bases</code> in your <code>helmfile.yaml</code> are evaluated to produce:</p> <pre><code>---\n# environments.yaml\nenvironments:\ndevelopment:\nproduction:\n---\n# helmfile.yaml\nreleases:\n- name: myapp\nchart: mychart\n- name: metricbeat\nchart: stable/metricbeat\n</code></pre> <p>Finally the resulting YAML documents are merged in the order of occurrence, so that your <code>helmfile.yaml</code> becomes:</p> <pre><code>environments:\ndevelopment:\nproduction:\n\nreleases:\n- name: metricbeat\nchart: stable/metricbeat\n- name: myapp\nchart: mychart\n</code></pre> <p>Using this concept, we can reuse the environments section as:</p> <p>File: <code>vars/environments.yaml</code></p> <pre><code>environments:\ndefault:\nsecrets:\n- ../vars/staging-secrets.yaml\nvalues:\n- ../vars/staging-values.yaml\nproduction:\nsecrets:\n- ../vars/production-secrets.yaml\nvalues:\n- ../vars/production-values.yaml\n</code></pre> <p>And the default release templates as:</p> <p>File: <code>templates/templates.yaml</code></p> <pre><code>templates:\ndefault: &amp;default\nvalues:\n- {{`{{ .Release.Name }}`}}/values.yaml\n- {{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}.yaml\nsecrets:\n- config/{{`{{ .Release.Name }}`}}/secrets.yaml\n- config/{{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}-secrets.yaml\n</code></pre> <p>So the service's <code>helmfile.yaml</code> turns out to be:</p> <pre><code>bases:\n- ../templates/environments.yaml\n- ../templates/templates.yaml\n\nreleases:\n- name: chart_1\nchart: stable/chart_1\n&lt;&lt;: *default\n- name: chart_2\nchart: stable/chart_2\n&lt;&lt;: *default\n</code></pre> <p>Much shorter and simple.</p>"}, {"location": "devops/helmfile/#managing-dependencies", "title": "Managing dependencies", "text": "<p>Helmfile support concurrency with the option <code>--concurrency=N</code> so we can take advantage of it and improve our deployment speed, but to ensure it works as expected we have to define the dependencies among charts. For example, if an application needs a database, it has to be deployed before hand.</p> <pre><code>releases:\n- name: vpn-dashboard\nchart: incubator/raw\nneeds:\n- monitoring/prometheus-operator\n- name: prometheus-operator\nnamespace: monitoring\nchart: prometheus-community/kube-prometheus-stack\n</code></pre>"}, {"location": "devops/helmfile/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "devops/helmfile/#yaml-templates-in-go-templates", "title": "Yaml templates in go templates", "text": "<p>If you are using a <code>values.yaml.gotmpl</code> file you won't be able to use <code>{{ whatever }}</code>. The solution is to extract that part to a yaml file and include it in the go template. For example:</p> <ul> <li> <p><code>values.yaml.gotmpl</code>:   <pre><code>metrics:\nserviceMonitor:\n  enabled: true\n  annotations:\n  additionalLabels:\n    release: prometheus-operator\n\n{{ readFile \"prometheus_rules.yaml\" }}\n</code></pre></p> </li> <li> <p><code>prometheus_rules.yaml</code></p> </li> </ul> <pre><code>prometheusRule:\nenabled: true\nadditionalLabels:\nrelease: prometheus-operator\nspec:\n- alert: VeleroBackupPartialFailures\nannotations:\nmessage: Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} partialy failed backups.\nexpr: increase(velero_backup_partial_failure_total{schedule!=\"\"}[1h]) &gt; 0\nfor: 15m\nlabels:\nseverity: warning\n</code></pre>"}, {"location": "devops/helmfile/#error-release-name-has-no-deployed-releases", "title": "Error: \"release-name\" has no deployed releases", "text": "<p>This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use <code>helm delete --purge {{ release-name }}</code> and then <code>apply</code> again.</p>"}, {"location": "devops/helmfile/#error-failed-to-download-stablemetrics-server-hint-running-helm-repo-update-may-help", "title": "Error: failed to download \"stable/metrics-server\" (hint: running <code>helm repo update</code> may help)", "text": "<p>I had this issue if <code>verify: true</code> in the helmfile.yaml file. Comment it or set it to false.</p>"}, {"location": "devops/helmfile/#cannot-patch-x-field-is-immutable", "title": "Cannot patch X field is immutable", "text": "<p>You may think that deleting the resource, usually a deployment or daemonset will fix it, but <code>helmfile apply</code> will end without any error, the resource won't be recreated , and if you do a <code>helm list</code>, the deployment will be marked as failed.</p> <p>The solution we've found is disabling the resource in the chart's values so that it's uninstalled an install it again.</p> <p>This can be a problem with the resources that have persistence. To patch it, edit the volume resource with <code>kubectl edit pv -n namespace volume_pvc</code>, change the <code>persistentVolumeReclaimPolicy</code> to <code>Retain</code>, apply the changes to uninstall, and when reinstalling configure the chart to use that volume (easier said than done).</p>"}, {"location": "devops/helmfile/#links", "title": "Links", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/jwt/", "title": "JWT", "text": "<p>JWT (JSON Web Token) is a proposed Internet standard for creating data with optional signature and/or optional encryption whose payload holds JSON that asserts some number of claims. The tokens are signed either using a private secret or a public/private key.</p>"}, {"location": "devops/jwt/#references", "title": "References", "text": "<ul> <li>Hasura.io best practices using jwt</li> </ul>"}, {"location": "devops/markdownlint/", "title": "Markdownlint", "text": "<p>markdownlint-cli is a command line interface for the markdownlint Node.js style checker and lint tool for Markdown/CommonMark files.</p> <p>I've evaluated these other projects (1, 2, but their configuration is less user friendly and are less maintained.</p> <p>You can use this cookiecutter template to create a python project with <code>markdownlint</code> already configured.</p>"}, {"location": "devops/markdownlint/#installation", "title": "Installation", "text": "<pre><code>npm install -g markdownlint-cli\n</code></pre>"}, {"location": "devops/markdownlint/#configuration", "title": "Configuration", "text": "<p>To configure your project, add a <code>.markdownlint.json</code> in your project root directory, or in any parent.  I've opened an issue to see if they are going to support <code>pyproject.toml</code> to save the configuration. Check the styles examples.</p> <p>Go to the rules document if you ever need to check more information on a specific rule.</p> <p>You can use it both with:</p> <ul> <li> <p>The Vim through the ALE plugin.</p> </li> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>- repo: https://github.com/igorshubovych/markdownlint-cli\nrev: v0.23.2\nhooks:\n- id: markdownlint\n</code></pre> </li> </ul>"}, {"location": "devops/markdownlint/#troubleshooting", "title": "Troubleshooting", "text": "<p>Until the #2926 PR is merged you need to change the <code>let l:pattern=.*</code> file to make the linting work to:</p> <p>File: ~/.vim/bundle/ale/autoload/ale/handlers</p> <pre><code>let l:pattern=': \\?\\(\\d*\\):\\? \\(MD\\d\\{3}\\)\\(\\/\\)\\([A-Za-z0-9-\\/]\\+\\)\\(.*\\)$'\n</code></pre>"}, {"location": "devops/markdownlint/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/mypy/", "title": "Mypy", "text": "<p>Mypy is an optional static type checker for Python that aims to combine the benefits of dynamic (or \"duck\") typing and static typing. Mypy combines the expressive power and convenience of Python with a powerful type system and compile-time type checking.</p> <p>You can use this cookiecutter template to create a python project with <code>mypy</code> already configured.</p>"}, {"location": "devops/mypy/#installation", "title": "Installation", "text": "<pre><code>pip install mypy\n</code></pre>"}, {"location": "devops/mypy/#configuration", "title": "Configuration", "text": "<p>Mypy configuration is saved in the <code>mypy.ini</code> file, and they don't yet support <code>pyproject.toml</code>.</p> <p>File: mypy.ini</p> <pre><code>[mypy]\nshow_error_codes = True\nfollow_imports = silent\nstrict_optional = True\nwarn_redundant_casts = True\nwarn_unused_ignores = True\ndisallow_any_generics = True\ncheck_untyped_defs = True\nno_implicit_reexport = True\nwarn_unused_configs = True\ndisallow_subclassing_any = True\ndisallow_incomplete_defs = True\ndisallow_untyped_decorators = True\ndisallow_untyped_calls = True\n\n# for strict mypy: (this is the tricky one :-))\ndisallow_untyped_defs = True\n</code></pre> <p>You can use it both with:</p> <ul> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>repos:\n- repo: https://github.com/pre-commit/mirrors-mypy\nrev: v0.782\nhooks:\n- name: Run mypy static analysis tool\nid: mypy\n</code></pre> </li> <li> <p>Github Actions:</p> <p>File: .github/workflows/lint.yml</p> <pre><code>name: Lint\n\non: [push, pull_request]\n\njobs:\nMypy:\nruns-on: ubuntu-latest\nname: Mypy\nsteps:\n- uses: actions/checkout@v1\n- name: Set up Python 3.7\nuses: actions/setup-python@v1\nwith:\npython-version: 3.7\n- name: Install Dependencies\nrun: pip install mypy\n- name: mypy\nrun: mypy\n</code></pre> </li> </ul> <p>Add <code># type: ignore</code> to the line you want to skip.</p>"}, {"location": "devops/mypy/#ignore-one-line", "title": "Ignore one line", "text": ""}, {"location": "devops/mypy/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "devops/mypy/#module-x-has-no-attribute-y", "title": "Module X has no attribute Y", "text": "<p>If you're importing objects from your own module, you need to tell mypy that those objects are available. To do so in the <code>__init__.py</code> of your module, list them under the <code>__all__</code> variable.</p> <p>File: init.py</p> <pre><code>from .model import Entity\n\n__all__ = [\n    \"Entity\",\n]\n</code></pre>"}, {"location": "devops/mypy/#w0707-consider-explicitly-re-raising-using-the-from", "title": "[W0707: Consider explicitly re-raising using the 'from'", "text": "<p>keyword](https://blog.ram.rachum.com/post/621791438475296768/improving-python-exception-chaining-with)</p> <p>The error can be raised by two cases.</p> <ul> <li>An exception was raised, we were handling it, and something went wrong in the     process of handling it.</li> <li>An exception was raised, and we decided to replace it with a different     exception that will make more sense to whoever called this code.</li> </ul> <pre><code>try:\n  self.connection, _ = self.sock.accept()\nexcept socket.timeout as error:\n  raise IPCException('The socket timed out') from error\n</code></pre> <p>The <code>error</code> bit at the end tells Python: The <code>IPCException</code> that we\u2019re raising is just a friendlier version of the <code>socket.timeout</code> that we just caught.</p> <p>When we run that code and reach that exception, the traceback is going to look like this:</p> <pre><code>Traceback (most recent call last):\n  File \"foo.py\", line 19, in\n    self.connection, _ = self.sock.accept()\n  File \"foo.py\", line 7, in accept\n    raise socket.timeout\nsocket.timeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"foo.py\", line 21, in\n    raise IPCException('The socket timed out') from e\nIPCException: The socket timed out\n</code></pre> <p>The <code>The above exception was the direct cause of the following exception:</code> part tells us that we are in the second case.</p> <p>If you were dealing with the first one, the message between the two tracebacks would be:</p> <pre><code>During handling of the above exception, another exception occurred:\n</code></pre>"}, {"location": "devops/mypy/#module-typing-has-no-attribute-annotated", "title": "Module \"typing\" has no attribute \"Annotated\"", "text": "<p>This one happens only because <code>annotated</code> is not available in python &lt; 3.9.</p> <pre><code>try:\n    # mypy is complaining that it can't import it, but it's solved below\n    from typing import Annotated # type: ignore\nexcept ImportError:\n    from typing_extensions import Annotated\n</code></pre>"}, {"location": "devops/mypy/#issues", "title": "Issues", "text": "<ul> <li>Incompatible return value with     TypeVar: search for <code>10003</code> in     repository-pattern and fix the <code>type: ignore</code>.</li> </ul>"}, {"location": "devops/mypy/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Homepage</li> </ul>"}, {"location": "devops/pip_tools/", "title": "Pip-tools", "text": "<p>Deprecated: Use poetry instead.</p> <p>Pip-tools is a set of command line tools to help you keep your pip-based packages fresh, even when you've pinned them.</p> <p>For stability reasons it's a good idea to hardcode the dependencies versions. Furthermore, safety needs them to work properly.</p> <p>You can use this cookiecutter template to create a python project with <code>pip-tools</code> already configured.</p> <p>We've got three places where the dependencies are defined:</p> <ul> <li><code>setup.py</code> should declare the loosest possible dependency versions that are     still workable. Its job is to say what a particular package can work with.</li> <li><code>requirements.txt</code> is a deployment manifest that defines an entire     installation job, and shouldn't be thought of as tied to any one package.     Its job is to declare an exhaustive list of all the necessary packages to     make a deployment work.</li> <li><code>requirements-dev.txt</code> Adds the dependencies required for the development of     the program.</li> </ul> <p>Content of examples may be outdated</p> <p>An updated version of setup.py and requirements-dev.in can be found in the cookiecutter template.</p> <p>With pip-tools, the dependency management is trivial.</p> <ul> <li> <p>Install the tool:</p> <pre><code>pip install pip-tools\n</code></pre> </li> <li> <p>Set the general dependencies in the <code>setup.py</code> <code>install_requires</code>.</p> </li> <li> <p>Generate the <code>requirements.txt</code> file:</p> <pre><code>pip-compile -U --allow-unsafe`\n</code></pre> <p>The <code>-U</code> flag will try to upgrade the dependencies, and <code>--allow-unsafe</code> will let you manage the <code>setuptools</code> and <code>pip</code> dependencies.</p> </li> <li> <p>Add the additional testing dependencies in the <code>requirements-dev.in</code> file.</p> <p>File: requirements-dev.in</p> <pre><code>-c requirements.txt\npip-tools\nfactory_boy\npytest\npytest-cov\n</code></pre> <p>The <code>-c</code> line will make <code>pip-compile</code> look at that file for compatibility, but it won't duplicate those requirements in the <code>requirements-dev.txt</code>.</p> </li> <li> <p>Compile the development requirements <code>requirements-dev.txt</code> with <code>pip-compile     dev-requirements.in</code>.</p> </li> <li> <p>If you have another <code>requirements.txt</code> for the mkdocs documentation, run     <code>pip-compile docs/requirements.txt</code>.</p> </li> <li> <p>To sync the virtualenv libraries with the     files,     use <code>sync</code>:</p> <pre><code>python -m piptools sync requirements.txt requirements-dev.txt\n</code></pre> </li> <li> <p>To uninstall all pip packages use     <pre><code>pip freeze | xargs pip uninstall -y\n</code></pre></p> </li> </ul> <p>Trigger hooks:</p> <ul> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>  - repo: https://github.com/jazzband/pip-tools\nrev: 5.0.0\nhooks:\n- name: Build requirements.txt\nid: pip-compile\n- name: Build dev-requirements.txt\nid: pip-compile\nargs: ['dev-requirements.in']\n- name: Build mkdocs requirements.txt\nid: pip-compile\nargs: ['docs/requirements.txt']\n</code></pre> <p>pip-tools generates different results in the CI than in the development environment breaking the CI without an easy way to fix it. Therefore it should be run by the developers periodically.</p> </li> </ul>"}, {"location": "devops/pip_tools/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/proselint/", "title": "Proselint", "text": "<p>Proselint is another linter for prose.</p>"}, {"location": "devops/proselint/#installation", "title": "Installation", "text": "<pre><code>pip install proselint\n</code></pre>"}, {"location": "devops/proselint/#configuration", "title": "Configuration", "text": "<p>It can be configured through the <code>~/.config/proselint/config</code> file, such as:</p> <pre><code>{\n\"checks\": {\n\"typography.diacritical_marks\": false\n}\n}\n</code></pre> <ul> <li> <p>The Vim through the ALE plugin.</p> </li> <li> <p>Pre-commit:</p> <pre><code>- repo: https://github.com/amperser/proselint/\nrev: 0.10.2\nhooks:\n- id: proselint\nexclude: LICENSE|requirements\nfiles: \\.(md|mdown|markdown)$\n</code></pre> </li> </ul>"}, {"location": "devops/proselint/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/safety/", "title": "Safety", "text": "<p>Safety checks your installed dependencies for known security vulnerabilities.</p> <p>You can use this cookiecutter template to create a python project with <code>safety</code> already configured.</p>"}, {"location": "devops/safety/#installation", "title": "Installation", "text": "<pre><code>pip install safety\n</code></pre>"}, {"location": "devops/safety/#configuration", "title": "Configuration", "text": "<p>Safety can be used through:</p> <ul> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>repos:\n- repo: https://github.com/Lucas-C/pre-commit-hooks-safety\nrev: v1.1.3\nhooks:\n- id: python-safety-dependencies-check\n</code></pre> </li> <li> <p>Github Actions: Make sure to check that the correct python version is applied.</p> <p>File: .github/workflows/security.yml</p> <pre><code>name: Security\n\non: [push, pull_request]\n\njobs:\nSafety:\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout\nuses: actions/checkout@v2\n- uses: actions/setup-python@v2\nwith:\npython-version: 3.7\n- name: Install dependencies\nrun: pip install safety\n- name: Execute safety\nrun: safety check\n</code></pre> </li> </ul>"}, {"location": "devops/safety/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/write_good/", "title": "Write Good", "text": "<p>write-good is a naive linter for English prose.</p>"}, {"location": "devops/write_good/#installation", "title": "Installation", "text": "<pre><code>npm install -g write-good\n</code></pre> <p>There is no way to configure it through a configuration file, but it accepts command line arguments.</p> <p>The ALE vim implementation supports the specification of such flags with the <code>ale_writegood_options</code> variable:</p> <pre><code>let g:ale_writegood_options = \"--no-passive\"\n</code></pre> <p>Use <code>write-good --help</code> to see the available flags.</p> <p>As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter.</p>"}, {"location": "devops/write_good/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/yamllint/", "title": "Yamllint", "text": "<p>Yamllint is a linter for YAML files.</p> <p><code>yamllint</code> does not only check for syntax validity, but for weirdnesses like key repetition and cosmetic problems such as lines length, trailing spaces or indentation.</p> <p>You can use it both with:</p> <ul> <li> <p>The Vim through the ALE plugin.</p> </li> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>- repo: https://github.com/adrienverge/yamllint\nrev: v1.21.0\nhooks:\n- id: yamllint\n</code></pre> </li> </ul>"}, {"location": "devops/yamllint/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "devops/aws/aws/", "title": "AWS", "text": "<p>Amazon Web Services (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis. In aggregate, these cloud computing web services provide a set of primitive abstract technical infrastructure and distributed computing building blocks and tools.</p>", "tags": ["WIP"]}, {"location": "devops/aws/aws/#learn-path", "title": "Learn path", "text": "<p>TBD</p>", "tags": ["WIP"]}, {"location": "devops/aws/eks/", "title": "EKS", "text": "<p>Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that makes it easy for you to run Kubernetes on AWS without needing to stand up or maintain your own Kubernetes control plane.</p>"}, {"location": "devops/aws/eks/#pod-limit-per-node", "title": "Pod limit per node", "text": "<p>AWS EKS supports native VPC networking with the Amazon VPC Container Network Interface (CNI) plugin for Kubernetes. Using this plugin allows Kubernetes Pods to have the same IP address inside the pod as they do on the VPC network.</p> <p>This is a great feature but it introduces a limitation in the number of Pods per EC2 Node instance. Whenever you deploy a Pod in the EKS worker Node, EKS creates a new IP address from VPC subnet and attach to the instance.</p> <p>The formula for defining the maximum number of pods per instance is as follows:</p> <pre><code>N * (M-1) + 2\n</code></pre> <p>Where:</p> <ul> <li><code>N</code> is the number of Elastic Network Interfaces (ENI) of the instance type.</li> <li><code>M</code> is the number of IP addresses of a single ENI.</li> </ul> <p>So, for <code>t3.small</code>, this calculation is <code>3 * (4-1) + 2 = 11</code>. For a list of all the instance types and their limits see this document</p>"}, {"location": "devops/aws/eks/#upgrade-an-eks-cluster", "title": "Upgrade an EKS cluster", "text": "<p>New Kubernetes versions introduce significant changes, so it's recommended that you test the behavior of your applications against a new Kubernetes version before performing the update on your production clusters.</p> <p>The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that they are working as expected. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications are not affected, and your cluster is never left in a non-deterministic or unrecoverable state. Amazon EKS regularly backs up all managed clusters, and mechanisms exist to recover clusters if necessary. We are constantly evaluating and improving our Kubernetes infrastructure management processes.</p> <p>To upgrade a cluster follow these steps:</p> <ul> <li>Upgrade all your charts to the latest version with helmfile.     <pre><code>helmfile deps\nhelmfile apply\n</code></pre></li> <li>Check your current version and compare it with the one you want to upgrade.     <pre><code>kubectl version --short\nkubectl get nodes\n</code></pre></li> <li>Check the     docs     to see if the version you want to upgrade requires some special steps.</li> <li>If your worker nodes aren't at the same version as the cluster control plane     upgrade them to the control plane version (never higher).</li> <li> <p>Edit the <code>cluster_version</code> attribute of the eks terraform module and apply the     changes (reviewing them first).     <pre><code>terraform apply\n</code></pre></p> <p>This is a long step (approximately 40 minutes) * Upgrade your charts again.</p> </li> </ul>"}, {"location": "devops/aws/eks/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "devops/aws/s3/", "title": "S3", "text": "<p>S3 is the secure, durable, and scalable object storage infrastructure of AWS. Often used for serving static website content or holding backups or data.</p>"}, {"location": "devops/aws/s3/#commands", "title": "Commands", "text": ""}, {"location": "devops/aws/s3/#bucket-management", "title": "Bucket management", "text": ""}, {"location": "devops/aws/s3/#list-buckets", "title": "List buckets", "text": "<pre><code>aws s3 ls\n</code></pre>"}, {"location": "devops/aws/s3/#create-bucket", "title": "Create bucket", "text": "<pre><code>aws s3api create-bucket \\\n--bucket {{ bucket_name }} \\\n--create-bucket-configuration LocationConstraint=us-east-1\n</code></pre>"}, {"location": "devops/aws/s3/#enable-versioning", "title": "Enable versioning", "text": "<pre><code>aws s3api put-bucket-versioning --bucket {{ bucket_name }} --versioning-configuration Status=Enabled\n</code></pre>"}, {"location": "devops/aws/s3/#enable-encryption", "title": "Enable encryption", "text": "<pre><code>aws s3api put-bucket-encryption --bucket {{ bucket_name }} \\\n--server-side-encryption-configuration='{\n  \"Rules\":\n  [\n    {\n      \"ApplyServerSideEncryptionByDefault\":\n      {\n        \"SSEAlgorithm\": \"AES256\"\n      }\n    }\n  ]\n}'\n</code></pre>"}, {"location": "devops/aws/s3/#download-bucket", "title": "Download bucket", "text": "<pre><code>aws s3 cp --recursive s3://{{ bucket_name }} .\n</code></pre>"}, {"location": "devops/aws/s3/#audit-the-s3-bucket-policy", "title": "Audit the S3 bucket policy", "text": "<pre><code>IFS=$(echo -en \"\\n\\b\")\nfor bucket in `aws s3 ls | awk '{ print $3 }'`\ndo echo \"Bucket $bucket:\"\naws s3api get-bucket-acl --bucket \"$bucket\"\ndone\n</code></pre>"}, {"location": "devops/aws/s3/#add-cache-header-to-all-items-in-a-bucket", "title": "Add cache header to all items in a bucket", "text": "<ul> <li>Log in to AWS Management Console.</li> <li>Go into S3 bucket.</li> <li>Select all files by route.</li> <li>Choose \"More\" from the menu.</li> <li>Select \"Change metadata\".</li> <li>In the \"Key\" field, select \"Cache-Control\" from the drop down menu     max-age=604800Enter (7 days in seconds) for Value.</li> <li>Press \"Save\" button.</li> </ul>"}, {"location": "devops/aws/s3/#object-management", "title": "Object management", "text": ""}, {"location": "devops/aws/s3/#remove-an-object", "title": "Remove an object", "text": "<pre><code>aws s3 rm s3://{{ bucket_name }}/{{ path_to_file }}\n</code></pre>"}, {"location": "devops/aws/s3/#upload", "title": "Upload", "text": ""}, {"location": "devops/aws/s3/#upload-a-local-file-with-the-cli", "title": "Upload a local file with the cli", "text": "<pre><code>aws s3 cp {{ path_to_file }} s3://{{ bucket_name }}/{{ upload_path }}\n</code></pre>"}, {"location": "devops/aws/s3/#upload-a-file-unauthenticated", "title": "Upload a file unauthenticated", "text": "<pre><code>curl --request PUT --upload-file test.txt https://{{ bucket_name }}.s3.amazonaws.com/uploads/\n</code></pre>"}, {"location": "devops/aws/s3/#restore-an-object", "title": "Restore an object", "text": "<p>First you need to get the version of the object</p> <pre><code>aws s3api list-object-versions \\\n--bucket {{ bucket_name }} \\\n--prefix {{ bucket_path_to_file }}\n</code></pre> <p>Fetch the <code>VersionId</code> and download the file</p> <pre><code>aws s3api get-object \\\n--bucket {{ bucket_name }} \\\n--key {{ bucket_path_to_file }} \\\n--version-id {{ versionid }}\n</code></pre> <p>Once you have it, overwrite the same object in the same path</p> <pre><code>aws s3 cp \\\n{{ local_path_to_restored_file }} \\\ns3://{{ bucket_name }}/{{ upload_path }}\n</code></pre>"}, {"location": "devops/aws/s3/#copy-objects-between-buckets", "title": "Copy objects between buckets", "text": "<pre><code>aws s3 sync s3://SOURCE_BUCKET_NAME s3://NEW_BUCKET_NAME\n</code></pre>"}, {"location": "devops/aws/s3/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "devops/aws/s3/#get_environ_proxies-missing-1-required-positional-argument-no_proxy", "title": "get_environ_proxies() missing 1 required positional argument: 'no_proxy'", "text": "<pre><code>sudo pip3 install --upgrade boto3\n</code></pre>"}, {"location": "devops/aws/s3/#links", "title": "Links", "text": "<ul> <li>User guide</li> </ul>"}, {"location": "devops/aws/security_groups/", "title": "Security groups", "text": "<p>Security groups are the AWS way of defining firewall rules between the resources. If not handled properly they can soon become hard to read, which can lead to an insecure infrastructure.</p> <p>It has helped me to use four types of security groups:</p> <ul> <li>Default security groups: Security groups created by AWS per VPC and region,     they can't be deleted.</li> <li>Naming security groups: Used to identify an aws resource. They are usually     referenced in other security groups.</li> <li>Ingress security groups: Used to define the rules of ingress traffic to the     resource.</li> <li>Egress security groups: Used to define the rules of egress traffic to the     resource.</li> </ul> <p>But what helped most has been using clinv while refactoring all the security groups.</p> <p>With <code>clinv unused</code> I got rid of all the security groups that weren't used by any AWS resource (beware of #16, 17, #18 and #19), then used the <code>clinv unassigned security_groups</code> to methodically decide if they were correct and add them to my inventory or if I needed to refactor them.</p>"}, {"location": "devops/aws/security_groups/#best-practices", "title": "Best practices", "text": "<ul> <li>Follow a naming convention.</li> <li>Avoid as much as you can the use of CIDRs in the definition of security     groups. Instead, use naming security groups as much as you can. This will     probably mean that you'll need to create security rules for each service     that is going to use the security group. It is cumbersome but from     a security point of view we gain traceability.</li> <li>Follow the principle of least privileges. Open the least number of ports     required for the service to work.</li> <li>Reuse existing security groups. If there is a security group for web servers     that uses port 80, don't create the new service using port 8080.</li> <li>Remove all rules from the default security groups and don't use them.</li> <li>Don't define the rules in the <code>aws_security_group</code> terraform resource. Use     <code>aws_security_group_rules</code> for each security group to avoid creation     dependency loops.</li> <li>Add descriptions to each security group and security group rule.</li> <li>Avoid using port ranges in the security group rule definitions, as you     probably won't need them.</li> </ul>"}, {"location": "devops/aws/security_groups/#naming-convention", "title": "Naming convention", "text": "<p>A naming convention is required once the number of security groups starts to grow, both to understand them and to be able to search in them.</p> <p>Note</p> <p>It is assumed that terraform is used to create the resources</p>"}, {"location": "devops/aws/security_groups/#default-security-groups", "title": "Default security groups", "text": "<p>There are going to be two kinds of default security groups:</p> <ul> <li>VPC default security groups.</li> <li>Region default security groups.</li> </ul> <p>For the first one we'll use:</p> <pre><code>resource \"aws_default_security_group\" \"{{ region_id }}_{{ vpc_friendly_identifier }}\" {\nvpc_id = \"{{ vpc_id }}\"\n}\n</code></pre> <p>Where:</p> <ul> <li><code>region_id</code> is the region identifier with underscores, for example <code>us_east_1</code></li> <li><code>vpc_friendly_identifier</code> is a human understandable identifier, such as     <code>publicdmz</code>.</li> <li><code>vpc_id</code> is the VPC id such as <code>vpc-xxxxxxxxxxxxxxxxx</code>.</li> </ul> <p>For the second one:</p> <pre><code>resource \"aws_default_security_group\" \"{{ region_id }}\" {\nprovider = aws.{{ region_id }}\n}\n\nWhere the provider must be configured in the `terraform_config.tf` file, for\nexample:\n\n```terraform\nprovider \"aws\" {\nalias  = \"us_west_2\"\nregion = \"us-west-2\"\n}\n</code></pre>"}, {"location": "devops/aws/security_groups/#naming-security-groups", "title": "Naming security groups", "text": "<p>For the naming security groups I've created an UltiSnips template.</p> <pre><code>snippet naming \"naming security group rule\" b\nresource \"aws_security_group\" \"${1:resource_name}_${2:resource_type}\" {\nname        = \"$1-$2\"\ndescription = \"Identify the $1 $2.\"\nvpc_id      = data.terraform_remote_state.vpc.outputs.vpc_${3:vpc}_id\ntags = {\nName = \"$1 $2\"\n}\n}\n\noutput \"$1_$2_id\" {\nvalue = aws_security_group.$1_$2.id\n}\n$0\nendsnippet\n</code></pre> <p>Where:</p> <ul> <li><code>instance_name</code> is a human friendly identifier of the resource that the     security group is going to identify, for example <code>gitea</code>, <code>ci</code> or <code>bastion</code>.</li> <li><code>resource_type</code> identifies the type of resource, such as <code>instance</code> for EC2,     or <code>load_balancer</code> for ELBs.</li> <li><code>vpc</code>: is a human friendly identifier of the vpc. It has to match the outputs     of the vpc resources, as it's going to be also used in the definition of the     vpc used by the security group.</li> </ul> <p>Once you've finished defining the security group, move the <code>output</code> resource to the <code>outputs.tf</code> file.</p>"}, {"location": "devops/aws/security_groups/#ingress-security-groups", "title": "Ingress security groups", "text": "<p>For the ingress security groups I've created another UltiSnips template.</p> <pre><code>snippet ingress \"ingress security group rule\" b\nresource \"aws_security_group\" \"ingress_${1:protocol}_from_${2:destination}_at_${3:vpc}\" {\nname        = \"ingress-$1-from-$2-at-$3\"\ndescription = \"Allow the ingress of $1 traffic from the $2 instances at $3\"\nvpc_id      = data.terraform_remote_state.vpc.outputs.vpc_$3_id\ntags = {\nName = \"Ingress $1 from $2 at $3\"\n}\n}\n\nresource \"aws_security_group_rule\" \"ingress_$1_from_$2_at_$3\" {\ntype              = \"ingress\"\nfrom_port         = ${4:port}\nto_port           = ${5:$4}\nprotocol          = \"${6:tcp}\"\nsecurity_group_id = aws_security_group.ingress_$1_from_$2_at_$3.id\nsource_security_group_id = aws_security_group.$7.id\ndescription      = \"Allow the ingress $1 traffic on port $4 from the $2 instances at $3.\"\n}\n\noutput \"ingress_$1_from_$2_at_$3_id\" {\nvalue = aws_security_group.ingress_$1_from_$2_at_$3.id\n}\n$0\nendsnippet\n</code></pre> <p>Where:</p> <ul> <li><code>protocol</code> is a human friendly identifier of what kind of traffic the security     group is going to allow, for example <code>gitea</code>, <code>ssh</code>, <code>proxy</code> or <code>openvpn</code>.</li> <li><code>destination</code> identifies the resources that are going to use the security     group, for example <code>drone_instance</code>, <code>ldap_instance</code> or <code>everywhere</code>.</li> <li><code>vpc</code>: is a human friendly identifier of the vpc. It has to match the outputs     of the vpc resources, as it's going to be also used in the definition of the     vpc used by the security group.</li> <li><code>port</code> is the port we are going to open.</li> <li><code>$6</code> we assume that the <code>to_port</code> is the same as <code>from_port</code>.</li> <li> <p><code>$7</code> ID of the naming security group that will have access to the particular     security group rule. If you need to use CIDRs for the rule definition,     change that line for the following:</p> <pre><code>cidr_blocks       = [\"{{ cidr }}\"]\n</code></pre> </li> </ul> <p>Once you've finished defining the security group, move the <code>output</code> resource to the <code>outputs.tf</code> file.</p>"}, {"location": "devops/aws/security_groups/#egress-security-groups", "title": "Egress security groups", "text": "<p>For the egress security groups I've created another UltiSnips template.</p> <pre><code>snippet egress \"egress security group rule\" b\nresource \"aws_security_group\" \"egress_${1:protocol}_to_${2:destination}_from_${3:vpc}\" {\nname        = \"egress-$1-to-$2-from-$3\"\ndescription = \"Allow the egress of $1 traffic to the $2 instances at $3\"\nvpc_id      = data.terraform_remote_state.vpc.outputs.vpc_$3_id\ntags = {\nName = \"Egress $1 to $2 at $3\"\n}\n}\n\nresource \"aws_security_group_rule\" \"egress_$1_to_$2_from_$3\" {\ntype              = \"egress\"\nfrom_port         = ${4:port}\nto_port           = ${5:$4}\nprotocol          = \"${6:tcp}\"\nsecurity_group_id = aws_security_group.egress_$1_to_$2_from_$3.id\nsource_security_group_id = aws_security_group.$7.id\ndescription      = \"Allow the egress of $1 traffic on port $4 to the $2 instances at $3.\"\n}\n\noutput \"egress_$1_to_$2_from_$3_id\" {\nvalue = aws_security_group.egress_$1_to_$2_from_$3.id\n}\n$0\nendsnippet\n</code></pre> <p>Where:</p> <ul> <li><code>protocol</code> is a human friendly identifier of what kind of traffic the security     group is going to allow, for example <code>gitea</code>, <code>ssh</code>, <code>proxy</code> or <code>openvpn</code>.</li> <li><code>destination</code> identifies the resources that are going to be accessed by the security     group, for example <code>drone_instance</code>, <code>ldap_instance</code> or <code>everywhere</code>.</li> <li><code>vpc</code>: is a human friendly identifier of the vpc. It has to match the outputs     of the vpc resources, as it's going to be also used in the definition of the     vpc used by the security group.</li> <li><code>port</code> is the port we are going to open.</li> <li><code>$6</code> we assume that the <code>to_port</code> is the same as <code>from_port</code>.</li> <li> <p><code>$7</code> ID of the naming security group that will be accessed by the particular     security group rule. If you need to use CIDRs for the rule definition,     change that line for the following:</p> <pre><code>cidr_blocks       = [\"{{ cidr }}\"]\n</code></pre> </li> </ul> <p>Once you've finished defining the security group, move the <code>output</code> resource to the <code>outputs.tf</code> file.</p>"}, {"location": "devops/aws/security_groups/#instance-security-group-definition", "title": "Instance security group definition", "text": "<p>When defining the security groups in the <code>aws_instance</code> resources, define them in this order:</p> <ul> <li>Naming security groups.</li> <li>Ingress security groups.</li> <li>Egress security groups.</li> </ul> <p>For example</p> <pre><code>resource \"aws_instance\" \"gitea_production\" {\nami               = ...\navailability_zone = ...\nsubnet_id         = ...\nvpc_security_group_ids = [\ndata.terraform_remote_state.security_groups.outputs.gitea_instance_id,\ndata.terraform_remote_state.security_groups.outputs.ingress_http_from_gitea_loadbalancer_at_publicdmz_id,\ndata.terraform_remote_state.security_groups.outputs.ingress_http_from_monitoring_at_privatedmz_id,\ndata.terraform_remote_state.security_groups.outputs.ingress_administration_from_bastion_at_connectiondmz_id,\ndata.terraform_remote_state.security_groups.outputs.egress_ldap_to_ldap_instance_from_publicdmz_id,\ndata.terraform_remote_state.security_groups.outputs.egress_https_to_debian_repositories_from_publicdmz_id,\n]\n</code></pre>"}, {"location": "devops/aws/iam/iam/", "title": "IAM", "text": "<p>AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. You use IAM to control who can use your AWS resources (authentication) and what resources they can use and in what ways (authorization).</p> <p>Configurable AWS access controls:</p> <ul> <li>Grant access to AWS Management console, APIs</li> <li>Create individual users</li> <li>Manage permissions with groups</li> <li>Configure a strong password policy</li> <li>Enable Multi-Factor Authentication for privileged users</li> <li>Use IAM roles for EC2 instances</li> <li>Use IAM roles to share access</li> <li>Rotate security credentials regularly</li> <li>Restrict privileged access further with conditions</li> <li>Use your corporate directory system or a third party authentication</li> </ul>"}, {"location": "devops/aws/iam/iam/#links", "title": "Links", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "devops/aws/iam/iam_commands/", "title": "IAM Commands", "text": ""}, {"location": "devops/aws/iam/iam_commands/#information-gathering", "title": "Information gathering", "text": ""}, {"location": "devops/aws/iam/iam_commands/#list-roles", "title": "List roles", "text": "<pre><code>aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table\n</code></pre>"}, {"location": "devops/aws/iam/iam_commands/#list-policies", "title": "List policies", "text": "<pre><code>aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table\n</code></pre>"}, {"location": "devops/aws/iam/iam_commands/#list-attached-policies", "title": "List attached policies", "text": "<pre><code>aws iam list-attached-role-policies --role-name {{ role_name }}\n</code></pre>"}, {"location": "devops/aws/iam/iam_commands/#get-role-configuration", "title": "Get role configuration", "text": "<pre><code>aws iam get-role --role-name {{ role_name }}\n</code></pre>"}, {"location": "devops/aws/iam/iam_commands/#get-role-policies", "title": "Get role policies", "text": "<pre><code>aws iam list-role-policies --role-name {{ role_name }}\n</code></pre>"}, {"location": "devops/aws/iam/iam_debug/", "title": "Problems encountered with AWS IAM", "text": ""}, {"location": "devops/aws/iam/iam_debug/#mfadevice-entity-at-the-same-path-and-name-already-exists", "title": "MFADevice entity at the same path and name already exists", "text": "<p>It happens when a user receives an error while creating her MFA authentication.</p> <p>To solve it:</p> <p>List the existing MFA physical or virtual devices</p> <pre><code>aws iam list-mfa-devices\naws iam list-virtual-mfa-devices\n</code></pre> <p>Delete the conflictive one</p> <pre><code>aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}\n</code></pre>"}, {"location": "devops/helm/helm/", "title": "Helm", "text": "<p>Helm is the package manager for Kubernetes. Through charts it helps you define, install and upgrade even the most complex Kubernetes applications.</p> <p> </p> <p>The advantages of using helm over <code>kubectl apply</code> are the easiness of:</p> <ul> <li>Repeatable application installation.</li> <li>CI integration.</li> <li>Versioning and sharing.</li> </ul> <p>Charts are a group of Go templates of kubernetes yaml resource manifests, they are easy to create, version, share, and publish.</p> <p>Helm alone lacks some features, that are satisfied through some external programs:</p> <ul> <li>Helmfile is used to declaratively configure your charts, so   they can be versioned through git.</li> <li>Helm-secrets is used to remove hardcoded credentials from <code>values.yaml</code>   files. Helm has an open issue to   integrate it into it's codebase.</li> <li>Helm-git is used to install helm charts directly from Git     repositories.</li> </ul>"}, {"location": "devops/helm/helm/#links", "title": "Links", "text": "<ul> <li>Homepage</li> <li>Docs</li> <li>Git</li> <li>Chart hub</li> <li>Git charts repositories</li> </ul>"}, {"location": "devops/helm/helm_commands/", "title": "Helm Commands", "text": "<p>Small cheatsheet on how to use the <code>helm</code> command.</p>"}, {"location": "devops/helm/helm_commands/#list-charts", "title": "List charts", "text": "<pre><code>helm ls\n</code></pre>"}, {"location": "devops/helm/helm_commands/#get-information-of-chart", "title": "Get information of chart", "text": "<pre><code>helm inspect {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#list-all-the-available-versions-of-a-chart", "title": "List all the available versions of a chart", "text": "<pre><code>helm search -l {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#download-a-chart", "title": "Download a chart", "text": "<pre><code>helm fetch {{ package_name }}\n</code></pre> <p>Download and extract <pre><code>helm fetch --untar {{ package_name }}\n</code></pre></p>"}, {"location": "devops/helm/helm_commands/#search-charts", "title": "Search charts", "text": "<pre><code>helm search {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#operations-you-should-do-with-helmfile", "title": "Operations you should do with helmfile", "text": "<p>The following operations can be done with helm, but consider using helmfile instead.</p>"}, {"location": "devops/helm/helm_commands/#install-chart", "title": "Install chart", "text": "<p>Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state.</p> <pre><code>helm install {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#give-it-a-name", "title": "Give it a name", "text": "<pre><code>helm install --name {{ release_name }} {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#give-it-a-namespace", "title": "Give it a namespace", "text": "<pre><code>helm install --namespace {{ namespace }} {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#customize-the-chart-before-installing", "title": "Customize the chart before installing", "text": "<pre><code>helm inspect values {{ package_name }} &gt; values.yml\n</code></pre> <p>Edit the <code>values.yml</code> <pre><code>helm install -f values.yml {{ package_name }}\n</code></pre></p>"}, {"location": "devops/helm/helm_commands/#upgrade-a-release", "title": "Upgrade a release", "text": "<p>If a new version of the chart is released or you want to change the configuration use</p> <pre><code>helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#rollback-an-upgrade", "title": "Rollback an upgrade", "text": "<p>First check the revisions <pre><code>helm history {{ release_name }}\n</code></pre></p> <p>Then rollback <pre><code>helm rollback {{ release_name }} {{ revision }}\n</code></pre></p>"}, {"location": "devops/helm/helm_commands/#delete-a-release", "title": "Delete a release", "text": "<pre><code>helm delete --purge {{ release_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#working-with-repositories", "title": "Working with repositories", "text": ""}, {"location": "devops/helm/helm_commands/#list-repositories", "title": "List repositories", "text": "<pre><code>helm repo list\n</code></pre>"}, {"location": "devops/helm/helm_commands/#add-repository", "title": "Add repository", "text": "<pre><code>helm repo add {{ repo_name }} {{ repo_url }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#update-repositories", "title": "Update repositories", "text": "<pre><code>helm repo update\n</code></pre>"}, {"location": "devops/helm/helm_installation/", "title": "Helm Installation", "text": "<p>There are two usable versions of Helm, v2 and v3, the latter is quite new so some of the things we need to install as of 2020-01-27 are not yet supported (Prometheus operator), so we are going to stick to the version 2.</p> <p>Helm has a client-server architecture, the server is installed in the Kubernetes cluster and the client is a Go executable installed in the user computer.</p>"}, {"location": "devops/helm/helm_installation/#helm-client", "title": "Helm client", "text": "<p>You'll first need to configure kubectl.</p> <p>The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository, and get the download link of the <code>tar.gz</code>.</p> <pre><code>wget {{ url_to_tar_gz }} -O helm.tar.gz\ntar -xvf helm.tar.gz\nmv linux-amd64/helm ~/.local/bin/\n</code></pre> <p>We are going to use some plugins inside Helmfile, so install them with:</p> <pre><code>helm plugin install https://github.com/jkroepke/helm-secrets\nhelm plugin install https://github.com/databus23/helm-diff\n</code></pre> <p>Now that you've got Helm installed, you'll probably want to install Helmfile.</p>"}, {"location": "devops/helm/helm_secrets/", "title": "Helm Secrets", "text": "<p>Helm-secrets is a helm plugin that manages secrets with Git workflow and stores them anywhere. It delegates the cryptographic operations to Mozilla's Sops tool, which supports PGP, AWS KMS and GCP KMS.</p> <p>The configuration is stored in <code>.sops.yaml</code> files. You can find in Mozilla's documentation a detailed configuration guide. For my use case, I'm only going to use a list of PGP keys, so the following contents should be in the <code>.sops.yaml</code> file at the project root directory.</p> <pre><code>creation_rules:\n- pgp: &gt;-\n{{ gpg_key_1 }},\n{{ gpg_key_2}}\n</code></pre>"}, {"location": "devops/helm/helm_secrets/#installation", "title": "Installation", "text": "<p>Weirdly, <code>helm plugin install https://github.com/jkroepke/helm-secrets --version v3.9.1</code> asks for your github user :S so I'd rather install it by hand.</p> <pre><code>wget https://github.com/jkroepke/helm-secrets/releases/download/v3.9.1/helm-secrets.tar.gz\ntar xvzf helm-secrets.tar.gz -C \"$(helm env HELM_PLUGINS)\"\nrm helm-secrets.tar.gz\n</code></pre> <p>If you're going to use GPG as backend you need to install <code>sops</code>. It's in your distribution repositories, but probably not in the latest version, therefore I suggest you install the binary directly:</p> <ul> <li>Grab the latest release</li> <li>Download, <code>chmod +x</code> and move it somewhere in your <code>$PATH</code>.</li> </ul>"}, {"location": "devops/helm/helm_secrets/#prevent-committing-decrypted-files-to-git", "title": "Prevent committing decrypted files to git", "text": "<p>From the docs:</p> <p>If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore.</p> <p>A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook.</p> <p>This will prevent committing decrypted files without sops metadata.</p> <p>.sopscommithook content example:</p> <pre><code>#!/bin/sh\n\nfor FILE in $(git diff-index HEAD --name-only | grep &lt;your vars dir&gt; | grep \"secrets.y\"); do\nif [ -f \"$FILE\" ] &amp;&amp; ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then\necho \"!!!!! $FILE\" 'File is not encrypted !!!!!'\necho \"Run: helm secrets enc &lt;file path&gt;\"\nexit 1\nfi\ndone\nexit\n</code></pre>"}, {"location": "devops/helm/helm_secrets/#usage", "title": "Usage", "text": ""}, {"location": "devops/helm/helm_secrets/#encrypt-secret-files", "title": "Encrypt secret files", "text": "<p>Imagine you've got a <code>values.yaml</code> with the following information: <pre><code>grafana:\nenabled: true\nadminPassword: admin\n</code></pre></p> <p>If you want to encrypt <code>adminPassword</code>, remove that line from the <code>values.yaml</code> and create a <code>secrets.yaml</code> file with: <pre><code>grafana:\nadminPassword: supersecretpassword\n</code></pre></p> <p>And encrypt the file. <pre><code>helm secrets enc secrets.yaml\n</code></pre></p> <p>If you use Helmfile, you'll need to add the secrets file to your helmfile.yaml. <pre><code>  values:\n- values.yaml\nsecrets:\n- secrets.yaml\n</code></pre></p> <p>From that point on, <code>helmfile</code> will automatically decrypt the credentials.</p>"}, {"location": "devops/helm/helm_secrets/#edit-secret-files", "title": "Edit secret files", "text": "<pre><code>helm secrets edit secrets.yaml\n</code></pre>"}, {"location": "devops/helm/helm_secrets/#decrypt-secret-files", "title": "Decrypt secret files", "text": "<pre><code>helm secrets dec secrets.yaml\n</code></pre> <p>It will generate a <code>secrets.yaml.dec</code> file that it's not decrypted.</p> <p>Be careful not to add these files to git.</p>"}, {"location": "devops/helm/helm_secrets/#clean-all-the-decrypted-files", "title": "Clean all the decrypted files", "text": "<pre><code>helm secrets clean .\n</code></pre>"}, {"location": "devops/helm/helm_secrets/#add-or-remove-keys", "title": "Add or remove keys", "text": "<p>If you want to add or remove PGP keys from <code>.sops.yaml</code>, you need to execute <code>sops updatekeys -y</code> for each <code>secrets.yaml</code> file in the repository. <code>helm-secrets</code> won't make this process easier for you.</p> <p>Check sops documentation for more options.</p>"}, {"location": "devops/helm/helm_secrets/#links", "title": "Links", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/kong/kong/", "title": "Kong", "text": "<p>Kong is a lua application API platform running in Nginx.</p>"}, {"location": "devops/kong/kong/#installation", "title": "Installation", "text": "<p>Kong supports several platforms of which we'll use Kubernetes with the helm chart, as it gives the following advantages:</p> <ul> <li>Kong is configured dynamically and responds to the changes in your     infrastructure.</li> <li>Kong is deployed onto Kubernetes with a Controller, which is responsible for     configuring Kong.</li> <li>All of Kong\u2019s configuration is done using Kubernetes resources, stored in     Kubernetes\u2019 data-store (etcd).</li> <li>Use the power of kubectl (or any custom tooling around kubectl) to configure     Kong and get benefits of all Kubernetes, such as declarative configuration,     cloud-provider agnostic deployments, RBAC, reconciliation of desired state,     and elastic scalability.</li> <li>Kong is configured using a combination of Ingress Resource and Custom Resource     Definitions(CRDs).</li> <li>DB-less by default, meaning Kong has the capability of running without     a database and using only memory storage for entities.</li> </ul> <p>In the <code>helmfile.yaml</code> add the repository and the release:</p> <pre><code>repositories:\n- name: kong\nurl: https://charts.konghq.com\nreleases:\n- name: kong\nnamespace: api-manager\nchart: kong/kong\nvalues:\n- kong/values.yaml\nsecrets:\n- kong/secrets.yaml\n</code></pre> <p>While particularizing the <code>values.yaml</code> keep in mind that:</p> <ul> <li>If you don't want the ingress controller set up <code>ingressController.enabled:     false</code>, and in <code>proxy</code> set <code>service: ClusterIP</code> and <code>ingress.enabled:     true</code>.</li> <li>Kong can be run with or without a database. By default the chart installs it     without database.</li> <li> <p>If you deploy it without database and without the ingress controller, you have     to provide a declarative configuration for Kong to run. It can be provided     using an existing ConfigMap <code>dblessConfig.configMap</code> or the whole     configuration can be put into the <code>values.yaml</code> file for deployment itself,     under the <code>dblessConfig.config</code> parameter.</p> </li> <li> <p>Although kong supports it's own Kubernetes resources     (CRD)     for     plugins     and     consumers,     I've found now way of integrating them into the helm chart, therefore I'm     going to specify everything in the <code>dblessConfig.config</code>.</p> </li> </ul> <p>So the general kong configuration <code>values.yaml</code> would be:</p> <pre><code>dblessConfig:\nconfig:\n_format_version: \"1.1\"\nservices:\n- name: example.com\nurl: https://api.example.com\nplugins:\n- name: key-auth\n- name: rate-limiting\nconfig:\nsecond: 10\nhour: 1000\npolicy: local\nroutes:\n- name: example\npaths:\n- /example\n</code></pre> <p>And the <code>secrets.yaml</code>:</p> <pre><code>consumers:\n- username: lyz\nkeyauth_credentials:\n- key: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4\n</code></pre> <p>To test that everything works use</p> <pre><code>curl -I https://api.example.com/example -H 'apikey: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4'\n</code></pre> <p>To add the prometheus monitorization, enable the <code>serviceMonitor.enabled: true</code> and make sure you set the correct labels. There is a grafana official dashboard you can also use.</p>"}, {"location": "devops/kong/kong/#links", "title": "Links", "text": "<ul> <li>Homepage</li> <li>Docs</li> </ul>"}, {"location": "devops/kubectl/kubectl/", "title": "Kubectl", "text": "<p>Kubectl Definition</p> <p>Kubectl is a command line tool for controlling Kubernetes clusters.</p> <p><code>kubectl</code> looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the --kubeconfig flag.</p>"}, {"location": "devops/kubectl/kubectl/#resource-types-and-its-aliases", "title": "Resource types and it's aliases", "text": "Resource Name Short Name clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies netpol nodes no statefulsets sts persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources"}, {"location": "devops/kubectl/kubectl/#usage", "title": "Usage", "text": ""}, {"location": "devops/kubectl/kubectl/#port-forward-tunnel-to-an-internal-service", "title": "Port forward / Tunnel to an internal service", "text": "<p>If you have a service running in kubernetes and you want to directly access it instead of going through the usual path, you can use <code>kubectl port-forward</code>.</p> <p><code>kubectl port-forward</code> allows using resource name, such as a pod name, service replica set or deployment, to select the matching resource to port forward to. For example, the next commands are equivalent:</p> <pre><code>kubectl port-forward mongo-75f59d57f4-4nd6q 28015:27017\nkubectl port-forward deployment/mongo 28015:27017\nkubectl port-forward replicaset/mongo-75f59d57f4 28015:27017\nkubectl port-forward service/mongo 28015:27017\n</code></pre> <p>The output is similar to this:</p> <pre><code>Forwarding from 127.0.0.1:28015 -&gt; 27017\nForwarding from [::1]:28015 -&gt; 27017\n</code></pre> <p>If you don't need a specific local port, you can let <code>kubectl</code> choose and allocate the local port and thus relieve you from having to manage local port conflicts, with the slightly simpler syntax:</p> <pre><code>$: kubectl port-forward deployment/mongo :27017\n\nForwarding from 127.0.0.1:63753 -&gt; 27017\nForwarding from [::1]:63753 -&gt; 27017\n</code></pre>"}, {"location": "devops/kubectl/kubectl/#run-a-command-against-a-specific-context", "title": "Run a command against a specific context", "text": "<p>If you have multiple contexts and you want to be able to run commands against a context that you have access to but is not your active context you can use the <code>--context</code> global option for all <code>kubectl</code> commands:</p> <pre><code>kubectl get pods --context &lt;context_B&gt;\n</code></pre> <p>To get a list of available contexts use <code>kubectl config get-contexts</code></p>"}, {"location": "devops/kubectl/kubectl/#links", "title": "Links", "text": "<ul> <li>Overview.</li> <li>Cheatsheet.</li> <li>Kbenv: Virtualenv for kubectl.</li> </ul>"}, {"location": "devops/kubectl/kubectl_commands/", "title": "Kubectl Commands", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#configuration-and-context", "title": "Configuration and context", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#add-a-new-cluster-to-your-kubeconf-that-supports-basic-auth", "title": "Add a new cluster to your kubeconf that supports basic auth", "text": "<pre><code>kubectl config set-credentials {{ username }}/{{ cluster_dns }} --username={{ username }} --password={{ password }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#create-new-context", "title": "Create new context", "text": "<pre><code>kubectl config set-context {{ context_name }} --user={{ username }} --namespace={{ namespace }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-current-context", "title": "Get current context", "text": "<pre><code>kubectl config current-context\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#list-contexts", "title": "List contexts", "text": "<pre><code>kubectl config get-contexts\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#switch-context", "title": "Switch context", "text": "<pre><code>kubectl config use-context {{ context_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#creating-objects", "title": "Creating objects", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#create-resource", "title": "Create Resource", "text": "<pre><code>kubectl create -f {{ resource.definition.yaml | dir.where.yamls.live | url.to.yaml }} --record\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#create-a-configmap-from-a-file", "title": "Create a configmap from a file", "text": "<pre><code>kubectl create configmap {{ configmap_name }} --from-file {{ path/to/file }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#deleting-resources", "title": "Deleting resources", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#delete-the-pod-using-the-type-and-name-specified-in-a-file", "title": "Delete the pod using the type and name specified in a file", "text": "<pre><code>kubectl delete -f {{ path_to_file }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#delete-pods-and-services-by-name", "title": "Delete pods and services by name", "text": "<pre><code>kubectl delete pod,service {{ pod_names }} {{ service_names }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#delete-pods-and-services-by-label", "title": "Delete pods and services by label", "text": "<pre><code>kubectl delete pod,services -l {{ label_name }}={{ label_value }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#delete-all-pods-and-services-in-namespace", "title": "Delete all pods and services in namespace", "text": "<pre><code>kubectl -n {{ namespace_name }} delete po,svc --all\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#delete-all-evicted-pods", "title": "Delete all evicted pods", "text": "<pre><code>while read i; do kubectl delete pod \"$i\"; done &lt; &lt;(kubectl get pods | grep -i evicted | sed 's/ .*//g')\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#editing-resources", "title": "Editing resources", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#edit-a-service", "title": "Edit a service", "text": "<pre><code>kubectl edit svc/{{ service_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#information-gathering", "title": "Information gathering", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#get-credentials", "title": "Get credentials", "text": "<p>Get credentials <pre><code>kubectl config view --minify\n</code></pre></p>"}, {"location": "devops/kubectl/kubectl_commands/#deployments", "title": "Deployments", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#restart-pods-without-taking-the-service-down", "title": "Restart pods without taking the service down", "text": "<pre><code>kubectl rollout deployment {{ deployment_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#view-status-of-deployments", "title": "View status of deployments", "text": "<pre><code>kubectl get deployments\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#describe-deployments", "title": "Describe Deployments", "text": "<pre><code>kubectl describe deployment {{ deployment_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-images-of-deployment", "title": "Get images of deployment", "text": "<pre><code>kubectl get pods --selector=app={{ deployment_name }} -o json |\\\njq '.items[] | .metadata.name + \": \" + .spec.containers[0].image'\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#nodes", "title": "Nodes", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#list-all-nodes", "title": "List all nodes", "text": "<pre><code>kubectl get nodes\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#check-which-nodes-are-ready", "title": "Check which nodes are ready", "text": "<pre><code>JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\\n&amp;&amp; kubectl get nodes -o jsonpath=$JSONPATH | grep \"Ready=True\"\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#external-ips-of-all-nodes", "title": "External IPs of all nodes", "text": "<pre><code>kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}'\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-exposed-ports-of-node", "title": "Get exposed ports of node", "text": "<pre><code>export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#pods", "title": "Pods", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#list-all-pods-in-the-current-namespace", "title": "List all pods in the current namespace", "text": "<pre><code>kubectl get pods\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#list-all-pods-in-all-namespaces", "title": "List all pods in all namespaces", "text": "<pre><code>kubectl get pods --all-namespaces\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#list-all-pods-of-a-selected-namespace", "title": "List all pods of a selected namespace", "text": "<pre><code>kubectl get pods -n {{ namespace }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#list-with-more-detail", "title": "List with more detail", "text": "<pre><code>kubectl get pods -o wide\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-pods-of-a-selected-deployment", "title": "Get pods of a selected deployment", "text": "<pre><code>kubectl get pods --selector=\"name={{ name }}\"\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-pods-of-a-given-label", "title": "Get pods of a given label", "text": "<pre><code>kubectl get pods -l {{ label_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-pods-by-ip", "title": "Get pods by IP", "text": "<pre><code>kubectl get pods -o wide\nNAME                               READY     STATUS    RESTARTS   AGE       IP            NODE\nalpine-3835730047-ggn2v            1/1       Running   0          5d        10.22.19.69   ip-10-35-80-221.ec2.internal\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#sort-pods-by-restart-count", "title": "Sort pods by restart count", "text": "<pre><code>kubectl get pods --sort-by='.status.containerStatuses[0].restartCount'\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#describe-pods", "title": "Describe Pods", "text": "<pre><code>kubectl describe pods {{ pod_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-name-of-pod", "title": "Get name of pod", "text": "<pre><code>pod=$(kubectl get pod --selector={{ selector_label }}={{ selector_value }} -o jsonpath={.items..metadata.name})\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#list-pods-that-belong-to-a-particular-rc", "title": "List pods that belong to a particular RC", "text": "<pre><code>sel=${$(kubectl get rc my-rc --output=json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"')%?}\necho $(kubectl get pods --selector=$sel --output=jsonpath={.items..metadata.name})\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#show-the-remaining-space-of-a-persistent-volume-claim", "title": "Show the remaining space of a persistent volume claim", "text": "<p>Either look it in Prometheus or run in the pod that has the PVC mounted:</p> <pre><code>kubectl -n &lt;namespace&gt; exec &lt;pod-name&gt; -- df -ah\n</code></pre> <p>You may need to use <code>kubectl get pod &lt;pod-name&gt; -o yaml</code> to know what volume is mounted where.</p>"}, {"location": "devops/kubectl/kubectl_commands/#services", "title": "Services", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#list-services-in-namespace", "title": "List services in namespace", "text": "<pre><code>kubectl get services\n</code></pre> <p>List services sorted by name <pre><code>kubectl get services --sort-by=.metadata.name\n</code></pre></p>"}, {"location": "devops/kubectl/kubectl_commands/#describe-services", "title": "Describe Services", "text": "<pre><code>kubectl describe services {{ service_name }}\nkubectl describe svc {{ service_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#replication-controller", "title": "Replication controller", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#list-all-replication-controller", "title": "List all replication controller", "text": "<pre><code>kubectl get rc\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#secrets", "title": "Secrets", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#view-status-of-secrets", "title": "View status of secrets", "text": "<pre><code>kubectl get secrets\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#namespaces", "title": "Namespaces", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#view-namespaces", "title": "View namespaces", "text": "<pre><code>kubectl get namespaces\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#limits", "title": "Limits", "text": "<pre><code>kubectl get limitrange\n</code></pre> <pre><code>kubectl describe limitrange limits\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#jobs-and-cronjobs", "title": "Jobs and cronjobs", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#get-cronjobs-of-a-namespace", "title": "Get cronjobs of a namespace", "text": "<pre><code>kubectl get cronjobs -n {{ namespace }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-jobs-of-a-namespace", "title": "Get jobs of a namespace", "text": "<pre><code>kubectl get jobs -n {{ namespace }}\n</code></pre> <p>You can then describe a specific job to get the pod it created.</p> <pre><code>kubectl describe job -n {{ namespace }} {{ job_name }}\n</code></pre> <p>And now you can see the evolution of the job with:</p> <pre><code>kubectl logs -n {{ namespace }} {{ pod_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#interacting-with-nodes-and-cluster", "title": "Interacting with nodes and cluster", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#mark-node-as-unschedulable", "title": "Mark node as unschedulable", "text": "<pre><code>kubectl cordon {{ node_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#mark-node-as-schedulable", "title": "Mark node as schedulable", "text": "<pre><code>kubectl uncordon {{ node_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#drain-node-in-preparation-for-maintenance", "title": "Drain node in preparation for maintenance", "text": "<pre><code>kubectl drain {{ node_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#show-metrics-of-all-node", "title": "Show metrics of all node", "text": "<pre><code>kubectl top node\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#show-metrics-of-a-node", "title": "Show metrics of a node", "text": "<pre><code>kubectl top node {{ node_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#display-addresses-of-the-master-and-servies", "title": "Display addresses of the master and servies", "text": "<pre><code>kubectl cluster-info\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#dump-current-cluster-state-to-stdout", "title": "Dump current cluster state to stdout", "text": "<pre><code>kubectl cluster-info dump\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#dump-current-cluster-state-to-directory", "title": "Dump current cluster state to directory", "text": "<pre><code>kubectl cluster-info dump --output-directory={{ path_to_directory }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#interacting-with-pods", "title": "Interacting with pods", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#dump-logs-of-pod", "title": "Dump logs of pod", "text": "<pre><code>kubectl logs {{ pod_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#dump-logs-of-pod-and-specified-container", "title": "Dump logs of pod and specified container", "text": "<pre><code>kubectl logs {{ pod_name }} -c {{ container_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#stream-logs-of-pod", "title": "Stream logs of pod", "text": "<pre><code>kubectl logs -f {{ pod_name }}\nkubectl logs -f {{ pod_name }} -c {{ container_name }}\n</code></pre> <p>Another option is to use the kubetail program.</p>"}, {"location": "devops/kubectl/kubectl_commands/#attach-to-running-container", "title": "Attach to running container", "text": "<pre><code>kubectl attach {{ pod_name }} -i\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-a-shell-of-a-running-container", "title": "Get a shell of a running container", "text": "<pre><code>kubectl exec {{ pod_name }} -it bash\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-a-debian-container-inside-kubernetes", "title": "Get a debian container inside kubernetes", "text": "<pre><code>kubectl run --generator=run-pod/v1 -i --tty debian --image=debian -- bash\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-a-root-shell-of-a-running-container", "title": "Get a root shell of a running container", "text": "<ol> <li> <p>Get the Node where the pod is and the docker ID <pre><code>kubectl describe pod {{ pod_name }}\n</code></pre></p> </li> <li> <p>SSH into the node <pre><code>ssh {{ node }}\n</code></pre></p> </li> <li> <p>Get into docker <pre><code>docker exec -it -u root {{ docker_id }} bash\n</code></pre></p> </li> </ol>"}, {"location": "devops/kubectl/kubectl_commands/#forward-port-of-pod-to-your-local-machine", "title": "Forward port of pod to your local machine", "text": "<pre><code>kubectl port-forward {{ pod_name }} {{ pod_port }}:{{ local_port }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#expose-port", "title": "Expose port", "text": "<pre><code>kubectl expose {{ deployment_name }} --type=\"{{ expose_type }}\" --port {{ port_number }}\n</code></pre> <p>Where expose_type is one of: ['NodePort', 'ClusterIP', 'LoadBalancer', 'externalName']</p>"}, {"location": "devops/kubectl/kubectl_commands/#run-command-on-existing-pod", "title": "Run command on existing pod", "text": "<pre><code>kubectl exec {{ pod_name }} -- ls /\nkubectl exec {{ pod_name }} -c {{ container_name }} -- ls /\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#show-metrics-for-a-given-pod-and-its-containers", "title": "Show metrics for a given pod and it's containers", "text": "<pre><code>kubectl top pod {{ pod_name }} --containers\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#extract-file-from-pod", "title": "Extract file from pod", "text": "<pre><code>kubectl cp {{ container_id }}:{{ path_to_file }} {{ path_to_local_file }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#scaling-resources", "title": "Scaling resources", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#scale-a-deployment-with-a-specified-size", "title": "Scale a deployment with a specified size", "text": "<pre><code>kubectl scale deployment {{ deployment_name }} --replicas {{ replicas_number }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#scale-a-replicaset", "title": "Scale a replicaset", "text": "<pre><code>kubectl scale --replicas={{ replicas_number }} rs/{{ replicaset_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#scale-a-resource-specified-in-a-file", "title": "Scale a resource specified in a file", "text": "<pre><code>kubectl scale --replicas={{ replicas_number }} -f {{ path_to_yaml }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#updating-resources", "title": "Updating resources", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#namespaces_1", "title": "Namespaces", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#temporary-set-the-namespace-for-a-request", "title": "Temporary set the namespace for a request", "text": "<pre><code>kubectl -n {{ namespace_name }} {{ command_to_execute }}\nkubectl --namespace={{ namespace_name }} {{ command_to_execute }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#permanently-set-the-namespace-for-a-request", "title": "Permanently set the namespace for a request", "text": "<pre><code>kubectl config set-context $(kubectl config current-context) --namespace={{ namespace_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#deployment", "title": "Deployment", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#modify-the-image-of-a-deployment", "title": "Modify the image of a deployment", "text": "<p><pre><code>kubectl set image {{ deployment_name }} {{ label }}:{{ label_value }}\n</code></pre> for example <pre><code>kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1\n</code></pre></p> <p>Or edit it by hand <pre><code>kubectl edit {{ deployment_name }}\n</code></pre></p>"}, {"location": "devops/kubectl/kubectl_commands/#get-the-status-of-the-rolling-update", "title": "Get the status of the rolling update", "text": "<pre><code>kubectl rollout status {{ deployment_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-the-history-of-the-deployment", "title": "Get the history of the deployment", "text": "<pre><code>kubectl rollout history deployment {{ deployment_name }}\n</code></pre> <p>To get more details of a selected revision: <pre><code>kubectl rollout history deployment {{ deployment_name }} --revision={{ revision_number }}\n</code></pre></p>"}, {"location": "devops/kubectl/kubectl_commands/#get-back-to-a-specified-revision", "title": "Get back to a specified revision", "text": "<p>To get to the last version <pre><code>kubectl rollout undo deployment {{ deployment_name }}\n</code></pre></p> <p>To go to a specific version <pre><code>kubectl rollout undo {{ deployment_name }} --to-revision={{ revision_number }}\n</code></pre></p>"}, {"location": "devops/kubectl/kubectl_commands/#pods_1", "title": "Pods", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#rolling-update-of-pods", "title": "Rolling update of pods", "text": "<p>Is prefered to use the deployment rollout</p> <pre><code>kubectl rolling-update {{ pod_name }} -f {{ new_pod_definition_yaml }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#change-the-name-of-the-resource-and-update-the-image", "title": "Change the name of the resource and update the image", "text": "<pre><code>kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --image=image:{{ new_pod_version }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#abort-existing-rollout-in-progress", "title": "Abort existing rollout in progress", "text": "<pre><code>kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --rollback\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#force-replace-delete-and-then-re-create-the-resource", "title": "Force replace, delete and then re-create the resource", "text": "<p>** Will cause a service outage **</p> <pre><code>kubectl replace --force -f {{ new_pod_yaml }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#add-a-label", "title": "Add a label", "text": "<pre><code>kubectl label pods {{ pod_name }} new-label={{ new_label }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#autoscale-a-deployment", "title": "Autoscale a deployment", "text": "<pre><code>kubectl autoscale deployment {{ deployment_name }} --min={{ min_instances }} --max={{ max_instances }} [--cpu-percent={{ cpu_percent }}]\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#copy-resources-between-namespaces", "title": "Copy resources between namespaces", "text": "<pre><code>kubectl get rs,secrets -o json --namespace old | jq '.items[].metadata.namespace = \"new\"' | kubectl create-f -\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#formatting-output", "title": "Formatting output", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#print-a-table-using-a-comma-separated-list-of-custom-columns", "title": "Print a table using a comma separated list of custom columns", "text": "<pre><code>-o=custom-columns=&lt;spec&gt;\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#print-a-table-using-the-custom-columns-template-in-the-file", "title": "Print a table using the custom columns template in the  file <pre><code>-o=custom-columns-file=&lt;filename&gt;\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#output-a-json-formatted-api-object", "title": "Output a JSON formatted API object <pre><code>-o=json\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#print-the-fields-defined-in-a-jsonpath-expression", "title": "Print the fields defined in a jsonpath expression <pre><code>-o=jsonpath=&lt;template&gt;\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#print-the-fields-defined-by-the-jsonpath-expression-in-the-file", "title": "Print the fields defined by the jsonpath expression in the  file <pre><code>-o=jsonpath-file=&lt;filename&gt;\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#print-only-the-resource-name-and-nothing-else", "title": "Print only the resource name and nothing else <pre><code>-o=name\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#output-in-the-plain-text-format-with-any-additional-information-and-for-pods-the-node-name-is-included", "title": "Output in the plain-text format with any additional information, and for pods, the node name is included <pre><code>-o=wide\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#output-a-yaml-formatted-api-object", "title": "Output a YAML formatted API object <pre><code>-o=yaml\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_installation/", "title": "Kubectl Installation", "text": "<p>Kubectl is available in the distribution package managers, </p> <pre><code>sudo apt-get install kubernetes-client\n</code></pre> <p>If you want the latest version you can install it manually.</p> <pre><code>curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(\\\ncurl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt\\\n)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nmv kubectl ~/.local/bin/kubectl\n</code></pre>"}, {"location": "devops/kubectl/kubectl_installation/#configure-kubectl", "title": "Configure kubectl", "text": ""}, {"location": "devops/kubectl/kubectl_installation/#set-editor", "title": "Set editor", "text": "<pre><code># File ~/.bashrc\nKUBE_EDITOR=\"vim\"\n</code></pre>"}, {"location": "devops/kubectl/kubectl_installation/#set-auto-completion", "title": "Set auto completion", "text": "<pre><code># File ~/.bashrc\nsource &lt;(kubectl completion bash)\n</code></pre>"}, {"location": "devops/kubectl/kubectl_installation/#configure-eks-cluster", "title": "Configure EKS cluster", "text": "<p>To configure the access to an existing cluster, we'll let aws-cli create the required files:</p> <pre><code>aws eks update-kubeconfig --name {{ cluster_name }}\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes/", "title": "Introduction to Kubernetes", "text": "<p>Kubernetes (commonly stylized as k8s) is an open-source container-orchestration system for automating application deployment, scaling, and management. Developed by Google in Go under the Apache 2.0 license, it was first released on June 7, 2014 reaching 1.0 by July 21, 2015. It works with a range of container tools, including Docker. Many cloud services offer a Kubernetes-based platform or infrastructure as a service (PaaS or IaaS) on which Kubernetes can be deployed as a platform-providing service.  Many vendors also provide their own branded Kubernetes distributions.</p> <p> </p> <p>It has become the standard infrastructure to manage containers in production environments. Docker Swarm would be an alternative but it falls short in features compared with Kubernetes.</p> <p>These are some of the advantages of using Kubernetes:</p> <ul> <li>Widely used in production and actively developed.</li> <li>Ensure high availability of your services with autohealing and autoscaling.</li> <li>Easy, quickly and predictable deployment and promotion of applications.</li> <li>Seamless roll out of features.</li> <li>Optimize hardware use while guaranteeing resource isolation.</li> <li>Easiest way to build multi-cloud and baremetal environments.</li> </ul> <p>Several companies have used Kubernetes to release their own PaaS:</p> <ul> <li>OpenShift by Red Hat.</li> <li>Tectonic by CoreOS.</li> <li>Rancher labs by Rancher.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes/#learn-roadmap", "title": "Learn roadmap", "text": "<p>K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily.</p> <p>This is how I learnt, but probably there are better resources now:</p> <ul> <li>Read containing container chaos kubernetes.</li> <li>Test the katacoda lab.</li> <li>Install Kubernetes in laptop with   minikube.</li> <li>Read K8s concepts.</li> <li>Then K8s tasks.</li> <li>I didn't like the book Getting started with kubernetes</li> <li>I'd personally avoid the book Getting started with   kubernetes,   I didn't like it <code>\u00af\\(\u00b0_o)/\u00af</code>.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes/#tools-to-test", "title": "Tools to test", "text": "<ul> <li> <p>Velero: To backup and migrate Kubernetes resources and     persistent volumes.</p> </li> <li> <p>Popeye is a utility that scans live     Kubernetes cluster and reports potential issues with deployed resources and     configurations. It sanitizes your cluster based on what's deployed and not     what's sitting on disk. By scanning your cluster, it detects     misconfigurations and helps you to ensure that best practices are in place,     thus preventing future headaches. It aims at reducing the cognitive overload     one faces when operating a Kubernetes cluster in the wild. Furthermore, if     your cluster employs a metric-server, it reports potential resources     over/under allocations and attempts to warn you should your cluster run out     of capacity.</p> <p>Popeye is a readonly tool, it does not alter any of your Kubernetes resources in any way!</p> </li> <li> <p>Stern allows you to tail multiple pods on     Kubernetes and multiple containers within the pod. Each result is color     coded for quicker debugging.</p> <p>The query is a regular expression so the pod name can easily be filtered and you don't need to specify the exact id (for instance omitting the deployment id). If a pod is deleted it gets removed from tail and if a new pod is added it automatically gets tailed.</p> <p>When a pod contains multiple containers Stern can tail all of them too without having to do this manually for each one. Simply specify the container flag to limit what containers to show. By default all containers are listened to.</p> </li> <li> <p>Fairwinds' Polaris keeps your     clusters sailing smoothly. It runs a variety of checks to ensure that     Kubernetes pods and controllers are configured using best practices, helping     you avoid problems in the future.</p> </li> <li> <p>kube-hunter hunts for security     weaknesses in Kubernetes clusters. The tool was developed to increase     awareness and visibility for security issues in Kubernetes environments.</p> </li> </ul>"}, {"location": "devops/kubernetes/kubernetes/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Awesome K8s</li> <li>Katacoda playground</li> <li>Comic</li> </ul>"}, {"location": "devops/kubernetes/kubernetes/#diving-deeper", "title": "Diving deeper", "text": "<ul> <li>Architecture</li> <li>Resources</li> <li>Kubectl</li> <li>Additional Components</li> <li>Networking</li> <li>Helm</li> <li>Tools</li> <li>Debugging</li> </ul>"}, {"location": "devops/kubernetes/kubernetes/#reference", "title": "Reference", "text": "<ul> <li>References</li> <li>API conventions</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_annotations/", "title": "Kubernetes annotations", "text": "<p>Annotations are non-identifying metadata key/value pairs attached to objects, such as pods. Annotations are intended to give meaningful and relevant information to libraries and tools.</p> <p>Annotations, like labels, are key/value maps:</p> <pre><code>\"annotations\": {\n\"key1\" : \"value1\",\n\"key2\" : \"value2\"\n}\n</code></pre> <p>Here are some examples of information that could be recorded in annotations:</p> <ul> <li>Fields managed by a declarative configuration layer. Attaching these fields   as annotations distinguishes them from default values set by clients or   servers, and from auto generated fields and fields set by auto sizing or   auto scaling systems.</li> <li>Build, release, or image information like timestamps, release IDs, git   branch, PR numbers, image hashes, and registry address.</li> <li>Pointers to logging, monitoring, analytics, or audit repositories.</li> <li>Client library or tool information that can be used for debugging purposes,   for example, name, version, and build information.</li> <li>User or tool/system provenance information, such as URLs of related objects   from other ecosystem components.</li> <li>Lightweight rollout tool metadata: for example, config or checkpoints.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_architecture/", "title": "Kubernetes architecture", "text": "<p>Kubernetes is a combination of components distributed between two kind of nodes, Masters and Workers.</p> <p></p>"}, {"location": "devops/kubernetes/kubernetes_architecture/#master-nodes", "title": "Master Nodes", "text": "<p>Master nodes or Kubernetes Control Plane are the controlling unit for the cluster. They manage scheduling of new containers, configure networking and provide health information.</p> <p>To do so it uses:</p> <ul> <li> <p>kube-api-server   exposes the Kubernetes control plane API validating and configuring data for   the different API objects. It's used by all the components to interact between   themselves.</p> </li> <li> <p>etcd is a \"Distributed   reliable key-value store for the most critical data of a distributed system\".   Kubernetes uses Etcd to store state about the cluster and service discovery   between nodes. This state includes what nodes exist in the cluster, which   nodes they are running on and what containers should be running.</p> </li> <li> <p>kube-scheduler   watches for newly created pods with no assigned node, and selects a node for   them to run on.</p> <p>Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines.</p> </li> <li> <p>kube-controller-manager   runs the following controllers:</p> <ul> <li>Node Controller: Responsible for noticing and responding when nodes go   down.</li> <li>Replication Controller: Responsible for maintaining the correct number   of pods for every replication controller object in the system.</li> <li>Endpoints Controller: Populates the Endpoints object (that is, joins   Services &amp; Pods).</li> <li>Service Account &amp; Token Controllers: Create default accounts and API   access tokens for new namespaces.</li> </ul> </li> <li> <p>cloud-controller-manager   runs controllers that interact with the underlying cloud providers.</p> <ul> <li>Node Controller: For checking the cloud provider to determine if a node   has been deleted in the cloud after it stops responding.</li> <li>Route Controller: For setting up routes in the underlying cloud   infrastructure.</li> <li>Service Controller: For creating, updating and deleting cloud provider   load balancers.</li> <li>Volume Controller: For creating, attaching, and mounting volumes, and   interacting with the cloud provider to orchestrate volumes.</li> </ul> </li> </ul>"}, {"location": "devops/kubernetes/kubernetes_architecture/#worker-nodes", "title": "Worker Nodes", "text": "<p>Worker nodes are the units that hold the application data of the cluster. There can be different types of nodes: CPU architecture, amount of resources (CPU, RAM, GPU) or cloud provider.</p> <p>Each node has the services necessary to run pods:</p> <ul> <li>Container   Runtime:   The software responsible for running containers (Docker, rkt, containerd,   CRI-O).</li> <li>kubelet: The primary \u201cnode   agent\u201d. It works in terms of a PodSpec (a YAML or JSON object that describes   it). <code>kubelet</code> takes a set of PodSpecs from the masters <code>kube-api-server</code> and   ensures that the containers described are running and healthy.</li> <li> <p>kube-proxy is   the network proxy that runs on each node. This reflects services as defined in   the Kubernetes API on each node and can do simple TCP and UDP stream   forwarding or round robin across a set of backends.</p> <p><code>kube-proxy</code> maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.</p> <p><code>kube-proxy</code> uses the operating system packet filtering layer if there is one and it's available. Otherwise, it will forward the traffic itself.</p> </li> </ul>"}, {"location": "devops/kubernetes/kubernetes_architecture/#kube-proxy-operation-modes", "title": "kube-proxy operation modes", "text": "<p><code>kube-proxy</code> currently supports three different operation modes:</p> <ul> <li>User space: This mode gets its name because the service routing takes place in   kube-proxy in the user process space instead of in the kernel network stack.   It is not commonly used as it is slow and outdated.</li> <li>iptables: This mode uses Linux kernel-level Netfilter rules to configure all   routing for Kubernetes Services. This mode is the default for kube-proxy on   most platforms. When load balancing for multiple backend pods, it uses   unweighted round-robin scheduling.</li> <li>IPVS (IP Virtual Server): Built on the Netfilter framework, IPVS implements   Layer-4 load balancing in the Linux kernel, supporting multiple load-balancing   algorithms, including least connections and shortest expected delay. This   kube-proxy mode became generally available in Kubernetes 1.11, but it requires   the Linux kernel to have the IPVS modules loaded. It is also not as widely   supported by various Kubernetes networking projects as the iptables mode.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_architecture/#kubectl", "title": "Kubectl", "text": "<p>The kubectl is the command line client used to communicate with the Masters.</p>"}, {"location": "devops/kubernetes/kubernetes_architecture/#number-of-clusters", "title": "Number of clusters", "text": "<p>You can run a given set of workloads either on few large clusters (with many workloads in each cluster) or on many clusters (with few workloads in each cluster).</p> <p>Here's a table that summarizes the pros and cons of various approaches:</p> <p> Figure: Possibilities of number of clusters from learnk8s.io article</p> <p>Reference to the original article for a full read (it's really good!). I'm going to analyze only the Large shared cluster and Cluster per environment options, as they are the closest to my use case.</p>"}, {"location": "devops/kubernetes/kubernetes_architecture/#one-large-shared-cluster", "title": "One Large shared cluster", "text": "<p>With this option, you run all your workloads in the same cluster. Kubernetes provides namespaces to logically separate portions of a cluster from each other, and in the above case, you could use a separate namespace for each application instance.</p> <p>Pros:</p> <ul> <li> <p>Efficient resource usage: You need to have only one copy of all the     resources that are needed to run and manage a Kubernetes cluster (master     nodes, load balancers, Ingress controllers, authentication, logging, and     monitoring).</p> </li> <li> <p>Cheap: As you avoid the duplication of resources, you require less hardware.</p> </li> <li> <p>Efficient administration: Administering a Kubernetes cluster requires:</p> <ul> <li>Upgrading the Kubernetes version</li> <li>Setting up a CI/CD pipeline</li> <li>Installing a CNI plugin</li> <li>Setting up the user authentication system</li> <li>Installing an admission controller</li> </ul> <p>If you have only a single cluster, you need to do all of this only once.</p> <p>If you have many clusters, then you need to apply everything multiple times, which probably requires you to develop some automated processes and tools for being able to do this consistently.</p> </li> </ul> <p>Cons:</p> <ul> <li> <p>Single point of failure: If you have only one cluster and if that cluster     breaks, then all your workloads are down.</p> <p>There are many ways that something can go wrong:</p> <ul> <li>A Kubernetes upgrade produces unexpected side effects.</li> <li>An cluster-wide component (such as a CNI plugin) doesn't work as expected.</li> <li>An erroneous configuration is made to one of the cluster components.</li> <li>An outage occurs in the underlying infrastructure.</li> </ul> <p>A single incident like this can produce major damage across all your workloads if you have only a single shared cluster.</p> </li> <li> <p>No hard security isolation: If multiple apps run in the same Kubernetes     cluster, this means that these apps share the hardware, network, and     operating system on the nodes of the cluster.</p> <p>Concretely, two containers of two different apps running on the same node are technically two processes running on the same hardware and operating system kernel.</p> <p>Linux containers provide some form of isolation, but this isolation is not as strong as the one provided by, for example, virtual machines (VMs). Under the hood, a process in a container is still just a process running on the host's operating system.</p> <p>This may be an issue from a security point of view \u2014 it theoretically allows unrelated apps to interact with each other in undesired ways (intentionally and unintentionally).</p> <p>Furthermore, all the workloads in a Kubernetes cluster share certain cluster-wide services, such as DNS \u2014 this allows apps to discover the Services of other apps in the cluster.</p> <p>It's important to keep in mind that Kubernetes is designed for sharing, and not for isolation and security.</p> </li> <li> <p>No hard multi-tenancy: Given the many shared resources in a Kubernetes     cluster, there are many ways that different apps can \"step on each other's     toes\".</p> <p>For example, an app may monopolize a certain shared resource, such as the CPU or memory, and thus starve other apps running on the same node.</p> <p>Kubernetes provides various ways to control this behaviour, however it's not trivial to tweak these tools in exactly the right way, and they cannot prevent every unwanted side effect either.</p> </li> <li> <p>Many users: If you have only a single cluster, then many people in your     organisation must have access to this cluster.</p> <p>The more people have access to a system, the higher the risk that they break something.</p> <p>Within the cluster, you can control who can do what with role-based access control (RBAC) \u2014 however, this still can't prevent that users break something within their area of authorisation.</p> </li> </ul>"}, {"location": "devops/kubernetes/kubernetes_architecture/#cluster-per-environment", "title": "Cluster per environment", "text": "<p>With this option you have a separate cluster for each environment.</p> <p>For example, you can have a <code>dev</code>, <code>test</code>, and <code>prod</code> cluster where you run all the application instances of a specific environment.</p> <ul> <li> <p>Isolation of the *prod environment*: In general, this approach isolates all     the environments from each other \u2014 but, in practice, this especially matters     for the prod environment.</p> <p>The production versions of your app are now not affected by whatever happens in any of the other clusters and application environments.</p> <p>So, if some misconfiguration creates havoc in your dev cluster, the prod versions of your apps keep running as if nothing had happened.</p> </li> <li> <p>Cluster can be customised for an environment:</p> <p>You can optimise each cluster for its environment \u2014 for example, you can:</p> <pre><code>* Install development and debugging tools in the dev cluster.\n* Install testing frameworks and tools in the test cluster.\n* Use more powerful hardware and network connections for the prod\ncluster.\n</code></pre> <p>This may improve the efficiency of both the development and operation of your apps. * Lock down access to prod cluster: Nobody really needs to do work on the prod cluster, so you can make access to it very restrictive.</p> </li> </ul> <p>Cons:</p> <ul> <li> <p>More administration and resources: In comparison with the single cluster.</p> </li> <li> <p>Lack of isolation between apps: The main disadvantage of this approach is     the missing hardware and resource isolation between apps.</p> <p>Unrelated apps share cluster resources, such as the operating system kernel, CPU, memory, and several other services.</p> <p>As already mentioned, this may be a security issue.</p> </li> <li> <p>App requirements are not localised: If an app has special requirements, then     these requirements must be satisfied in all clusters.</p> </li> </ul>"}, {"location": "devops/kubernetes/kubernetes_architecture/#conclusion", "title": "Conclusion", "text": "<p>If you don't need environment isolation and are not afraid of upgrading the Kubernetes cluster, go for the single cluster, otherwise use a cluster per environment.</p>"}, {"location": "devops/kubernetes/kubernetes_architecture/#references", "title": "References", "text": "<ul> <li>Kubernetes components overview</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_cluster_autoscaler/", "title": "Kubernetes cluster autoscaler", "text": "<p>While Horizontal pod autoscaling allows a deployment to scale given the resources needed, they are limited to the kubernetes existing working nodes.</p> <p>To autoscale the number of working nodes we need the cluster autoscaler.</p> <p>For AWS, there are the Amazon guidelines to enable it. But I'd use the <code>cluster-autoscaler</code> helm chart.</p>"}, {"location": "devops/kubernetes/kubernetes_dashboard/", "title": "Kubernetes Dashboard", "text": "<p>Dashboard definition</p> <p>Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard.</p> <p>Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred.</p> <p></p>"}, {"location": "devops/kubernetes/kubernetes_dashboard/#deployment", "title": "Deployment", "text": "<p>The best way to install it is with the stable/kubernetes-dashboard chart with helmfile.</p>"}, {"location": "devops/kubernetes/kubernetes_dashboard/#links", "title": "Links", "text": "<ul> <li>Git</li> <li>Documentation</li> <li>Kubernetes introduction to the dashboard</li> <li>Hasham Haider guide</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_deployments/", "title": "Kubernetes Deployments", "text": "<p>The different types of deployments configure a ReplicaSet and a PodSchema for your application.</p> <p>Depending on the type of application we'll use one of the following types.</p>"}, {"location": "devops/kubernetes/kubernetes_deployments/#deployments", "title": "Deployments", "text": "<p>Deployments are the controller for stateless applications, therefore it favors availability over consistency.</p> <p>It provides availability by creating multiple copies of the same pod. Those pods are disposable\u200a\u2014\u200aif they become unhealthy, Deployment will just create new replacements. What\u2019s more, you can update Deployment at a controlled rate, without a service outage. When an incident like the AWS outage happens, your workloads will automatically recover.</p> <p>Pods created by Deployment won't have the same identity after being killed and recreated, and they don't have unique persistent storage either.</p> <p>Concrete examples: Nginx, Tomcat</p> <p>A typical use case is:</p> <ul> <li>Create a Deployment to bring up a Replica Set and Pods.</li> <li>Check the status of a Deployment to see if it succeeds or not.</li> <li>Later, update that Deployment to recreate the Pods (for example, to use a new   image).</li> <li>Rollback to an earlier Deployment revision if the current Deployment isn't   stable.</li> <li>Pause and resume a Deployment.</li> </ul> <p>Deployment example</p> <pre><code>apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nspec:\nreplicas: 3\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.7.9\nports:\n- containerPort: 80\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes_deployments/#statefulsets", "title": "StatefulSets", "text": "<p>StatefulSets are the controller for stateful applications, therefore it favors consistency over availability.</p> <p>If your applications need to store data, like databases, cache, and message queues, each of your stateful pod will need a stronger notion of identity. Unlike Deployment, StatefulSets creates pods with stable, unique, and sticky identity and storage. You can deploy, scale, and delete pods in order. Which is safer, and makes it easier for you to reason about your stateful applications.</p> <p>Concrete examples: Zookeeper, MongoDB, MySQL</p> <p>The tricky part of the StatefulSets is that in multi node cluster on different regions in AWS, as the persistence is achieved with EBS volumes and they are fixed to a region. Your pod will always spawn in the same node. If there is a failure in that node, the pod will be marked as unschedulable and the service will be down.</p> <p>So as of 2020-03-02 is still not advised to host your databases inside kubernetes unless you have an operator.</p>"}, {"location": "devops/kubernetes/kubernetes_deployments/#daemonset", "title": "DaemonSet", "text": "<p>DaemonSets are the controller for applications that need to be run in each node. It's useful for recollecting log or monitoring purposes.</p> <p>DaemonSet makes sure that every node runs a copy of a pod. If you add or remove nodes, pods will be created or removed on them automatically. If you just want to run the daemons on some of the nodes, use node labels to control it.</p> <p>Concrete examples: fluentd, linkerd</p>"}, {"location": "devops/kubernetes/kubernetes_deployments/#job", "title": "Job", "text": "<p>Jobs are the controller to run batch processing workloads. It creates multiple pods running in parallel to process independent but related work items. This can be the emails to be sent or frames to be rendered.</p>"}, {"location": "devops/kubernetes/kubernetes_external_dns/", "title": "External DNS", "text": "<p>The external-dns resource allows the creation of DNS records from within kubernetes inside the definition of service and ingress resources.</p> <p>It currently supports the following providers:</p> Provider Status Google Cloud DNS Stable AWS Route 53 Stable AWS Cloud Map Beta AzureDNS Beta CloudFlare Beta RcodeZero Alpha DigitalOcean Alpha DNSimple Alpha Infoblox Alpha Dyn Alpha OpenStack Designate Alpha PowerDNS Alpha CoreDNS Alpha Exoscale Alpha Oracle Cloud Infrastructure DNS Alpha Linode DNS Alpha RFC2136 Alpha NS1 Alpha TransIP Alpha VinylDNS Alpha RancherDNS Alpha Akamai FastDNS Alpha <p>There are two reasons to enable it:</p> <ul> <li>If there is any change in the ingress or service load balancer endpoint, due   to a deployment, the dns records are automatically changed.</li> <li>It's easier for developers to connect their applications.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_external_dns/#deployment-in-aws", "title": "Deployment in AWS", "text": "<p>To install it inside EKS, create the <code>ExternalDNSEKSIAMPolicy</code>.</p> ExternalDNSEKSIAMPolicy <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"route53:ChangeResourceRecordSets\"\n],\n\"Resource\": [\n\"arn:aws:route53:::hostedzone/*\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"route53:ListHostedZones\",\n\"route53:ListResourceRecordSets\"\n],\n\"Resource\": [\n\"*\"\n]\n}\n]\n}\n</code></pre> <p>and the associated <code>eks-external-dns</code> role that will be attached to the pod service account.</p> <p>When defining <code>iam_role</code> resources that are going to be used inside EKS, you need to use the EKS OIDC provider as trusted entity, so instead of using the default empty role policy:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Action\": \"sts:AssumeRole\",\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Service\": \"ec2.amazonaws.com\"\n}\n}\n]\n}\n</code></pre> <p>We'll use, as stated in the Creating an IAM Role and Policy for Service Account and Specifying an IAM Role for your Service Account documents:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Action\": \"sts:AssumeRoleWithWebIdentity\",\n\"Condition\": {\n\"StringEquals\": {\n\"oidc.eks.us-east-1.amazonaws.com/id/{{ cluster_fingerprint }}:sub\": \"system:serviceaccount:{{ service_account_namespace }}:{{ service_account_name }}\"\n}\n},\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Federated\": \"arn:aws:iam::{{ aws_account_id }}:oidc-provider/oidc.eks.{{ aws_zone }}.amazonaws.com/id/{{ cluster_fingerprint }}\"\n}\n}\n]\n}\n</code></pre> <p>Then particularize the external-dns helm chart.</p> <p>There are two ways of attaching the IAM role to <code>external-dns</code>, using the <code>asumeRoleArn</code> attribute on the <code>aws</code> values.yaml key or under the <code>rbac</code> <code>serviceAccountAnnotations</code>. I've tried the first but it gave an error because the cluster IAM role didn't have permissions to execute the assume role on that specific role. The second worked flawlessly.</p> <p>For more information visit the official external-dns aws documentation.</p>"}, {"location": "devops/kubernetes/kubernetes_hpa/", "title": "Kubernetes Horizontal pod autoscaling", "text": "<p>With Horizontal pod autoscaling, Kubernetes automatically scales the number of pods in a deployment or replication controller based on observed CPU utilization or on some other application provided metrics.</p> <p>The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user.</p> <p>To make it work, the definition of pod resource consumption needs to be specified.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress/", "title": "Kubernetes Ingress", "text": "<p>An Ingress is An API object that manages external access to the services in a cluster, typically HTTP.</p> <p>Ingress provide a centralized way to:</p> <ul> <li>Load balancing.</li> <li>SSL termination.</li> <li>Dynamic service discovery.</li> <li>Traffic routing.</li> <li>Authentication.</li> <li>Traffic distribution: canary deployments, A/B testing, mirroring/shadowing.</li> <li>Graphical user interface.</li> <li>JWT validation.</li> <li>WAF and DDOS protection.</li> <li>Requests tracing.</li> </ul> <p>An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.</p> <p>An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/", "title": "Kubernetes Ingress Controller", "text": "<p>Ingress controllers monitor the cluster events for the creation or modification of Ingress resources, modifying accordingly the underlying load balancers. They are not part of the master kube-controller-manager, so you'll need to install them manually.</p> <p>There are different Ingress controllers, such as AWS ALB, Nginx, HAProxy or Traefik, using one or other depends on your needs.</p> <p>Almost all controllers are open sourced and support dynamic service discovery, SSL termination or WebSockets. But they differ in:</p> <ul> <li>Supported protocols: HTTP, HTTPS, gRPC, HTTP/2.0, TCP (with SNI) or UDP.</li> <li>Underlying software: NGINX, Traefik, HAProxy or Envoy.</li> <li>Traffic routing: host and path, regular expression support.</li> <li>Namespace limitations: supported or not.</li> <li>Upstream probes: active checks, passive checks, retries, circuit breakers,     custom health checks...</li> <li>Load balancing algorithms: round-robin, sticky sessions, rdp-cookie...</li> <li>Authentication: Basic, digest, Oauth, external auth, SSL certificate...</li> <li>Traffic distribution: canary deployments, A/B testing, mirroring/shadowing.</li> <li>Paid subscription: extended functionality or technical support.</li> <li>Graphical user interface:</li> <li>JWT validation:</li> <li>Customization of configuration:</li> <li>Basic DDOS protection mechanisms: rate limit, traffic filtering.</li> <li>WAF:</li> <li>Requests tracing: monitor, trace and debug requests via OpenTracing or other     options.</li> </ul> <p>Both ITNext and Flant provide good ingress controller comparisons, a synoptical resume of both articles follows.</p> <p></p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#kubernetes-ingress-controller", "title": "Kubernetes Ingress controller", "text": "<p>The \u201cofficial\u201d Kubernetes controller. Not to be confused with the one offered by the NGINX company. Developed by the community, it's based on the Nginx web server with a set of Lua plugins to implement extra features.</p> <p>Thanks to the popularity of NGINX and minimal modifications over it when using as a controller, it can be the simplest and most straightforward option for an average engineer dealing with K8s.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#traefik", "title": "Traefik", "text": "<p>Originally, this proxy was created for the routing of requests for microservices and their dynamic environment, hence many of its useful features:</p> <ul> <li>Continuous update of configuration (no restarts) .</li> <li>Support for multiple load balancing algorithms.</li> <li>Web UI.</li> <li>Metrics export.</li> <li>Support for various protocols.</li> <li>REST API.</li> <li>Canary releases.</li> <li>Let\u2019s Encrypt certificates support.</li> <li>TCP/SSL with SNI.</li> <li>Traffic mirroring/shadowing.</li> </ul> <p>The main disadvantage is that in order to organize the high availability of the controller you have to install and connect its own KV-storage.</p> <p>In 2019, the same developers have developed Maesh. Another service mesh solution built on top of Traefik.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#haproxy", "title": "HAProxy", "text": "<p>HAProxy is well known as a proxy server and load balancer. As part of the Kubernetes cluster, it offers: * \u201csoft\u201d configuration update (without traffic loss) * DNS-based service discovery * Dynamic configuration through API. * Full customization of a config-files template (via replacing a ConfigMap). * Using Spring Boot functions. * Great number of supported balancing algorithms.</p> <p>In general, developers put emphasis on high speed, optimization, and efficiency in consumed resources.</p> <p>It\u2019s worth mentioning a lot of new features have appeared in a recent (June\u201919) v2.0 release, and even more (including OpenTracing support) is expected with upcoming v2.1.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#istio-ingress", "title": "Istio Ingress", "text": "<p>Istio is a comprehensive service mesh solution. It can manage not just all incoming outside traffic (as an Ingress controller) but control all traffic inside the cluster as well. Under the hood, Istio uses Envoy as a sidecar-proxy for each service. In essence, it is a large processor that can do almost anything. Its central idea is maximum control, extensibility, security, and transparency.</p> <p>With Istio Ingress, you can fine tune traffic routing, access authorization between services, balancing, monitoring, canary releases and much more.</p> <p>\u201cBack to microservices with Istio\u201d is a great intro to learn about Istio.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-controller", "title": "ALB Ingress controller", "text": "<p>The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers.</p> <p>It's advantages are:</p> <ul> <li>AWS managed loadbalancer.</li> <li>Authentication with OIDC or Cognito.</li> <li>AWS WAF support.</li> <li>Natively redirect HTTP to HTTPS.</li> <li>Supports fixed response without forwarding to the application..</li> </ul> <p>It has also the potential advantage of using IP traffic mode. ALB support two types of traffic:</p> <ul> <li>instance mode: Ingress traffic starts from the ALB and reaches the NodePort   opened for your service. Traffic is then routed to the container Pods within   the cluster. The number of hops for the packet to reach its destination in   this mode is always two.</li> <li>IP mode: Ingress traffic starts from the ALB and reaches the container Pods   within cluster directly. In order to use this mode, the networking plugin for   the K8s cluster must use a secondary IP address on ENI as pod IP, aka AWS CNI   plugin for K8s. The number of hops for the packet to reach its destination is   always one.</li> </ul> <p>The IP mode gives the following advantages:</p> <ul> <li>The load balancer can be pod location-aware: reduce the chance to route   traffic to an irrelevant node and then rely on kube-proxy and network agent.</li> <li>The number of hops for the packet to reach its destination is always one</li> <li>No extra overlay network comparing to using Network plugins (Calico, Flannel)   directly int he cloud (AWS).</li> </ul> <p>It also has it's disadvantages:</p> <ul> <li>Even though AWS guides you on it's     deployment,     after two months of AWS Support cases, I wasn't able to deploy it using     terraform and helm.</li> <li> <p>You can't reuse existing ALBs instead of creating new ALB per   ingress.</p> <p>Therefore <code>ingress: false</code> needs to be specified in the service helm chart and manually edit the ALB ingress helm chart to add each new service.</p> </li> </ul>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-deployment", "title": "ALB ingress deployment", "text": "<p>This section is a defunct work in progress. I wasn't able to make it work, but it can be useful if you want to deploy it yourself. If you success, please make a PR.</p> <p>I've used the AWS Guide, in conjunction with the AWS general ALB controller documentation and the AWS general ALB documentation to define the properties of the incubator helm chart.</p> <p>Before that you need to an IAM policy ALBIngressControllerIAMPolicy to give the required permissions to manage the certificates, ALB creation and WAF integration. That IAM policy needs to be attached to the <code>eks-alb-ingress-controller</code> IAM role.</p> <p>You also had to create an IAM OIDC provider, which are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Github. You use an IAM OIDC identity provider when you want to establish trust between an OIDC compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign in code or manage your own user identities.</p> <p>The IAM OIDC provider is going to be used by the AWS EKS pod identity admission controller to attach IAM roles to specific service accounts. This will prevent the attachment of the IAM role to the whole node group. So instead of allowing every pod inside the worker groups to create the ALB, only the ones attached to the eks-alb-ingress-controller service account will be able to do so.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#links", "title": "Links", "text": "<ul> <li>ITNext ingress controller   comparison</li> <li>Flant ingress controller comparison</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_jobs/", "title": "Kubernetes jobs", "text": "<p>Kubernetes jobs creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created.</p> <p>Cronjobs creates Jobs on a repeating schedule.</p> <p>This example CronJob manifest prints the current time and a hello message every minute:</p> <pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\nname: hello\nspec:\nschedule: \"*/1 * * * *\"\njobTemplate:\nspec:\ntemplate:\nspec:\ncontainers:\n- name: hello\nimage: busybox\nargs:\n- /bin/sh\n- -c\n- date; echo Hello from the Kubernetes cluster\nrestartPolicy: OnFailure\n</code></pre> <p>To deploy cronjobs you can use the bambash helm chart.</p> <p>Check the kubectl commands to interact with jobs.</p>"}, {"location": "devops/kubernetes/kubernetes_jobs/#debugging-job-logs", "title": "Debugging job logs", "text": "<p>To obtain the logs of a completed or failed job, you need to:</p> <ul> <li>Locate the cronjob you want to debug: <code>kubectl get cronjobs -n cronjobs</code>.</li> <li>Locate the associated job: <code>kubectl get jobs -n cronjobs</code>.</li> <li>Locate the associated pod: <code>kubectl get pods -n cronjobs</code>.</li> </ul> <p>If the pod still exists, you can execute <code>kubectl logs -n cronjobs {{ pod_name }}</code>. If the pod doesn't exist anymore, you need to search the pod in your log centralizer solution.</p>"}, {"location": "devops/kubernetes/kubernetes_jobs/#rerunning-failed-jobs", "title": "Rerunning failed jobs", "text": "<p>If you have a job that has failed after the 6 default retries, it will show up in your monitorization forever, to fix it, you can manually trigger the job yourself with:</p> <pre><code>kubectl get job \"your-job\" -o json \\\n| jq 'del(.spec.selector)' \\\n| jq 'del(.spec.template.metadata.labels)' \\\n| kubectl replace --force -f -\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes_jobs/#manually-creating-a-job-from-a-cronjob", "title": "Manually creating a job from a cronjob", "text": "<pre><code>kubectl create job {{ job_name }} -n {{ namespace }} \\\n--from=cronjobs/{{ cronjob_name}}\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes_jobs/#monitorization-of-cronjobs", "title": "Monitorization of cronjobs", "text": "<p>Alerting of traditional Unix cronjobs meant sending an email if the job failed. Most job scheduling systems that have followed have provided the same experience, Kubernetes does not. One approach to alerting jobs is to use the Prometheus push gateway, allowing us to push richer metrics than the success/failure status. This approach has it\u2019s downsides; we have to update the code for our jobs, we also have to explicitly configure a push gateway location and update it if it changes (a burden alleviated by the pull based metrics for long lived workloads). You can use tools such as Sentry, but it will also require changes to the jobs.</p> <p>Jobs are powerful things allowing us to implement several different workflows, the combination of options can be overwhelming compared to a traditional Unix cron job. This variety makes it difficult to establish one simple rule for alerting failed jobs. Things get easier if we restrict ourselves to a subset of possible options. We will focus on non-concurrent jobs.</p> <p>The relationship between cronjobs and jobs makes the task ahead difficult. To make our life easier we will put one requirement on the jobs we create, they will have to include a label that associates them with the original cronjob.</p> <p>Below we present an example of our ideal cronjob (which matches what the helm chart deploys):</p> <pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\nname: our-task\nspec:\nschedule: \"*/5 * * * *\"\nsuccessfulJobsHistoryLimit: 3\nconcurrencyPolicy: Forbid\njobTemplate:\nmetadata:\nlabels:\ncron: our-task # &lt;-- match created jobs with the cronjob\nspec:\nbackoffLimit: 3\ntemplate:\nmetadata:\nlabels:\ncronjob: our-task\nspec:\ncontainers:\n- name: our-task\ncommand:\n- /user/bin/false\nimage: alpine\nrestartPolicy: Never\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes_jobs/#building-our-alert", "title": "Building our alert", "text": "<p>We are also going to need some metrics to get us started. K8s does not provide us any by default, but fortunately <code>kube-state-metrics</code> is installed with the Prometheus operator chart, so we have the following metrics:</p> <pre><code>kube_cronjob_labels{\n  cronjob=\"our-task\",\n  namespace=\"default\"} 1\nkube_job_created{\n  job=\"our-task-1520165700\",\n  namespace=\"default\"} 1.520165707e+09\nkube_job_failed{\n  condition=\"false\",\n  job=\"our-task-1520165700\",\n  namespace=\"default\"} 0\nkube_job_failed{\n  condition=\"true\",\n  job=\"our-task-1520165700\",\n  namespace=\"default\"} 1\nkube_job_labels{\n  job=\"our-task-1520165700\",\n  label_cron=\"our-task\",\n  namespace=\"default\"} 1\n</code></pre> <p>This shows the primary set of metrics we will be using to construct our alert. What is not shown above is the status of the cronjob. The big challenge with K8s cronjob alerting is that cronjobs themselves do not possess any status information, beyond the last time the cronjob created a job. The status information only exists on the job that the cronjob creates.</p> <p>In order to determine if our cronjob is failing, our first order of business is to find which jobs we should be looking at. A K8s cronjob creates new job objects and keeps a number of them around to help us debug the runs of our jobs. We have to be determine which job corresponds to the last run of our cronjob. If we have added the cron label to the jobs as above, we can find the last run time of the jobs for a given cronjob as follows:</p> <pre><code>max(\nkube_job_status_start_time\n* ON(job_name) GROUP_RIGHT()\nkube_job_labels{label_cron!=\"\"}\n) BY (job_name, label_cron)\n</code></pre> <p>This query demonstrates an important technique when working with <code>kube-state-metrics</code>. For each API object it exported data on, it exports a time series including all the labels for that object. These time series have a value of 1. As such we can join the set of labels for an object onto the metrics about that object by multiplication.</p> <p>Depending on how your Prometheus instance is configured, the value of the job label on your metrics will likely be <code>kube-state-metrics</code>. <code>kube-state-metrics</code> adds a job label itself with the name of the job object. Prometheus resolves this collision of label names by including the raw metric\u2019s label as an <code>job_name</code> label.</p> <p>Since we are querying the start time of jobs, and there should only ever be one job with a given name. You may wonder why we need the max aggregation. Manually plugging the query into Prometheus may convince you that it is unnecessary. Consider though that you may have multiple instances of <code>kube-state-metrics</code> running for redundancy. Using max ensures our query is valid even if we have multiple instances of kube-state-metrics running. Issues of duplicate metrics are common when constructing production recording rules and alerts.</p> <p>We can find the start time of the most recent job for a given cronjob by finding the maximum of all job start times as follows:</p> <pre><code>max(\nkube_job_status_start_time\n* ON(job_name) GROUP_RIGHT()\nkube_job_labels{label_cron!=\"\"}\n) BY (label_cron)\n</code></pre> <p>The only difference between this and the previous query is in the labels used for the aggregation. Now that we have the start time of each job, and the start time of the most recent job, we can do a simple equality match to find the start time of the most recent job for a given cronjob. We will create a metric for this:</p> <pre><code>- record: job_cronjob:kube_job_status_start_time:max\nexpr: |\nsum without (label_cron, job_name) (\nlabel_replace(\nlabel_replace(\nmax(\nkube_job_status_start_time\n* ON(job_name) GROUP_RIGHT()\nkube_job_labels{label_cron!=\"\"}\n) BY (job_name, label_cron)\n\n== ON(label_cron) GROUP_LEFT()\n\nmax(\nkube_job_status_start_time\n* ON(job_name) GROUP_RIGHT()\nkube_job_labels{label_cron!=\"\"}\n) BY (label_cron),\n\"job\", \"$1\", \"job_name\", \"(.+)\"\n),\n\"cronjob\", \"$1\", \"label_cron\", \"(.+)\"\n)\n)\n</code></pre> <p>We have also taken the opportunity to adjust the labels to be a little more aesthetically pleasing. By copying <code>job_name</code> to <code>job</code>, <code>label_cron</code> to <code>cronjob</code> and removing <code>job_name</code> and <code>label_cron</code>. Now that we have the most recently started job for a given cronjob, we can find which, if any, have failed attempts:</p> <pre><code>- record: job_cronjob:kube_job_status_failed:sum\nexpr: |\nsum without (label_cron, job_name) (\nclamp_max(\njob_cronjob:kube_job_status_start_time:max,\n1\n)\n\n* ON(job) GROUP_LEFT()\n\nlabel_replace(\nlabel_replace(\n(\nkube_job_status_failed != 0 and\nkube_job_status_succeeded == 0\n),\n\"job\", \"$1\", \"job_name\", \"(.+)\"\n),\n\"cronjob\", \"$1\", \"label_cron\", \"(.+)\"\n)\n)\n</code></pre> <p>The initial <code>clamp_max</code> clause is used to transform our start times metric into a set of time series to perform label matching to filter another set of metrics. Multiplication by 1 (or addition of 0), is a useful means of filter and merging time series and it is well worth taking the time to understand the technique.</p> <p>We get those cronjobs that have a failed job and no successful ones with the query:</p> <pre><code>(\nkube_job_status_failed != 0 and\nkube_job_status_succeeded == 0\n)\n</code></pre> <p>The <code>kube_job_status_succeeded == 0</code> it's important, otherwise once a job has a failed instance, it doesn't matter if there's a posterior one that succeeded, we're going to keep on receiving the alert that it failed.</p> <p>We adjust the labels on the previous query to match our start time metric so ensure the labels have the same meaning as those on our <code>job_cronjob:kube_job_status_start_time:max</code> metric. The label matching on the multiplication will then perform our filtering. We now have a metric containing the set of most recently failed jobs, labeled by their parent cronjob, so we can now construct the alert:</p> <pre><code>- alert: CronJobStatusFailed\nexpr: job_cronjob:kube_job_status_failed:sum &gt; 0\nfor: 1m\nannotations:\ndescription: '{{ $labels.cronjob }} last run has failed {{ $value }} times.'\n</code></pre> <p>We use the <code>kube_cronjob_labels</code> here to merge in labels from the original cronjob.</p>"}, {"location": "devops/kubernetes/kubernetes_labels/", "title": "Kubernetes Labels", "text": "<p>Labels are identifying metadata key/value pairs attached to objects, such as pods. Labels are intended to give meaningful and relevant information to users, but which do not directly imply semantics to the core system.</p> <pre><code>\"labels\": {\n\"key1\" : \"value1\",\n\"key2\" : \"value2\"\n}\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes_metric_server/", "title": "Kubernetes Metric server", "text": "<p>The metrics server monitors the resource consumption inside the cluster. It populates the information in <code>kubectl top nodes</code> to get the node status and gives the information to automatically autoscale deployments with Horizontal pod autoscaling.</p> <p>To install it, you can use the <code>metrics-server</code> helm chart.</p> <p>To test that the horizontal pod autoscaling is working, follow the AWS EKS guide.</p>"}, {"location": "devops/kubernetes/kubernetes_namespaces/", "title": "Kubernetes Namespaces", "text": "<p>Namespaces are virtual clusters backed by the same physical cluster. It's the first level of isolation between applications.</p>"}, {"location": "devops/kubernetes/kubernetes_namespaces/#when-to-use-multiple-namespaces", "title": "When to Use Multiple Namespaces", "text": "<p>Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.</p> <p>Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces.</p> <p>Namespaces are a way to divide cluster resources between multiple uses (via resource quota).</p> <p>It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/", "title": "Networking", "text": "<p>Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet.</p> <p>If you want to get a quickly grasp on how k8s networking works, I suggest you to read StackRox's Kubernetes networking demystified article.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#cni-comparison", "title": "CNI comparison", "text": "<p>Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet.</p> <p>There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources:</p> <ul> <li>Rancher k8s CNI comparison.</li> <li>ITnext k8s CNI   comparison.</li> <li>Mark Ramm-Christensen AWS CNI   analysis.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_networking/#tldr", "title": "TL;DR", "text": "<p>When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico.</p> <p>Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support.</p> <p>If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step.</p> <p>I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes.</p> <p>I wouldn't use Weave either unless you need  encryption throughout all the internal network and multicast.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#flannel", "title": "Flannel", "text": "<p>Flannel, a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry.</p> <p>Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store.</p> <p>Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination.</p> <p>Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options.</p> <p>Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption.</p> <p>It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#calico", "title": "Calico", "text": "<p>Calico, is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration.</p> <p>On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster.</p> <p>Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic.</p> <p>Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior.</p> <p>In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio, a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment.</p> <p>Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it.</p> <p>If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#canal", "title": "Canal", "text": "<p>Canal is an interesting option for quite a few reasons.</p> <p>First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists.</p> <p>Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control.</p> <p>After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking.</p> <p>In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#weave-net", "title": "Weave Net", "text": "<p>Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems.</p> <p>To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method.</p> <p>Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes.</p> <p>Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic.</p> <p>Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it.</p> <p>Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance.  It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#aws-cni", "title": "AWS CNI", "text": "<p>AWS developed their own CNI that uses Elastic Network Interfaces for pod networking.</p> <p>It's the default CNI if you use EKS.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#advantages-of-the-aws-cni", "title": "Advantages of the AWS CNI", "text": "<p>Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes:</p> <ul> <li>Raw AWS network performance.</li> <li>Integration of tools familiar to AWS developers and admins, like AWS VPC flow   logs and Security Groups \u2014 allowing users with existing VPC networks and   networking best practices to carry those over directly to Kubernetes.</li> <li>The ability to enforce network policy decisions at the Kubernetes layer if you   install Calico.</li> </ul> <p>If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#disadvantages-of-the-aws-cni", "title": "Disadvantages of the AWS CNI", "text": "<p>On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network.</p> <ul> <li>Makes the multi-cloud k8s advantage more difficult.</li> <li>the CNI limits the number of pods that can be scheduled on each k8s node   according to the number of IP Addresses available to each EC2 instance type so   that each pod can be allocated an IP.</li> <li>Doesn't support encryption on the network.</li> <li>Multicast requirements.</li> <li>It eats up the number of IP Addresses available within your VPC unless you   give it an alternate subnet.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_networking/#vpc-cni-pod-density-limitations", "title": "VPC CNI Pod Density Limitations", "text": "<p>First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly.</p> <p>This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster.</p> <p>The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom DaemonSet that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance:</p> <pre><code>ENIs * (IPs_per_ENI - 1)\n</code></pre> <p>Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod.</p> <p>Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations.</p> <p>And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam DaemonSet, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have DaemonSets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node.</p> <p>So now the formula is:</p> <pre><code>(ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets\n</code></pre> <p>This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods.</p> <p>On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead.</p> <p>Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions:</p> <ul> <li>A 100 pod/node limit setting in Kubernetes,</li> <li>A default of 4 DaemonSets (2 for AWS networking, 1 for log aggregation, and   1 for metric collection),</li> <li>A simple cost calculation for per-pod pricing.</li> </ul> <p>This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet:</p> <ul> <li>CPU and memory requirements will often dictate lower pod density than the   theoretical maximum here.</li> <li>Beyond DaemonSets, Kubernetes System pods, and other \u201csystem level\u201d   operational tools used in your cluster will consume Pod IP\u2019s and limit the   number of application pods that you can run.</li> <li>Each instance type also has network performance limitations which may impact   performance often far before theoretical pod limits are reached.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_networking/#cloud-portability", "title": "Cloud Portability", "text": "<p>Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions.</p> <p>However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#links", "title": "Links", "text": "<ul> <li>StackRox Kubernetes networking demystified article.</li> <li>Writing your own simple CNI plug   in.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_operators/", "title": "Kubernetes Operators", "text": "<p>Operators are Kubernetes specific applications (pods) that configure, manage and optimize other Kubernetes deployments automatically.</p> <p>A Kubernetes Operator might be able to:</p> <ul> <li>Install and provide sane initial configuration and sizing for your deployment,   according to the specs of your Kubernetes cluster.</li> <li>Perform live reloading of deployments and pods to accommodate for any   user requested parameter modification (hot config reloading).</li> <li>Safe coordination of application upgrades.</li> <li>Automatically scale up or down according to performance metrics.</li> <li>Service discovery via native Kubernetes APIs</li> <li>Application TLS certificate configuration</li> <li>Disaster recovery.</li> <li>Perform backups to offsite storage, integrity checks or any other maintenance task.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_operators/#how-do-they-work", "title": "How do they work?", "text": "<p>An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster.</p> <p>Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets.</p> <p>An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1.</p>"}, {"location": "devops/kubernetes/kubernetes_operators/#links", "title": "Links", "text": "<ul> <li>CoreOS introduction to Operators</li> <li>Sysdig Prometheus Operator guide part 3</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_pods/", "title": "Kubernetes Pods", "text": "<p>Pods are the basic building block of Kubernetes, the smallest and simplest unit in the object model that you create or deploy. A Pod represents a running process on your cluster.</p> <p>A Pod represents a unit of deployment. It encapsulates:</p> <ul> <li>An application container (or, in some cases, multiple tightly coupled containers).</li> <li>Storage resources.</li> <li>A unique network IP.</li> <li>Options that govern how the container(s) should run.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_replicasets/", "title": "Kubernetes ReplicaSets", "text": "<p>ReplicaSet maintains a stable set of replica Pods running at any given time.  As such, it is often used to guarantee the availability of a specified number of identical Pods.</p> <p>You'll probably never manually use these resources, as they are defined inside the deployments. The older version of this resource are the Replication controllers.</p>"}, {"location": "devops/kubernetes/kubernetes_services/", "title": "Kubernetes Services", "text": "<p>A Service defines a policy to access a logical set of Pods using a reliable endpoint. Users and other programs can access pods running on your cluster seamlessly. Therefore allowing a loose coupling between dependent Pods.</p> <p>When a request arrives the endpoint, the kube-proxy pod of the node forwards the request to the Pods that match the service LabelSelector.</p> <p>Services can be exposed in different ways by specifying a type in the ServiceSpec:</p> <ul> <li> <p>ClusterIP (default): Exposes the Service on an internal IP in the cluster.   This type makes the Service only reachable from within the cluster.</p> </li> <li> <p>NodePort: Exposes the Service on the same port of each selected Node in the   cluster using NAT to the outside.</p> </li> <li> <p>LoadBalancer: Creates an external load balancer in the current cloud   and assigns a fixed, external IP to the Service.</p> <p>To create an internal ELB of AWs add to the annotations: <pre><code>annotations:\nservice.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\n</code></pre></p> </li> <li> <p>ExternalName: Exposes the Service using an arbitrary name by returning   a CNAME record with the name. No proxy is used.</p> </li> </ul> <p>If no RBAC or NetworkPolicies are applied, you can call a service of another namespace with the following nomenclature.</p> <pre><code>curl {{ service_name}}.{{ service_namespace }}.svc.cluster.local\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes_storage_driver/", "title": "Kubernetes Storage Driver", "text": "<p>Storage drivers are pods that through the Container Storage Interface or CSI provide an interface to use external storage services from within Kubernetes.</p>"}, {"location": "devops/kubernetes/kubernetes_storage_driver/#amazon-ebs-csi-storage-driver", "title": "Amazon EBS CSI storage driver", "text": "<p>Allows Kubernetes clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes with the <code>awsElasticBlockStore</code> volume type.</p> <p>To install it, you first need to attach the <code>Amazon_EBS_CSI_Driver</code> IAM policy to the worker nodes. Then you can use the <code>aws-ebs-csi-driver</code> helm chart.</p> <p>To test it worked follow the steps under To deploy a sample application and verify that the CSI driver is working.</p>"}, {"location": "devops/kubernetes/kubernetes_tools/", "title": "Kubernetes Tools", "text": "<p>There are several tools built to enhance the operation, installation and use of Kubernetes.</p>"}, {"location": "devops/kubernetes/kubernetes_tools/#tried", "title": "Tried", "text": "<ul> <li>K3s: Recommended small kubernetes, like hyperkube.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_tools/#to-try", "title": "To try", "text": "<ul> <li>crossplane: Crossplane is an   open source multicloud control plane. It introduces workload and resource   abstractions on-top of existing managed services that enables a high degree of   workload portability across cloud providers. A single crossplane enables the   provisioning and full-lifecycle management of services and infrastructure   across a wide range of providers, offerings, vendors, regions, and clusters.   Crossplane offers a universal API for cloud computing, a workload scheduler,   and a set of smart controllers that can automate work across clouds.</li> <li>razee: A multi-cluster continuous delivery tool for Kubernetes   Automate the rollout process of Kubernetes resources across multiple clusters,   environments, and cloud providers, and gain insight into what applications and   versions run in your cluster.</li> <li>kube-ops-view: it shows how are   the ops on the nodes.</li> <li>kubediff: a tool for Kubernetes to   show differences between running state and version controlled configuration.</li> <li>ksniff: A kubectl plugin that utilize   tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes   cluster.</li> <li>kubeview: Visualize   dependencies kubernetes.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_vertical_pod_autoscaler/", "title": "Vertical Pod Autoscaler", "text": "<p>Kubernetes knows the amount of resources a pod needs to operate through some metadata specified in the deployment. Generally this values change and manually maintaining all the resources requested and limits is a nightmare.</p> <p>The Vertical pod autoscaler does data analysis on the pod metrics to automatically adjust these values.</p> <p>Nevertheless it's still not suggested to use it in conjunction with the horizontal pod autoscaler, so we'll need to watch out for future improvements.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/", "title": "Kubernetes Volumes", "text": "<p>On disk files in a Container are ephemeral by default, which presents the following issues:</p> <ul> <li>When a Container crashes, kubelet will restart it, but the files will be lost.</li> <li>When running Containers together in a Pod it is often necessary to share files   between those Containers.</li> </ul> <p>The Kubernetes Volume abstraction solves both of these problems with several types.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/#configmap", "title": "configMap", "text": "<p>The <code>configMap</code> resource provides a way to inject configuration data into Pods. The data stored in a ConfigMap object can be referenced in a volume of type <code>configMap</code> and then consumed by containerized applications running in a Pod.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/#emptydir", "title": "emptyDir", "text": "<p>An <code>emptyDir</code> volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the <code>emptyDir</code> volume. When a Pod is removed from a node for any reason, the data in the <code>emptyDir</code> is deleted forever.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/#hostpath", "title": "hostPath", "text": "<p>A <code>hostPath</code> volume mounts a file or directory from the host node\u2019s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications.</p> <p>For example, some uses for a hostPath are:</p> <ul> <li>Running a Container that needs access to Docker internals; use a hostPath of   <code>/var/lib/docker</code>.</li> <li>Running cAdvisor in a Container; use a hostPath of <code>/sys</code>.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_volumes/#secret", "title": "secret", "text": "<p>A <code>secret</code> volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by Pods without coupling to Kubernetes directly. <code>secret</code> volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/#awselasticblockstore", "title": "awsElasticBlockStore", "text": "<p>An <code>awsElasticBlockStore</code> volume mounts an Amazon Web Services (AWS) EBS Volume into your Pod. Unlike <code>emptyDir</code>, which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods.</p> <p>There are some restrictions when using an awsElasticBlockStore volume:</p> <ul> <li>The nodes on which Pods are running must be AWS EC2 instances.</li> <li>Those instances need to be in the same region and availability-zone as the EBS   volume.</li> <li>EBS only supports a single EC2 instance mounting a volume.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_volumes/#nfs", "title": "nfs", "text": "<p>An <code>nfs</code> volume allows an existing NFS (Network File System) share to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an <code>nfs</code> volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. NFS can be mounted by multiple writers simultaneously.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/#local", "title": "local", "text": "<p>A <code>local</code> volume represents a mounted local storage device such as a disk, partition or directory.</p> <p>Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet.</p> <p>Compared to <code>hostPath</code> volumes, local volumes can be used in a durable and portable manner without manually scheduling Pods to nodes, as the system is aware of the volume\u2019s node constraints by looking at the node affinity on the PersistentVolume.</p> <p>However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/#others", "title": "Others", "text": "<ul> <li>glusterfs</li> <li>cephfs</li> </ul>"}, {"location": "devops/prometheus/alertmanager/", "title": "AlertManager", "text": "<p>The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.</p> <p>It is configured through the <code>alertmanager.config</code> key of the <code>values.yaml</code> of the helm chart.</p> <p>As stated in the configuration file, it has four main keys (as <code>templates</code> is handled in <code>alertmanager.config.templateFiles</code>):</p> <ul> <li><code>global</code>: SMTP and API main configuration, it will be inherited by the other     elements.</li> <li><code>route</code>: Route tree definition.</li> <li><code>receivers</code>: Notification integrations configuration.</li> <li><code>inhibit_rules</code>: Alert inhibition configuration.</li> </ul>"}, {"location": "devops/prometheus/alertmanager/#route", "title": "Route", "text": "<p>A route block defines a node in a routing tree and its children. Its optional configuration parameters are inherited from its parent node if not set.</p> <p>Every alert enters the routing tree at the configured top-level route, which must match all alerts (i.e. not have any configured matchers). It then traverses the child nodes. If continue is set to false, it stops after the first matching child. If continue is true on a matching node, the alert will continue matching against subsequent siblings. If an alert does not match any children of a node (no matching child nodes, or none exist), the alert is handled based on the configuration parameters of the current node.</p>"}, {"location": "devops/prometheus/alertmanager/#receivers", "title": "Receivers", "text": "<p>Notification receivers are the named configurations of one or more notification integrations.</p>"}, {"location": "devops/prometheus/alertmanager/#email-notifications", "title": "Email notifications", "text": "<p>To configure email notifications, set up the following in your <code>config</code>:</p> <pre><code>  config:\nglobal:\nsmtp_from: {{ from_email_address }}\nsmtp_smarthost: {{ smtp_server_endpoint }}:{{ smtp_server_port }}\nsmtp_auth_username: {{ smpt_authentication_username }}\nsmtp_auth_password: {{ smpt_authentication_password }}\nreceivers:\n- name: 'email'\nemail_configs:\n- to: {{ receiver_email }}\nsend_resolved: true\n</code></pre> <p>If you need to set <code>smtp_auth_username</code> and <code>smtp_auth_password</code> you should value using helm secrets.</p> <p><code>send_resolved</code>, set to <code>False</code> by default, defines whether or not to notify about resolved alerts.</p>"}, {"location": "devops/prometheus/alertmanager/#rocketchat-notifications", "title": "Rocketchat Notifications", "text": "<p>Go to pavel-kazhavets AlertmanagerRocketChat repo for the updated rules.</p> <p>In RocketChat:</p> <ul> <li>Login as admin user and go to: Administration =&gt; Integrations =&gt; New Integration =&gt; Incoming WebHook.</li> <li>Set \"Enabled\" and \"Script Enabled\" to \"True\".</li> <li>Set all channel, icons, etc. as you need.</li> <li>Paste contents of the official AlertmanagerIntegrations.js or my version into Script field.</li> </ul> AlertmanagerIntegrations.js <pre><code>class Script {\nprocess_incoming_request({\nrequest\n}) {\nconsole.log(request.content);\n\nvar alertColor = \"warning\";\nif (request.content.status == \"resolved\") {\nalertColor = \"good\";\n} else if (request.content.status == \"firing\") {\nalertColor = \"danger\";\n}\n\nlet finFields = [];\nfor (i = 0; i &lt; request.content.alerts.length; i++) {\nvar endVal = request.content.alerts[i];\nvar elem = {\ntitle: \"alertname: \" + endVal.labels.alertname,\nvalue: \"*instance:* \" + endVal.labels.instance,\nshort: false\n};\n\nfinFields.push(elem);\n\nif (!!endVal.annotations.summary) {\nfinFields.push({\ntitle: \"summary\",\nvalue: endVal.annotations.summary\n});\n}\n\nif (!!endVal.annotations.severity) {\nfinFields.push({\ntitle: \"severity\",\nvalue: endVal.labels.severity\n});\n}\n\nif (!!endVal.annotations.grafana) {\nfinFields.push({\ntitle: \"grafana\",\nvalue: endVal.annotations.grafana\n});\n}\n\nif (!!endVal.annotations.prometheus) {\nfinFields.push({\ntitle: \"prometheus\",\nvalue: endVal.annotations.prometheus\n});\n}\n\nif (!!endVal.annotations.message) {\nfinFields.push({\ntitle: \"message\",\nvalue: endVal.annotations.message\n});\n}\n\nif (!!endVal.annotations.description) {\nfinFields.push({\ntitle: \"description\",\nvalue: endVal.annotations.description\n});\n}\n}\n\nreturn {\ncontent: {\nusername: \"Prometheus Alert\",\nattachments: [{\ncolor: alertColor,\ntitle_link: request.content.externalURL,\ntitle: \"Prometheus notification\",\nfields: finFields\n}]\n}\n};\n\nreturn {\nerror: {\nsuccess: false\n}\n};\n}\n}\n</code></pre> <ul> <li>Create Integration. The field <code>Webhook URL</code> will appear in the Integration configuration.</li> </ul> <p>In Alertmanager:</p> <ul> <li>Create new receiver or modify config of existing one. You'll need to add <code>webhooks_config</code> to it. Small example:</li> </ul> <pre><code>route:\nrepeat_interval: 30m\ngroup_interval: 30m\nreceiver: 'rocketchat'\n\nreceivers:\n- name: 'rocketchat'\nwebhook_configs:\n- send_resolved: false\nurl: '${WEBHOOK_URL}'\n</code></pre> <ul> <li>Reload/restart alertmanager.</li> </ul> <p>In order to test the webhook you can use the following curl (replace <code>{{ webhook-url }}</code>):</p> <pre><code>curl -X POST -H 'Content-Type: application/json' --data '\n{\n  \"text\": \"Example message\",\n  \"attachments\": [\n    {\n      \"title\": \"Rocket.Chat\",\n      \"title_link\": \"https://rocket.chat\",\n      \"text\": \"Rocket.Chat, the best open source chat\",\n      \"image_url\": \"https://rocket.cha t/images/mockup.png\",\n      \"color\": \"#764FA5\"\n    }\n  ],\n  \"status\": \"firing\",\n  \"alerts\": [\n    {\n      \"labels\": {\n        \"alertname\": \"high_load\",\n        \"severity\": \"major\",\n        \"instance\": \"node-exporter:9100\"\n      },\n      \"annotations\": {\n        \"message\": \"node-exporter:9100 of job xxxx is under high load.\",\n        \"summary\": \"node-exporter:9100 under high load.\"\n      }\n    }\n  ]\n}\n' {{ webhook-url }}\n</code></pre>"}, {"location": "devops/prometheus/alertmanager/#inhibit-rules", "title": "Inhibit rules", "text": "<p>Inhibit rules define which alerts triggered by Prometheus shouldn't be forwarded to the notification integrations. For example the <code>Watchdog</code> alert is meant to test that everything works as expected, but is not meant to be used by the users. Similarly, if you are using EKS, you'll probably have an <code>KubeVersionMismatch</code>, because Kubernetes allows a certain version skew between their components. So the alert is more strict than the Kubernetes policy.</p> <p>To disable both alerts, set a <code>match</code> rule in <code>config.inhibit_rules</code>:</p> <pre><code>  config:\ninhibit_rules:\n- target_match:\nalertname: Watchdog\n- target_match:\nalertname: KubeVersionMismatch\n</code></pre>"}, {"location": "devops/prometheus/alertmanager/#alert-rules", "title": "Alert rules", "text": "<p>Alert rules are a special kind of Prometheus Rules that trigger alerts based on PromQL expressions. People have gathered several examples under Awesome prometheus alert rules</p> <p>Alerts must be configured in the Prometheus operator helm chart, under the <code>additionalPrometheusRulesMap</code>. For example:</p> <pre><code>additionalPrometheusRulesMap:\n- groups:\n- name: alert-rules\nrules:\n- alert: BlackboxProbeFailed\nexpr: probe_success == 0\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Blackbox probe failed (instance {{ $labels.target }})\"\ndescription: \"Probe failed\\n  VALUE = {{ $value }}\\n  LABELS: {{ $labels }}\"\n</code></pre> <p>Other examples of rules are:</p> <ul> <li>Blackbox Exporter rules</li> </ul>"}, {"location": "devops/prometheus/alertmanager/#silences", "title": "Silences", "text": "<p>To silence an alert with a regular expression use the matcher <code>alertname=~\".*Condition\"</code>.</p>"}, {"location": "devops/prometheus/alertmanager/#references", "title": "References", "text": "<ul> <li>Awesome prometheus alert rules</li> </ul>"}, {"location": "devops/prometheus/blackbox_exporter/", "title": "Blackbox Exporter", "text": "<p>The blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP and ICMP.</p> <p>It can be used to test:</p> <ul> <li>Website accessibility. Both for availability and       security purposes.</li> <li>Website loading time.</li> <li>DNS response times to diagnose network latency issues.</li> <li>SSL certificates expiration.</li> <li>ICMP requests to gather network health information.</li> <li>Security protections such as if and endpoint stops being     protected by VPN, WAF or SSL client certificate.</li> <li>Unauthorized read or write S3 buckets.</li> </ul> <p>When running, the Blackbox exporter is going to expose a HTTP endpoint that can be used in order to monitor targets over the network. By default, the Blackbox exporter exposes the <code>/probe</code> endpoint that is used to retrieve those metrics.</p> <p>The blackbox exporter is configured with a YAML configuration file made of modules.</p>"}, {"location": "devops/prometheus/blackbox_exporter/#installation", "title": "Installation", "text": "<p>To install the exporter we'll use helmfile to install the stable/prometheus-blackbox-exporter chart.</p> <p>Add the following lines to your <code>helmfile.yaml</code>.</p> <pre><code>- name: prometheus-blackbox-exporter\nnamespace: monitoring\nchart: stable/prometheus-blackbox-exporter\nvalues:\n- prometheus-blackbox-exporter/values.yaml\n</code></pre> <p>Edit the chart values. <pre><code>mkdir prometheus-blackbox-exporter\nhelm inspect values stable/prometheus-blackbox-exporter &gt; prometheus-blackbox-exporter/values.yaml\nvi prometheus-blackbox-exporter/values.yaml\n</code></pre></p> <p>Make sure to enable the <code>serviceMonitor</code> in the values and target at least one page:</p> <pre><code>serviceMonitor:\nenabled: true\n\n# Default values that will be used for all ServiceMonitors created by `targets`\ndefaults:\nlabels:\nrelease: prometheus-operator\ninterval: 30s\nscrapeTimeout: 30s\nmodule: http_2xx\n\ntargets:\n- name: lyz-code.github.io/blue-book\nurl: https://lyz-code.github.io/blue-book\n</code></pre> <p>The label <code>release: prometheus-operator</code> must be the one your prometheus instance is searching for.</p> <p>If you want to use the <code>icmp</code> probe, make sure to allow <code>allowIcmp: true</code>.</p> <p>If you want to probe endpoints protected behind client SSL certificates, until this chart issue is solved, you need to create them manually as the Prometheus blackbox exporter helm chart doesn't yet create the required secrets.</p> <pre><code>kubectl create secret generic monitor-certificates \\\n--from-file=monitor.crt.pem \\\n--from-file=monitor.key.pem \\\n-n monitoring\n</code></pre> <p>Where <code>monitor.crt.pem</code> and <code>monitor.key.pem</code> are the SSL certificate and key for the monitor account.</p> <p>I've found two grafana dashboards for the blackbox exporter. <code>7587</code> didn't work straight out of the box while <code>5345</code> did. Taking as reference the grafana helm chart values, add the following yaml under the <code>grafana</code> key in the <code>prometheus-operator</code> <code>values.yaml</code>.</p> <pre><code>grafana:\nenabled: true\ndefaultDashboardsEnabled: true\ndashboardProviders:\ndashboardproviders.yaml:\napiVersion: 1\nproviders:\n- name: 'default'\norgId: 1\nfolder: ''\ntype: file\ndisableDeletion: false\neditable: true\noptions:\npath: /var/lib/grafana/dashboards/default\ndashboards:\ndefault:\nblackbox-exporter:\n# Ref: https://grafana.com/dashboards/5345\ngnetId: 5345\nrevision: 3\ndatasource: Prometheus\n</code></pre> <p>And install.</p> <pre><code>helmfile diff\nhelmfile apply\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-exporter-probes", "title": "Blackbox exporter probes", "text": "<p>Modules define how blackbox exporter is going to query the endpoint, therefore one needs to be created for each request type under the <code>config.modules</code> section of the chart.</p> <p>The modules are then used in the <code>targets</code> section for the desired endpoints.</p> <pre><code>  targets:\n- name: lyz-code.github.io/blue-book\nurl: https://lyz-code.github.io/blue-book\nmodule: https_2xx\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#http-endpoint-working-correctly", "title": "HTTP endpoint working correctly", "text": "<pre><code>http_2xx:\nprober: http\ntimeout: 5s\nhttp:\nvalid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"]\nvalid_status_codes: [200]\nno_follow_redirects: false\npreferred_ip_protocol: \"ip4\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-working-correctly", "title": "HTTPS endpoint working correctly", "text": "<pre><code>https_2xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: GET\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"]\nvalid_status_codes: [200]\nno_follow_redirects: false\npreferred_ip_protocol: \"ip4\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-behind-client-ssl-certificate", "title": "HTTPS endpoint behind client SSL certificate", "text": "<pre><code>https_client_2xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: GET\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"]\nvalid_status_codes: [200]\nno_follow_redirects: false\npreferred_ip_protocol: \"ip4\"\ntls_config:\ncert_file: /etc/secrets/monitor.crt.pem\nkey_file: /etc/secrets/monitor.key.pem\n</code></pre> <p>Where the secrets have been created throughout the installation.</p>"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-with-an-specific-error", "title": "HTTPS endpoint with an specific error", "text": "<p>If you don't want to configure the authentication for example for an API, you can fetch the expected error.</p> <pre><code>https_client_api:\nprober: http\ntimeout: 5s\nhttp:\nmethod: GET\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"]\nvalid_status_codes: [404]\nno_follow_redirects: false\npreferred_ip_protocol: \"ip4\"\nfail_if_body_not_matches_regexp:\n- '.*ERROR route not.*'\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#http-endpoint-returning-an-error", "title": "HTTP endpoint returning an error", "text": "<pre><code>http_4xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: HEAD\nvalid_status_codes: [404, 403]\nvalid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"]\nno_follow_redirects: false\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-through-an-http-proxy", "title": "HTTPS endpoint through an HTTP proxy", "text": "<pre><code>https_external_2xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: GET\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions: [\"HTTP/1.0\", \"HTTP/1.1\", \"HTTP/2.0\"]\nvalid_status_codes: [200]\nno_follow_redirects: false\nproxy_url: \"http://{{ proxy_url }}:{{ proxy_port }}\"\npreferred_ip_protocol: \"ip4\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-with-basic-auth", "title": "HTTPS endpoint with basic auth", "text": "<pre><code>https_basic_auth_2xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: GET\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions:\n- HTTP/1.1\n- HTTP/2.0\nvalid_status_codes:\n- 200\nno_follow_redirects: false\npreferred_ip_protocol: ip4\nbasic_auth:\nusername: {{ username }}\npassword: {{ password }}\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-with-api-key", "title": "HTTPs endpoint with API key", "text": "<pre><code>https_api_2xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: GET\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions:\n- HTTP/1.1\n- HTTP/2.0\nvalid_status_codes:\n- 200\nno_follow_redirects: false\npreferred_ip_protocol: ip4\nheaders:\napikey: {{ api_key }}\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#https-put-file", "title": "HTTPS Put file", "text": "<p>Test if the probe can upload a file.</p> <pre><code>    https_put_file_2xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: PUT\nbody: hi\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"]\nvalid_status_codes: [200]\nno_follow_redirects: false\npreferred_ip_protocol: \"ip4\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#check-open-port", "title": "Check open port", "text": "<pre><code>tcp_connect:\nprober: tcp\n</code></pre> <p>The port is specified when using the module.</p> <pre><code>- name: lyz-code.github.io\nurl: lyz-code.github.io:389\nmodule: tcp_connect\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#ping-to-the-resource", "title": "Ping to the resource", "text": "<p>Test if the target is alive. It's useful When you don't know what port to check or if it uses UDP.</p> <pre><code>ping:\nprober: icmp\ntimeout: 5s\nicmp:\npreferred_ip_protocol: \"ip4\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-exporter-alerts", "title": "Blackbox exporter alerts", "text": "<p>Now that we've got the metrics, we can define the alert rules. Most have been tweaked from the Awesome prometheus alert rules collection.</p> <p>To make security tests</p>"}, {"location": "devops/prometheus/blackbox_exporter/#availability-alerts", "title": "Availability alerts", "text": "<p>The most basic probes, test if the service is up and returning.</p>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-probe-failed", "title": "Blackbox probe failed", "text": "<p>Blackbox probe failed.</p> <pre><code>  - alert: BlackboxProbeFailed\nexpr: probe_success == 0\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Blackbox probe failed (instance {{ $labels.target }})\"\nmessage: \"Probe failed\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_success+%3D%3D+0&amp;g0.tab=1\"\n</code></pre> <p>If you use the security alerts, use the following <code>expr:</code> instead</p> <pre><code>    expr: probe_success{target!~\".*-fail-.*$\"} == 0\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-probe-http-failure", "title": "Blackbox probe HTTP failure", "text": "<p>HTTP status code is not 200-399.</p> <pre><code>  - alert: BlackboxProbeHttpFailure\nexpr: probe_http_status_code &lt;= 199 OR probe_http_status_code &gt;= 400\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Blackbox probe HTTP failure (instance {{ $labels.target }})\"\nmessage: \"HTTP status code is not 200-399\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#performance-alerts", "title": "Performance alerts", "text": ""}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-slow-probe", "title": "Blackbox slow probe", "text": "<p>Blackbox probe took more than 1s to complete.</p> <pre><code>  - alert: BlackboxSlowProbe\nexpr: avg_over_time(probe_duration_seconds[1m]) &gt; 1\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Blackbox slow probe (target {{ $labels.target }})\"\nmessage: \"Blackbox probe took more than 1s to complete\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_duration_seconds%5B1m%5D%29+%3E+1&amp;g0.tab=1\"\n</code></pre> <p>If you use the security alerts, use the following <code>expr:</code> instead</p> <pre><code>    expr: avg_over_time(probe_duration_seconds{,target!~\".*-fail-.*\"}[1m]) &gt; 1\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-probe-slow-http", "title": "Blackbox probe slow HTTP", "text": "<p>HTTP request took more than 1s.</p> <pre><code>  - alert: BlackboxProbeSlowHttp\nexpr: avg_over_time(probe_http_duration_seconds[1m]) &gt; 1\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Blackbox probe slow HTTP (instance {{ $labels.target }})\"\nmessage: \"HTTP request took more than 1s\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_http_duration_seconds%5B1m%5D%29+%3E+1&amp;g0.tab=1\"\n</code></pre> <p>If you use the security alerts, use the following <code>expr:</code> instead</p> <pre><code>    expr: avg_over_time(probe_http_duration_seconds{,target!~\".*-fail-.*\"}[1m]) &gt; 1\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-probe-slow-ping", "title": "Blackbox probe slow ping", "text": "<p>Blackbox ping took more than 1s.</p> <pre><code>  - alert: BlackboxProbeSlowPing\nexpr: avg_over_time(probe_icmp_duration_seconds[1m]) &gt; 1\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Blackbox probe slow ping (instance {{ $labels.target }})\"\nmessage: \"Blackbox ping took more than 1s\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_icmp_duration_seconds%5B1m%5D%29+%3E+1&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#ssl-certificate-alerts", "title": "SSL certificate alerts", "text": ""}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-month", "title": "Blackbox SSL certificate will expire in a month", "text": "<p>SSL certificate expires in 30 days.</p> <pre><code>  - alert: BlackboxSslCertificateWillExpireSoon\nexpr: probe_ssl_earliest_cert_expiry - time() &lt; 86400 * 30\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\"\nmessage: \"SSL certificate expires in 30 days\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-few-days", "title": "Blackbox SSL certificate will expire in a few days", "text": "<p>SSL certificate expires in 3 days.</p> <pre><code>  - alert: BlackboxSslCertificateWillExpireSoon\nexpr: probe_ssl_earliest_cert_expiry - time() &lt; 86400 * 3\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\"\nmessage: \"SSL certificate expires in 3 days\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+3&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-expired", "title": "Blackbox SSL certificate expired", "text": "<p>SSL certificate has expired already.</p> <pre><code>  - alert: BlackboxSslCertificateExpired\nexpr: probe_ssl_earliest_cert_expiry - time() &lt;= 0\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Blackbox SSL certificate expired (instance {{ $labels.target }})\"\nmessage: \"SSL certificate has expired already\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#security-alerts", "title": "Security alerts", "text": "<p>To define the security alerts, I've found easier to create a probe with the action I want to prevent and make sure that the probe fails.</p> <p>This probes contain the <code>-fail-</code> key in the target name, followed by the test it's performing. This convention allows the concatenation of tests. For example, when testing if and endpoint is accessible without basic auth and without vpn we'd use:</p> <pre><code>- name: protected.endpoint.org-fail-without-ssl-and-without-credentials\nurl: protected.endpoint.org\nmodule: https_external_2xx\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-network-policies", "title": "Test endpoints protected with network policies", "text": "<p>Assuming that the blackbox exporter is in the internal network and that there is an http proxy on the external network we want to test. Create a working probe with the <code>https_external_2xx</code> module containing the <code>-fail-without-vpn</code> key in the target name.</p> <pre><code>  - alert: BlackboxVPNProtectionRemoved\nexpr: probe_success{target=~\".*-fail-.*without-vpn.*\"} == 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"VPN protection was removed from (instance {{ $labels.target }})\"\nmessage: \"Successful probe to the endpoint from outside the internal network\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-ssl-client-certificate", "title": "Test endpoints protected with SSL client certificate", "text": "<p>Create a working probe with a module without the SSL client certificate configured, such as <code>https_2xx</code> and set the <code>-fail-without-ssl</code> key in the target name.</p> <pre><code>  - alert: BlackboxClientSSLProtectionRemoved\nexpr: probe_success{target=~\".*-fail-.*without-ssl.*\"} == 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"SSL client certificate protection was removed from (instance {{ $labels.target }})\"\nmessage: \"Successful probe to the endpoint without SSL certificate\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-credentials", "title": "Test endpoints protected with credentials.", "text": "<p>Create a working probe with a module without the basic auth credentials configured, such as <code>https_2xx</code> and set the <code>-fail-without-credentials</code> key in the target name.</p> <pre><code>  - alert: BlackboxCredentialsProtectionRemoved\nexpr: probe_success{target=~\".*-fail-.*without-credentials.*\"} == 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Credentials protection was removed from (instance {{ $labels.target }})\"\nmessage: \"Successful probe to the endpoint without credentials\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-waf", "title": "Test endpoints protected with WAF.", "text": "<p>Create a working probe with a module bypassing the WAF, for example directly attacking the service and set the <code>-fail-without-waf</code> key in the target name.</p> <pre><code>  - alert: BlackboxWAFProtectionRemoved\nexpr: probe_success{target=~\".*-fail-.*without-waf.*\"} == 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"WAF protection was removed from (instance {{ $labels.target }})\"\nmessage: \"Successful probe to the haproxy endpoint from the internal network (bypassed the WAF)\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#unauthorized-read-of-s3-buckets", "title": "Unauthorized read of S3 buckets", "text": "<p>Create a working probe to an existent private object in an S3 bucket and set the <code>-fail-read-object</code> key in the target name.</p> <pre><code>  - alert: BlackboxS3BucketWrongReadPermissions\nexpr: probe_success{target=~\".*-fail-.*read-object.*\"} == 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Wrong read permissions on S3 bucket (instance {{ $labels.target }})\"\nmessage: \"Successful read of a private object with an unauthenticated user\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#unauthorized-write-of-s3-buckets", "title": "Unauthorized write of S3 buckets", "text": "<p>Create a working probe using the <code>https_put_file_2xx</code> module to try to create a file in an S3 bucket and set the <code>-fail-write-object</code> key in the target name.</p> <pre><code>  - alert: BlackboxS3BucketWrongWritePermissions\nexpr: probe_success{target=~\".*-fail-.*write-object.*\"} == 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Wrong write permissions on S3 bucket (instance {{ $labels.target }})\"\nmessage: \"Successful write of a private object with an unauthenticated user\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#monitoring-external-access-to-internal-services", "title": "Monitoring external access to internal services", "text": "<p>There are two possible solutions to simulate traffic from outside your infrastructure to the internal services. Both require the installation of an agent outside of your internal infrastructure, it can be:</p> <ul> <li>An HTTP proxy.</li> <li>A blackbox exporter instance.</li> </ul> <p>Using the proxy you have following advantages:</p> <ul> <li>It's really easy to set up a transparent http     proxy.</li> <li>All probe configuration goes in the same blackbox exporter instance     <code>values.yaml</code>.</li> </ul> <p>With the following disadvantages:</p> <ul> <li> <p>When using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record.</p> <p>The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. * There is no way to do <code>tcp</code> or <code>ping</code> probes to simulate external traffic. * The latency between the blackbox exporter and the proxy is added to all the external probes.</p> </li> </ul> <p>While using an external blackbox exporter gives the following advantages:</p> <ul> <li>Traffic is completely external to the infrastructure, so the proxy     disadvantages would be solved.</li> </ul> <p>And the following disadvantages:</p> <ul> <li> <p>Simulation of external traffic in AWS could be done by spawning the blackbox     exporter instance in another region, but as there is no way of using EKS     worker nodes in different regions, there is no way of managing the exporter     from within Kubernetes. This means:</p> <ul> <li>The loose of the advantages of the Prometheus   operator, so we have to write the configuration   manually.</li> <li>Configuration can't be managed with Helm, so two solutions     should be used to manage the monitorization (Ansible could be used).</li> <li>Even if it's possible to host the second external blackbox exporter within Kubernetes, two independent Helm charts are needed, with the consequent configuration management burden.</li> </ul> </li> </ul> <p>In conclusion, when using a Kubernetes cluster that allows the creation of worker nodes outside the main infrastructure, or if several non HTTP/HTTPS endpoints need to be probed with the <code>tcp</code> or <code>ping</code> modules, install an external blackbox exporter instance. Otherwise install an HTTP proxy and assume that you can only simulate external HTTP/HTTPS traffic.</p>"}, {"location": "devops/prometheus/blackbox_exporter/#troubleshooting", "title": "Troubleshooting", "text": "<p>To get more debugging information of the blackbox probes, add <code>&amp;debug=true</code> to the probe url, for example http://localhost:9115/probe?module=http_2xx&amp;target=https://www.prometheus.io/&amp;debug=true .</p>"}, {"location": "devops/prometheus/blackbox_exporter/#service-monitors-are-not-being-created", "title": "Service monitors are not being created", "text": "<p>When running <code>helmfile apply</code> several times to update the resources, some are not being correctly created. Until the bug is solved, a workaround is to remove the chart release <code>helm delete --purge prometeus-blackbox-exporter</code> and running <code>helmfile apply</code> again.</p>"}, {"location": "devops/prometheus/blackbox_exporter/#probe_success-0-when-using-an-http-proxy", "title": "probe_success == 0 when using an http proxy", "text": "<p>Even when using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record.</p> <p>The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist.</p>"}, {"location": "devops/prometheus/blackbox_exporter/#links", "title": "Links", "text": "<ul> <li>Git.</li> <li>Blackbox exporter modules   configuration.</li> <li>Devconnected introduction to blackbox   exporter.</li> </ul>"}, {"location": "devops/prometheus/instance_sizing_analysis/", "title": "Instance sizing analysis", "text": "<p>Once we gather the instance metrics with the Node exporter, we can do statistical analysis on the evolution of time to detect the instances that are undersized or oversized.</p>"}, {"location": "devops/prometheus/instance_sizing_analysis/#ram-analysis", "title": "RAM analysis", "text": "<p>Instance RAM percent usage metric can be calculated with the following Prometheus rule:</p> <pre><code>  - record: instance_path:node_memory_MemAvailable_percent\nexpr: (1 - node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes ) * 100\n</code></pre> <p>The average, standard deviation and the standard score of the last two weeks would be:</p> <pre><code>  - record: instance_path:node_memory_MemAvailable_percent:avg_over_time_2w\nexpr: avg_over_time(instance_path:node_memory_MemAvailable_percent[2w])\n- record: instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w\nexpr: stddev_over_time(instance_path:node_memory_MemAvailable_percent[2w])\n- record: instance_path:node_memory_MemAvailable_percent:z_score\nexpr: &gt;\n(\ninstance_path:node_memory_MemAvailable_percent\n- instance_path:node_memory_MemAvailable_percent:avg_over_time_2w\n) / instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w\n</code></pre> <p>With that data we can define that an instance is oversized if the average plus the standard deviation is less than 60% and undersized if its greater than 90%.</p> <p>With the average we take into account the nominal RAM consumption, and with the standard deviation we take into account the spikes.</p> <p>Tweak this rule to your use case</p> <p>The criteria of undersized and oversized is just a first approximation I'm going to use. You can use it as a base criteria, but don't go through with it blindly.</p> <p>See the disclaimer below for more information.</p> <pre><code>  # RAM\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w &lt; 60\nlabels:\ntype: EC2\nmetric: RAM\nproblem: oversized\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w &gt; 90\nlabels:\ntype: EC2\nmetric: RAM\nproblem: undersized\n</code></pre> <p>Where <code>avg_plus_stddev_over_time_2w</code> is:</p> <pre><code>  - record: instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w\nexpr: &gt;\ninstance_path:node_memory_MemAvailable_percent:avg_over_time_2w\n+ instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w\n</code></pre>"}, {"location": "devops/prometheus/instance_sizing_analysis/#cpu-analysis", "title": "CPU analysis", "text": "<p>Instance CPU percent usage metric can be calculated with the following Prometheus rule:</p> <pre><code>  - record: instance_path:node_cpu_percent:rate1m\nexpr: &gt;\n(1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100\n</code></pre> <p>The <code>node_cpu_seconds_total</code> doesn't give us the percent of usage, that is why we need to do the average of the <code>rate</code> of the last minute.</p> <p>The average, standard deviation, the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only.</p> CPU usage rules <pre><code># ---------------------------------------\n# -- Resource consumption calculations --\n# ---------------------------------------\n\n# CPU\n- record: instance_path:node_cpu_percent:rate1m\nexpr: &gt;\n(1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100\n- record: instance_path:node_cpu_percent:rate1m:avg_over_time_2w\nexpr: avg_over_time(instance_path:node_cpu_percent:rate1m[2w])\n- record: instance_path:node_cpu_percent:rate1m:stddev_over_time_2w\nexpr: stddev_over_time(instance_path:node_cpu_percent:rate1m[2w])\n- record: instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w\nexpr: &gt;\ninstance_path:node_cpu_percent:rate1m:avg_over_time_2w\n+ instance_path:node_cpu_percent:rate1m:stddev_over_time_2w\n- record: instance_path:node_cpu_percent:rate1m:z_score\nexpr: &gt;\n(\ninstance_path:node_cpu_percent:rate1m\n- instance_path:node_cpu_percent:rate1m:avg_over_time_2w\n) / instance_path:node_cpu_percent:rate1m:stddev_over_time_2w\n\n# ----------------------------------\n# -- Resource sizing calculations --\n# ----------------------------------\n\n# CPU\n- record: instance_path:wrong_resource_size\nexpr: instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w &lt; 60\nlabels:\ntype: EC2\nmetric: CPU\nproblem: oversized\n- record: instance_path:wrong_resource_size\nexpr: instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w &gt; 80\nlabels:\ntype: EC2\nmetric: CPU\nproblem: undersized\n</code></pre>"}, {"location": "devops/prometheus/instance_sizing_analysis/#network-analysis", "title": "Network analysis", "text": "<p>We can deduce the network usage from the <code>node_network_receive_bytes_total</code> and <code>node_network_transmit_bytes_total</code> metrics. For example for the transmit, the Gigabits per second transmitted can be calculated with the following Prometheus rule:</p> <pre><code>  - record: instance_path:node_network_transmit_gigabits_per_second:rate5m\nexpr: &gt;\nincrease(\nnode_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m]\n) * 7.450580596923828 * 10^-9 / 60\n</code></pre> <p>Where we:</p> <ul> <li>Filter the traffic only to the external network interfaces     <code>node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}</code>. Those are the     ones used by AWS, but you'll need to tweak that for your case.</li> <li>Convert the <code>increase</code> of Kilobytes per minute <code>[1m]</code> to Gigabits per second     by multiplying it by <code>7.450580596923828 * 10^-9 / 60</code>.</li> </ul> <p>The average, standard deviation, the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only.</p> Network usage rules <pre><code># ---------------------------------------\n# -- Resource consumption calculations --\n# ---------------------------------------\n\n# NetworkReceive\n- record: instance_path:node_network_receive_gigabits_per_second:rate1m\nexpr: &gt;\nincrease(\nnode_network_receive_bytes_total{device=~\"(eth0|ens.*)\"}[1m]\n) * 7.450580596923828 * 10^-9 / 60\n- record: instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w\nexpr: &gt;\navg_over_time(\ninstance_path:node_network_receive_gigabits_per_second:rate1m[2w]\n)\n- record: instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w\nexpr: &gt;\nstddev_over_time(\ninstance_path:node_network_receive_gigabits_per_second:rate1m[2w]\n)\n- record: instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w\nexpr: &gt;\ninstance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w\n+ instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w\n- record: instance_path:node_network_receive_gigabits_per_second:rate1m:z_score\nexpr: &gt;\n(\ninstance_path:node_network_receive_gigabits_per_second:rate1m\n- instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w\n) / instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w\n\n# NetworkTransmit\n- record: instance_path:node_network_transmit_gigabits_per_second:rate1m\nexpr: &gt;\nincrease(\nnode_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m]\n) * 7.450580596923828 * 10^-9 / 60\n- record: instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w\nexpr: &gt;\navg_over_time(\ninstance_path:node_network_transmit_gigabits_per_second:rate1m[2w]\n)\n- record: instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w\nexpr: &gt;\nstddev_over_time(\ninstance_path:node_network_transmit_gigabits_per_second:rate1m[2w]\n)\n- record: instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w\nexpr: &gt;\ninstance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w\n+ instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w\n- record: instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score\nexpr: &gt;\n(\ninstance_path:node_network_transmit_gigabits_per_second:rate1m\n- instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w\n) / instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w\n\n# ----------------------------------\n# -- Resource sizing calculations --\n# ----------------------------------\n# NetworkReceive\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w &lt; 0.5\nlabels:\ntype: EC2\nmetric: NetworkReceive\nproblem: oversized\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w &gt; 3\nlabels:\ntype: EC2\nmetric: NetworkReceive\nproblem: undersized\n\n# NetworkTransmit\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w &lt; 0.5\nlabels:\ntype: EC2\nmetric: NetworkTransmit\nproblem: oversized\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w &gt; 3\nlabels:\ntype: EC2\nmetric: NetworkTransmit\nproblem: undersized\n</code></pre> <p>The difference with network is that we don't have a percent of the total instance bandwidth, In my case, my instances support from 0.5 to 5 Gbps which is more than I need, so most of my instances are marked as oversized with the <code>&lt; 0.5</code> rule. I will manually study the ones that go over 3 Gbps.</p> <p>The correct way to do it, is to tag the baseline, burst or/and maximum network performance by instance type. In the AWS case, the data can be extracted using the AWS docs or external benchmarks.</p> <p>Once you know the network performance per instance type, you can use relabeling in the Node exporter service monitor to add a label like <code>max_network_performance</code> and use it later in the rules.</p> <p>If you do follow this path, please contact me or do a pull request so I can test your solution.</p>"}, {"location": "devops/prometheus/instance_sizing_analysis/#overall-analysis", "title": "Overall analysis", "text": "<p>Now that we have all the analysis under the metric <code>instance_path:wrong_resource_size</code> with labels, we can aggregate them to see the number of rules each instance is breaking with the following rule:</p> <pre><code>  # Mark the number of oversize rules matched by each instance\n- record: instance_path:wrong_instance_size\nexpr: count by (instance) (sum by (metric, instance) (instance_path:wrong_resource_size))\n</code></pre> <p>By executing <code>sort_desc(instance_path:wrong_instance_size)</code> in the Prometheus web application, we'll be able to see such instances.</p> <pre><code>instance_path:wrong_instance_size{instance=\"frontend-production:192.168.1.2\"}   4\ninstance_path:wrong_instance_size{instance=\"backend-production-instance:172.30.0.195\"}  2\n...\n</code></pre> <p>To see the detail of what rules is our instance breaking we can use something like <code>instance_path:wrong_resource_size{instance =~'frontend.*'}</code></p> <pre><code>instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"RAM\",problem=\"oversized\",type=\"EC2\"} 5.126602454544287\ninstance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",metric=\"CPU\",problem=\"oversized\",type=\"EC2\"}    0.815639209497615\ninstance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkReceive\",problem=\"oversized\",type=\"EC2\"}    0.02973250128744766\ninstance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkTransmit\",problem=\"oversized\",type=\"EC2\"}   0.01586461503849804\n</code></pre> <p>Here we see that the <code>frontend-production</code> is a <code>c4.2xlarge</code> instance that is consuming an average plus standard deviation of CPU of 0.81%, RAM 5.12%, NetworkTransmit 0.015Gbps and NetworkReceive 0.029Gbps, which results in an <code>oversized</code> alert on all four metrics.</p> <p>If you want to see the evolution over the time, instead of <code>Console</code> click on <code>Graph</code> under the text box where you have entered the query.</p> <p>With this information, we can decide which is the correct instance for each application. Once all instances are migrated to their ideal size, we can add alerts on these metrics so we can have a continuous analysis of our instances. Once I've done it, I'll add the alerts here.</p>"}, {"location": "devops/prometheus/instance_sizing_analysis/#disclaimer", "title": "Disclaimer", "text": "<p>We haven't tested this rules yet in production to resize our infrastructure (will do soon), so use all the information in this document cautiously.</p> <p>What I can expect to fail is that the assumption of average plus a standard deviation criteria can not be enough, maybe I need to increase the resolution of the standard deviation so it can be more sensible to the spikes, or we need to use a safety factor of 2 or 3. We'll see :)</p> <p>Read throughly the Gitlab post on anomaly detection using Prometheus, it's awesome and it may give you insights on why this approach is not working with you, as well as other algorithms that for example take into account the seasonality of the metrics.</p> <p>In particular it's interesting to analyze your resources <code>z-score</code> evolution over time, if all values fall in the <code>+4</code> to <code>-4</code> range, you can statistically assert that your metric similarly follows the normal distribution, and can assume that any value of z_score above 3 is an anomaly. If your results return with a range of <code>+20</code> to <code>-20</code>, the tail is too long and your results will be skewed.</p> <p>To test it you can use the following queries to test the RAM behaviour, adapt them for the rest of the resources:</p> <pre><code># Minimum z_score value\n\nsort_desc(abs((min_over_time(instance_path:node_memory_MemAvailable_percent[1w]) - instance_path:node_memory_MemAvailable_percent:avg_over_time_2w) / instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w))\n\n# Maximum z_score value\n\nsort_desc(abs((max_over_time(instance_path:node_memory_MemAvailable_percent[1w]) - instance_path:node_memory_MemAvailable_percent:avg_over_time_2w) / instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w))\n</code></pre> <p>For a less exhaustive but more graphical analysis, execute <code>instance_path:node_memory_MemAvailable_percent:z_score</code> in <code>Graph</code> mode. In my case the RAM is in the +-5 interval, with some peaks of 20, but after reviewing <code>instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w</code> in those periods, I feel it's still safe to use the assumption.</p> <p>Same criteria applies to <code>instance_path:node_cpu_percent:rate1m:z_score</code>, <code>instance_path:node_network_receive_gigabits_per_second:rate1m:z_score</code>, and <code>instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score</code>, metrics.</p>"}, {"location": "devops/prometheus/instance_sizing_analysis/#references", "title": "References", "text": "<ul> <li>Gitlab post on anomaly detection using     Prometheus.</li> </ul>"}, {"location": "devops/prometheus/node_exporter/", "title": "Node Exporter", "text": "<p>Node Exporter is a Prometheus exporter for hardware and OS metrics exposed by *NIX kernels, written in Go with pluggable metric collectors.</p>"}, {"location": "devops/prometheus/node_exporter/#install", "title": "Install", "text": "<p>To install in kubernetes nodes, use this chart. Elsewhere use this ansible role.</p> <p>If you use node exporter agents outside kubernetes, you need to configure a prometheus service discovery to scrap the information from them.</p> <p>To auto discover EC2 instances use the ec2_sd_config configuration. It can be added in the helm chart values.yaml under the key <code>prometheus.prometheusSpec.additionalScrapeConfigs</code></p> <pre><code>      - job_name: node_exporter\nec2_sd_configs:\n- region: us-east-1\nport: 9100\nrefresh_interval: 1m\nrelabel_configs:\n- source_labels: ['__meta_ec2_tag_Name', '__meta_ec2_private_ip']\nseparator: ':'\ntarget_label: instance\n-   source_labels:\n- __meta_ec2_instance_type\ntarget_label: instance_type\n</code></pre> <p>The <code>relabel_configs</code> part will substitute the <code>instance</code> label of each target from <code>{{ instance_ip }}:9100</code> to <code>{{ instance_name }}:{{ instance_ip }}</code>.</p> <p>If the worker nodes already have an IAM role with the <code>ec2:DescribeInstances</code> permission there is no need to specify the <code>role_arn</code> or <code>access_keys</code> and <code>secret_key</code>.</p> <p>If you have stopped instances, the node exporter will raise an alert because it won't be able to scrape the metrics from them. To only fetch data from running instances add a filter:</p> <pre><code>        ec2_sd_configs:\n- region: us-east-1\nfilters:\n- name: instance-state-name\nvalues:\n- running\n</code></pre> <p>To monitor only the instances of a list of VPCs use this filter:</p> <pre><code>        ec2_sd_configs:\n- region: us-east-1\nfilters:\n- name: vpc-id\nvalues:\n- vpc-xxxxxxxxxxxxxxxxx\n- vpc-yyyyyyyyyyyyyyyyy\n</code></pre> <p>By default, prometheus will try to scrape the private instance ip. To use the public one you need to relabel it with the following snippet:</p> <pre><code>        ec2_sd_configs:\n- region: us-east-1\nrelabel_configs:\n- source_labels: ['__meta_ec2_public_ip']\nregex: ^(.*)$\ntarget_label: __address__\nreplacement: ${1}:9100\n</code></pre> <p>I'm using the <code>11074</code> grafana dashboards for the blackbox exporter,  which worked straight out of the box. Taking as reference the grafana helm chart values, add the following yaml under the <code>grafana</code> key in the <code>prometheus-operator</code> <code>values.yaml</code>.</p> <pre><code>grafana:\nenabled: true\ndefaultDashboardsEnabled: true\ndashboardProviders:\ndashboardproviders.yaml:\napiVersion: 1\nproviders:\n- name: 'default'\norgId: 1\nfolder: ''\ntype: file\ndisableDeletion: false\neditable: true\noptions:\npath: /var/lib/grafana/dashboards/default\ndashboards:\ndefault:\nnode_exporter:\n# Ref: https://grafana.com/dashboards/11074\ngnetId: 11074\nrevision: 4\ndatasource: Prometheus\n</code></pre> <p>And install.</p> <pre><code>helmfile diff\nhelmfile apply\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#node-exporter-size-analysis", "title": "Node exporter size analysis", "text": "<p>Once the instance metrics are being ingested, we can do a periodic analysis to deduce which instances are undersized or oversized.</p>"}, {"location": "devops/prometheus/node_exporter/#node-exporter-alerts", "title": "Node exporter alerts", "text": "<p>Now that we've got the metrics, we can define the alert rules. Most have been tweaked from the Awesome prometheus alert rules collection.</p>"}, {"location": "devops/prometheus/node_exporter/#host-out-of-memory", "title": "Host out of memory", "text": "<p>Node memory is filling up (<code>&lt; 10%</code> left).</p> <pre><code>- alert: HostOutOfMemory\nexpr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 &lt; 10\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host out of memory (instance {{ $labels.instance }})\"\nmessage: \"Node memory is filling up (&lt; 10% left)\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-memory-under-memory-pressure", "title": "Host memory under memory pressure", "text": "<p>The node is under heavy memory pressure. High rate of major page faults.</p> <pre><code>- alert: HostMemoryUnderMemoryPressure\nexpr: rate(node_vmstat_pgmajfault[1m]) &gt; 1000\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host memory under memory pressure (instance {{ $labels.instance }})\"\nmessage: \"The node is under heavy memory pressure. High rate of major page faults.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-unusual-network-throughput-in", "title": "Host unusual network throughput in", "text": "<p>Host network interfaces are probably receiving too much data (&gt; 100 MB/s)</p> <pre><code>- alert: HostUnusualNetworkThroughputIn\nexpr: sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 &gt; 100\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host unusual network throughput in (instance {{ $labels.instance }})\"\nmessage: \"Host network interfaces are probably receiving too much data (&gt; 100 MB/s)\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-unusual-network-throughput-out", "title": "Host unusual network throughput out", "text": "<p>Host network interfaces are probably sending too much data (&gt; 100 MB/s)</p> <pre><code>- alert: HostUnusualNetworkThroughputOut\nexpr: sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 &gt; 100\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host unusual network throughput out (instance {{ $labels.instance }})\"\nmessage: \"Host network interfaces are probably sending too much data (&gt; 100 MB/s)\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-unusual-disk-read-rate", "title": "Host unusual disk read rate", "text": "<p>Disk is probably reading too much data (&gt; 50 MB/s)</p> <pre><code>- alert: HostUnusualDiskReadRate\nexpr: sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 &gt; 50\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host unusual disk read rate (instance {{ $labels.instance }})\"\nmessage: \"Disk is probably reading too much data (&gt; 50 MB/s)\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-unusual-disk-write-rate", "title": "Host unusual disk write rate", "text": "<p>Disk is probably writing too much data (&gt; 50 MB/s)</p> <pre><code>- alert: HostUnusualDiskWriteRate\nexpr: sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 &gt; 50\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host unusual disk write rate (instance {{ $labels.instance }})\"\nmessage: \"Disk is probably writing too much data (&gt; 50 MB/s)\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-out-of-disk-space", "title": "Host out of disk space", "text": "<p>Disk is worryingly almost full (<code>&lt; 10% left</code>).</p> <pre><code>- alert: HostOutOfDiskSpace\nexpr: (node_filesystem_avail_bytes{fstype!~\"tmpfs\"}  * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} &lt; 10\nfor: 5m\nlabels:\nseverity: critical\nannotations:\nsummary: \"Host out of disk space (instance {{ $labels.instance }})\"\nmessage: \"Host disk is almost full (&lt; 10% left)\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre> <p>Disk is almost full (<code>&lt; 20% left</code>)</p> <pre><code>- alert: HostReachingOutOfDiskSpace\nexpr: (node_filesystem_avail_bytes{fstype!~\"tmpfs\"}  * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} &lt; 20\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host reaching out of disk space (instance {{ $labels.instance }})\"\nmessage: \"Host disk is almost full (&lt; 20% left)\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-disk-will-fill-in-4-hours", "title": "Host disk will fill in 4 hours", "text": "<p>Disk will fill in 4 hours at current write rate</p> <pre><code>- alert: HostDiskWillFillIn4Hours\nexpr: predict_linear(node_filesystem_free_bytes{fstype!~\"tmpfs\"}[1h], 4 * 3600) &lt; 0\nfor: 5m\nlabels:\nseverity: critical\nannotations:\nsummary: \"Host disk will fill in 4 hours (instance {{ $labels.instance }})\"\nmessage: \"Disk will fill in 4 hours at current write rate\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-out-of-inodes", "title": "Host out of inodes", "text": "<p>Disk is almost running out of available inodes (<code>&lt; 10% left</code>).</p> <pre><code>- alert: HostOutOfInodes\nexpr: node_filesystem_files_free{fstype!~\"tmpfs\"} / node_filesystem_files{fstype!~\"tmpfs\"} * 100 &lt; 10\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host out of inodes (instance {{ $labels.instance }})\"\nmessage: \"Disk is almost running out of available inodes (&lt; 10% left)\\n VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-unusual-disk-read-latency", "title": "Host unusual disk read latency", "text": "<p>Disk latency is growing (read operations &gt; 100ms).</p> <pre><code>- alert: HostUnusualDiskReadLatency\nexpr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) &gt; 100\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host unusual disk read latency (instance {{ $labels.instance }})\"\nmessage: \"Disk latency is growing (read operations &gt; 100ms)\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-unusual-disk-write-latency", "title": "Host unusual disk write latency", "text": "<p>Disk latency is growing (write operations &gt; 100ms)</p> <pre><code>- alert: HostUnusualDiskWriteLatency\nexpr: rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) &gt; 100\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host unusual disk write latency (instance {{ $labels.instance }})\"\nmessage: \"Disk latency is growing (write operations &gt; 100ms)\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-high-cpu-load", "title": "Host high CPU load", "text": "<p>CPU load is &gt; 80%</p> <pre><code>- alert: HostHighCpuLoad\nexpr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) &gt; 80\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host high CPU load (instance {{ $labels.instance }})\"\nmessage: \"CPU load is &gt; 80%\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-context-switching", "title": "Host context switching", "text": "<p>Context switching is growing on node (&gt; 1000 / s)</p> <pre><code># 1000 context switches is an arbitrary number.\n# Alert threshold depends on nature of application.\n# Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58\n- alert: HostContextSwitching\nexpr: (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode=\"idle\"})) &gt; 1000\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host context switching (instance {{ $labels.instance }})\"\nmessage: \"Context switching is growing on node (&gt; 1000 / s)\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-swap-is-filling-up", "title": "Host swap is filling up", "text": "<p>Swap is filling up (&gt;80%)</p> <pre><code>- alert: HostSwapIsFillingUp\nexpr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 &gt; 80\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host swap is filling up (instance {{ $labels.instance }})\"\nmessage: \"Swap is filling up (&gt;80%)\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-systemd-service-crashed", "title": "Host SystemD service crashed", "text": "<p>SystemD service crashed</p> <pre><code>- alert: HostSystemdServiceCrashed\nexpr: node_systemd_unit_state{state=\"failed\"} == 1\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host SystemD service crashed (instance {{ $labels.instance }})\"\nmessage: \"SystemD service crashed\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-physical-component-too-hot", "title": "Host physical component too hot", "text": "<p>Physical hardware component too hot</p> <pre><code>- alert: HostPhysicalComponentTooHot\nexpr: node_hwmon_temp_celsius &gt; 75\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host physical component too hot (instance {{ $labels.instance }})\"\nmessage: \"Physical hardware component too hot\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-node-overtemperature-alarm", "title": "Host node overtemperature alarm", "text": "<p>Physical node temperature alarm triggered</p> <pre><code>- alert: HostNodeOvertemperatureAlarm\nexpr: node_hwmon_temp_alarm == 1\nfor: 5m\nlabels:\nseverity: critical\nannotations:\nsummary: \"Host node overtemperature alarm (instance {{ $labels.instance }})\"\nmessage: \"Physical node temperature alarm triggered\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-raid-array-got-inactive", "title": "Host RAID array got inactive", "text": "<p>RAID array <code>{{ $labels.device }}</code> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.</p> <pre><code>- alert: HostRaidArrayGotInactive\nexpr: node_md_state{state=\"inactive\"} &gt; 0\nfor: 5m\nlabels:\nseverity: critical\nannotations:\nsummary: \"Host RAID array got inactive (instance {{ $labels.instance }})\"\nmessage: \"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-raid-disk-failure", "title": "Host RAID disk failure", "text": "<p>At least one device in RAID array on <code>{{ $labels.instance }}</code> failed. Array <code>{{ $labels.md_device }}</code> needs attention and possibly a disk swap.</p> <pre><code>- alert: HostRaidDiskFailure\nexpr: node_md_disks{state=\"fail\"} &gt; 0\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host RAID disk failure (instance {{ $labels.instance }})\"\nmessage: \"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-kernel-version-deviations", "title": "Host kernel version deviations", "text": "<p>Different kernel versions are running.</p> <pre><code>- alert: HostKernelVersionDeviations\nexpr: count(sum(label_replace(node_uname_info, \"kernel\", \"$1\", \"release\", \"([0-9]+.[0-9]+.[0-9]+).*\")) by (kernel)) &gt; 1\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host kernel version deviations (instance {{ $labels.instance }})\"\nmessage: \"Different kernel versions are running\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-oom-kill-detected", "title": "Host OOM kill detected", "text": "<p>OOM kill detected</p> <pre><code>- alert: HostOomKillDetected\nexpr: increase(node_vmstat_oom_kill[5m]) &gt; 0\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host OOM kill detected (instance {{ $labels.instance }})\"\nmessage: \"OOM kill detected\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-network-receive-errors", "title": "Host Network Receive Errors", "text": "<p><code>{{ $labels.instance }}</code> interface <code>{{ $labels.device }}</code> has encountered <code>{{ printf \"%.0f\" $value }}</code> receive errors in the last five minutes.</p> <pre><code>- alert: HostNetworkReceiveErrors\nexpr: increase(node_network_receive_errs_total[5m]) &gt; 0\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host Network Receive Errors (instance {{ $labels.instance }})\"\nmessage: \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} receive errors in the last five minutes.\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-network-transmit-errors", "title": "Host Network Transmit Errors", "text": "<p><code>{{ $labels.instance }}</code> interface <code>{{ $labels.device }}</code> has encountered <code>{{ printf \"%.0f\" $value }}</code> transmit errors in the last five minutes.</p> <pre><code>- alert: HostNetworkTransmitErrors\nexpr: increase(node_network_transmit_errs_total[5m]) &gt; 0\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host Network Transmit Errors (instance {{ $labels.instance }})\"\nmessage: \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} transmit errors in the last five minutes.\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Prometheus node exporter guide</li> <li>Node exporter alerts</li> </ul>"}, {"location": "devops/prometheus/prometheus/", "title": "Prometheus", "text": "<p>Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy.</p> <p> </p> <p>A quick overview of Prometheus would be, as stated in the coreos article:</p> <p>At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets.</p> <p>The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications.</p> <p>There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented.</p> <p>Go to the Prometheus architecture post for more details.</p> <p>We are living a shift to the DevOps culture, containers and Kubernetes. So nowadays:</p> <ul> <li>Developers need to integrate app and business related metrics as an organic   part of the infrastructure. So monitoring needs to be democratized, made more   accessible and cover additional layers of the stack.</li> <li>Container based infrastructures are changing how we monitor the resources.   Now we have a huge number of volatile software entities, services, virtual   network addresses, exposed metrics that suddenly appear or vanish. Traditional   monitoring tools are not designed to handle this.</li> </ul> <p>These reasons pushed Soundcloud to build a new monitoring system that had the following features</p> <ul> <li>Multi-dimensional data model: The model is based on key-value pairs, similar   to how Kubernetes itself organizes infrastructure metadata using labels. It   allows for flexible and accurate time series data, powering its Prometheus   query language.</li> <li>Accessible format and protocols: Exposing prometheus metrics is a pretty   straightforward task. Metrics are human readable, are in a self-explanatory   format, and are published using a standard HTTP transport. You can check that   the metrics are correctly exposed just using your web browser.</li> <li>Service discovery: The Prometheus server is in charge of periodically   scraping the targets, so that applications and services don\u2019t need to worry   about emitting data (metrics are pulled, not pushed). These Prometheus servers   have several methods to auto-discover scrape targets, some of them can be   configured to filter and match container metadata, making it an excellent fit   for ephemeral Kubernetes workloads.</li> <li>Modular and highly available components: Metric collection, alerting,   graphical visualization, etc, are performed by different composable services.   All these services are designed to support redundancy and sharding.</li> <li>Pull based metrics: Most monitoring systems are pushing metrics to     a centralized collection platform. Prometheus flips this model on it's head     with the following advantages:<ul> <li>No need to install custom software in the physical servers or containers.</li> <li>Doesn't require applications to use CPU cycles pushing metrics.</li> <li>Handles service failure/unavailability gracefully. If a target goes down,     Prometheus can record it was unable to retrieve data.</li> <li>You can use the Pushgateway if pulling metrics is not feasible.</li> </ul> </li> </ul>"}, {"location": "devops/prometheus/prometheus/#installation", "title": "Installation", "text": "<p>There are several ways to install prometheus, but I'd recommend using the Kubernetes or Docker Prometheus operator.</p>"}, {"location": "devops/prometheus/prometheus/#exposing-your-metrics", "title": "Exposing your metrics", "text": "<p>Prometheus defines a very nice text-based format for its metrics:</p> <pre><code># HELP prometheus_engine_query_duration_seconds Query timings\n# TYPE prometheus_engine_query_duration_seconds summary\nprometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 7.0442e-05\nprometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.0084092\nprometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.389403939\n</code></pre> <p>The data is relatively human readable and we even have TYPE and HELP decorators to increase the readability.</p> <p>To expose application metrics to the Prometheus server, use one of the client libraries and follow the suggested naming and units conventions for metrics.</p>"}, {"location": "devops/prometheus/prometheus/#metric-types", "title": "Metric types", "text": "<p>There are these metric types:</p> <ul> <li>Counter: A simple monotonically incrementing type; basically use this for   situations where you want to know \u201chow many times has x happened\u201d.</li> <li>Gauge: A representation of a metric that can go both up and down. Think of   a speedometer in a car, this type provides a snapshot of \u201cwhat is the current   value of x now\u201d.</li> <li>Histogram: It represents observed metrics sharded into distinct buckets.   Think of this as a mechanism to track \u201chow long something took\u201d or \u201chow big   something was\u201d.</li> <li>Summary: Similar to a histogram, except the bins are converted into an     aggregate immediately.</li> </ul>"}, {"location": "devops/prometheus/prometheus/#using-labels", "title": "Using labels", "text": "<p>Prometheus metrics support the concept of Labels to provide extra dimensions to your data. By using Labels efficiently we can essentially provide more insights into our data whilst having to manage less actual metrics.</p>"}, {"location": "devops/prometheus/prometheus/#prometheus-rules", "title": "Prometheus rules", "text": "<p>Prometheus supports two types of rules which may be configured and then evaluated at regular intervals: recording rules and alerting rules.</p> <p>Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed.</p> <p>A simple example rules file would be:</p> <pre><code>groups:\n- name: example\nrules:\n- record: job:http_inprogress_requests:sum\nexpr: sum by (job) (http_inprogress_requests)\n</code></pre> <p>Regarding naming and aggregation conventions, Recording rules should be of the general form <code>level:metric:operations</code>. <code>level</code> represents the aggregation level and labels of the rule output. <code>metric</code> is the metric name and should be unchanged other than stripping <code>_total</code> off counters when using <code>rate()</code> or <code>irate()</code>. <code>operations</code> is a list of operations (splitted by <code>:</code>) that were applied to the metric, newest operation first.</p> <p>If you want to add extra labels to the calculated rule use the <code>labels</code> tag like the following example:</p> <pre><code>groups:\n- name: example\nrules:\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w &lt; 60\nlabels:\ntype: EC2\nmetric: RAM\nproblem: oversized\n</code></pre>"}, {"location": "devops/prometheus/prometheus/#finding-a-metric", "title": "Finding a metric", "text": "<p>You can use <code>{__name__=~\".*deploy.*\"}</code> to find the metrics that have <code>deploy</code> somewhere in the name.</p>"}, {"location": "devops/prometheus/prometheus/#accessing-prometheus-metrics-through-python", "title": "Accessing Prometheus metrics through python", "text": "<pre><code>import requests\n\nresponse = requests.get(\n    \"http://127.0.0.1:9090/api/v1/query\",\n    params={\"query\": \"container_cpu_user_seconds_total\"},\n)\n</code></pre>"}, {"location": "devops/prometheus/prometheus/#links", "title": "Links", "text": "<ul> <li>Homepage.</li> <li>Docs.</li> <li>Awesome Prometheus.</li> <li>Prometheus rules best     practices     and configuration.</li> </ul>"}, {"location": "devops/prometheus/prometheus/#diving-deeper", "title": "Diving deeper", "text": "<ul> <li>Architecture</li> <li>Prometheus Operator</li> <li>Prometheus Installation</li> <li>Blackbox Exporter</li> <li>Node Exporter</li> <li>Prometheus Troubleshooting</li> </ul>"}, {"location": "devops/prometheus/prometheus/#introduction-posts", "title": "Introduction posts", "text": "<ul> <li>Soundcloud   introduction.</li> <li>Sysdig guide.</li> <li>Prometheus monitoring solutions   comparison.</li> <li>ITNEXT overview</li> </ul>"}, {"location": "devops/prometheus/prometheus/#books", "title": "Books", "text": "<ul> <li>Prometheus Up &amp; Running.</li> <li>Monitoring With Prometheus.</li> </ul>"}, {"location": "devops/prometheus/prometheus_architecture/", "title": "Prometheus architecture", "text": ""}, {"location": "devops/prometheus/prometheus_architecture/#prometheus-server", "title": "Prometheus Server", "text": "<p>Prometheus servers have the following assignments:</p> <ul> <li>Periodically scrape and store metrics from instrumented jobs, either directly   or via an intermediary push gateway for short-lived jobs.</li> <li>Run rules over scraped data to either record new timeseries from existing data or   generate alerts.</li> <li>Discovers new targets from the Service discovery.</li> <li>Push alerts to the Alertmanager.</li> <li>Executes PromQL queries.</li> </ul>"}, {"location": "devops/prometheus/prometheus_architecture/#prometheus-targets", "title": "Prometheus Targets", "text": "<p>Prometheus Targets define how does prometheus extract the metrics from the different sources.</p> <p>If the services expose the metrics themselves such as Kubernetes, Prometheus fetch them directly. On the other cases, exporters are used.</p> <p>Exporters are modules that extract information and translate it into the Prometheus format, which the server can then ingest. There are several exporters available, for example:</p> <ul> <li>Hardware: Node/system</li> <li>HTTP: HAProxy,     NGINX,     Apache.</li> <li>APIs: Github, Docker     Hub.</li> <li>Other monitoring systems:     Cloudwatch.</li> <li>Databases: MySQL,     Elasticsearch.</li> <li>Messaging systems: RabbitMQ,     Kafka.</li> <li>Miscellaneous: Blackbox,     JMX.</li> </ul>"}, {"location": "devops/prometheus/prometheus_architecture/#pushgateway", "title": "Pushgateway", "text": "<p>In case the nodes are not exposing an endpoint from which the Prometheus server can collect the metrics, the Prometheus ecosystem has a push gateway.</p> <p>This gateway API is useful for one-off jobs that run, capture the data, transform that data into the Prometheus data format and then push that data into the Prometheus server.</p>"}, {"location": "devops/prometheus/prometheus_architecture/#service-discovery", "title": "Service discovery", "text": "<p>Prometheus is designed to require very little configuration when first setup, and was designed from the ground up to run in dynamic environments such Kubernetes. It therefore performs automatic discovery of services running to try and make a \u201cbest guess\u201d of what it should be monitoring.</p>"}, {"location": "devops/prometheus/prometheus_architecture/#alertmanager", "title": "Alertmanager", "text": "<p>The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.</p>"}, {"location": "devops/prometheus/prometheus_architecture/#data-visualization-and-export", "title": "Data visualization and export", "text": "<p>There are several ways to visualize or export data from Prometheus.</p>"}, {"location": "devops/prometheus/prometheus_architecture/#prometheus-web-ui", "title": "Prometheus web UI", "text": "<p>Prometheus comes with its own user interface that you can use to:</p> <ul> <li>Run PromQL queries.</li> <li>Check the Alertmanager rules.</li> <li>Check the configuration.</li> <li>Check the Targets.</li> <li>Check the service discovery.</li> </ul>"}, {"location": "devops/prometheus/prometheus_architecture/#grafana", "title": "Grafana", "text": "<p>Grafana is the best way to visually analyze the evolution of the metrics throughout time.</p>"}, {"location": "devops/prometheus/prometheus_architecture/#api-clients", "title": "API clients", "text": "<p>Prometheus also exposes an API, so in case you are interested in writing your own clients, you can do that too.</p>"}, {"location": "devops/prometheus/prometheus_architecture/#links", "title": "Links", "text": "<ul> <li>Prometheus Overview</li> <li>Open Source for U architecture overview</li> </ul>"}, {"location": "devops/prometheus/prometheus_installation/", "title": "Prometheus Installation", "text": ""}, {"location": "devops/prometheus/prometheus_installation/#kubernetes", "title": "Kubernetes", "text": "<p>Helm 2 is not supported anymore.</p> <p>Later versions of the chart return an Error: apiVersion 'v2' is not valid. The value must be \"v1\" when using helm 2.</p> <p>Diving deeper, it seems that from 11.1.7 support for helm 2 was dropped.</p> <p>To install the operator we'll use helmfile to install the stable/prometheus-operator chart.</p> <p>Add the following lines to your <code>helmfile.yaml</code>. <pre><code>- name: prometheus-operator\nnamespace: monitoring\nchart: stable/prometheus-operator\nvalues:\n- prometheus-operator/values.yaml\n</code></pre></p> <p>Edit the chart values. <pre><code>mkdir prometheus-operator\nhelm inspect values stable/prometheus-operator &gt; prometheus-operator/values.yaml\nvi prometheus-operator/values.yaml\n</code></pre></p> <p>I've implemented the following changes:</p> <ul> <li> <p>If you are using a managed solution like EKS, the provider will hide     <code>kube-scheduler</code> and <code>kube-controller-manager</code> so those metrics will fail.     Therefore you need to disable:</p> <ul> <li><code>defaultRules.rules.kubeScheduler: false</code>.</li> <li><code>kubeScheduler.enabled: false</code>.</li> <li><code>kubeControllerManager.enabled: false</code>.</li> <li>Enabled the ingress of <code>alertmanager</code>, <code>grafana</code> and <code>prometheus</code>.</li> <li>Set up the <code>storage</code> of <code>alertmanager</code> and <code>prometheus</code> with   <code>storageClassName: gp2</code> (for AWS).</li> <li>Change <code>additionalPrometheusRules</code> to <code>additionalPrometheusRulesMap</code> as the former is going to be deprecated in future releases.</li> <li> <p>For private clusters, disable the admission   webhook.</p> </li> <li> <p><code>prometheusOperator.admissionWebhooks.enabled=false</code></p> </li> <li><code>prometheusOperator.admissionWebhooks.patch.enabled=false</code></li> <li><code>prometheusOperator.tlsProxy.enabled=false</code></li> </ul> </li> </ul> <p>And install.</p> <pre><code>helmfile diff\nhelmfile apply\n</code></pre> <p>Once it's installed you can check everything is working by accessing the grafana dashboard.</p> <p>First of all get the pod name (we'll asume you've used the <code>monitoring</code> namespace). <pre><code>kubectl get pods -n monitoring | grep grafana\n</code></pre></p> <p>Then set up the proxies <pre><code>kubectl port-forward {{ grafana_pod }} -n monitoring 3000:3000\nkubectl port-forward -n monitoring \\\nprometheus-prometheus-operator-prometheus-0 9090:9090\n</code></pre></p> <p>To access grafana, go to http://localhost:3000 through your browser and at the top left, click on Home and select any dashboard. To access prometheus, go to http://localhost:9090.</p> <p>If you're using the EKS helm chart, you'll need to manually edit the kube-proxy-config configmap until this bug has been solved.</p> <p>Edit the <code>127.0.0.1</code> value to <code>0.0.0.0</code> for the key <code>metricsBindAddress</code> in <pre><code>kubectl -n kube-system edit cm kube-proxy-config\n</code></pre></p> <p>And restart the DaemonSet: <pre><code>kubectl rollout restart -n kube-system daemonset.apps/kube-proxy\n</code></pre></p>"}, {"location": "devops/prometheus/prometheus_installation/#upgrading-notes", "title": "Upgrading notes", "text": ""}, {"location": "devops/prometheus/prometheus_installation/#10x-1117", "title": "10.x -&gt; 11.1.7", "text": "<p>If you have a private cluster in EKS, you are not able to use the admission webhooks as the nodes are not able to reach the master.</p> <p>Between those versions, something changed and you need to disable tls too with:</p> <pre><code>prometheusOperator:\ntls:\nenabled: false\nadmissionWebhooks:\nenabled: false\n</code></pre> <p>If you run <code>helmfile apply</code> without that flag, the deployment gets tainted, and you may need to edit the deployment to remove the <code>tls-secret</code> volume.</p>"}, {"location": "devops/prometheus/prometheus_installation/#docker", "title": "Docker", "text": "<p>To install it outside Kubernetes, use the cloudalchemy ansible role for host installations or the prom/prometheus docker with the following command:</p> <pre><code>/usr/bin/docker run --rm \\\n--name prometheus \\\n-v /data/prometheus:/etc/prometheus \\\nprom/prometheus:latest \\\n--storage.tsdb.retention.time=30d \\\n--config.file=/etc/prometheus/prometheus.yml \\\n</code></pre> <p>With a basic prometheus configuration:</p> <p>File: /data/prometheus/prometheus.yml</p> <p>And some basic rules:</p> File: /data/prometheus/rules/node_exporter.yaml <pre><code>groups:\n- name: ansible managed alert rules\nrules:\n- alert: Watchdog\nannotations:\ndescription: |-\nThis is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\nsummary: Ensure entire alerting pipeline is functional\nexpr: vector(1)\nfor: 10m\nlabels:\nseverity: warning\n- alert: InstanceDown\nannotations:\ndescription: '{{ $labels.instance }} of job {{ $labels.job }} has been down for\nmore than 5 minutes.'\nsummary: Instance {{ $labels.instance }} down\nexpr: up == 0\nfor: 5m\nlabels:\nseverity: critical\n- alert: RebootRequired\nannotations:\ndescription: '{{ $labels.instance }} requires a reboot.'\nsummary: Instance {{ $labels.instance }} - reboot required\nexpr: node_reboot_required &gt; 0\nlabels:\nseverity: warning\n- alert: NodeFilesystemSpaceFillingUp\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available space left and is filling up.\nsummary: Filesystem is predicted to run out of space within the next 24 hours.\nexpr: |-\n(\nnode_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 &lt; 40\nand\npredict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 24*60*60) &lt; 0\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: warning\n- alert: NodeFilesystemSpaceFillingUp\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available space left and is filling up fast.\nsummary: Filesystem is predicted to run out of space within the next 4 hours.\nexpr: |-\n(\nnode_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 &lt; 20\nand\npredict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 4*60*60) &lt; 0\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: critical\n- alert: NodeFilesystemAlmostOutOfSpace\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available space left.\nsummary: Filesystem has less than 5% space left.\nexpr: |-\n(\nnode_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 &lt; 5\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: warning\n- alert: NodeFilesystemAlmostOutOfSpace\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available space left.\nsummary: Filesystem has less than 3% space left.\nexpr: |-\n(\nnode_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 &lt; 3\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: critical\n- alert: NodeFilesystemFilesFillingUp\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available inodes left and is filling up.\nsummary: Filesystem is predicted to run out of inodes within the next 24 hours.\nexpr: |-\n(\nnode_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 &lt; 40\nand\npredict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 24*60*60) &lt; 0\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: warning\n- alert: NodeFilesystemFilesFillingUp\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast.\nsummary: Filesystem is predicted to run out of inodes within the next 4 hours.\nexpr: |-\n(\nnode_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 &lt; 20\nand\npredict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 4*60*60) &lt; 0\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: critical\n- alert: NodeFilesystemAlmostOutOfFiles\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available inodes left.\nsummary: Filesystem has less than 5% inodes left.\nexpr: |-\n(\nnode_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 &lt; 5\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: warning\n- alert: NodeFilesystemAlmostOutOfFiles\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available inodes left.\nsummary: Filesystem has less than 3% inodes left.\nexpr: |-\n(\nnode_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 &lt; 3\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: critical\n- alert: NodeNetworkReceiveErrs\nannotations:\ndescription: '{{ $labels.instance }} interface {{ $labels.device }} has encountered\n{{ printf \"%.0f\" $value }} receive errors in the last two minutes.'\nsummary: Network interface is reporting many receive errors.\nexpr: |-\nincrease(node_network_receive_errs_total[2m]) &gt; 10\nfor: 1h\nlabels:\nseverity: warning\n- alert: NodeNetworkTransmitErrs\nannotations:\ndescription: '{{ $labels.instance }} interface {{ $labels.device }} has encountered\n{{ printf \"%.0f\" $value }} transmit errors in the last two minutes.'\nsummary: Network interface is reporting many transmit errors.\nexpr: |-\nincrease(node_network_transmit_errs_total[2m]) &gt; 10\nfor: 1h\nlabels:\nseverity: warning\n- alert: NodeHighNumberConntrackEntriesUsed\nannotations:\ndescription: '{{ $value | humanizePercentage }} of conntrack entries are used'\nsummary: Number of conntrack are getting close to the limit\nexpr: |-\n(node_nf_conntrack_entries / node_nf_conntrack_entries_limit) &gt; 0.75\nlabels:\nseverity: warning\n- alert: NodeClockSkewDetected\nannotations:\nmessage: Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure\nNTP is configured correctly on this host.\nsummary: Clock skew detected.\nexpr: |-\n(\nnode_timex_offset_seconds &gt; 0.05\nand\nderiv(node_timex_offset_seconds[5m]) &gt;= 0\n)\nor\n(\nnode_timex_offset_seconds &lt; -0.05\nand\nderiv(node_timex_offset_seconds[5m]) &lt;= 0\n)\nfor: 10m\nlabels:\nseverity: warning\n- alert: NodeClockNotSynchronising\nannotations:\nmessage: Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured\non this host.\nsummary: Clock not synchronising.\nexpr: |-\nmin_over_time(node_timex_sync_status[5m]) == 0\nfor: 10m\nlabels:\nseverity: warning\n</code></pre>"}, {"location": "devops/prometheus/prometheus_installation/#yaml", "title": "```yaml", "text": ""}, {"location": "devops/prometheus/prometheus_installation/#httpprometheusiodocsoperatingconfiguration", "title": "http://prometheus.io/docs/operating/configuration/", "text": "<p>global:   evaluation_interval: 1m   scrape_interval: 1m   scrape_timeout: 10s   external_labels:     environment: helm rule_files:   - /etc/prometheus/rules/*.yaml scrape_configs:   - job_name: prometheus     metrics_path: /metrics     static_configs:     - targets:       - prometheus:9090  ```</p>"}, {"location": "devops/prometheus/prometheus_installation/#next-steps", "title": "Next steps", "text": "<ul> <li>Configure the alertmanager alerts.</li> <li>Configure the Blackbox Exporter.</li> <li>Configure the grafana dashboards.</li> </ul>"}, {"location": "devops/prometheus/prometheus_installation/#issues", "title": "Issues", "text": "<ul> <li>Error: apiVersion 'v2' is not valid.  The value must be     \"v1\":     Update the warning above and update the clusters.</li> </ul>"}, {"location": "devops/prometheus/prometheus_operator/", "title": "Prometheus Operator", "text": "<p>Prometheus has it's own kubernetes operator, which makes it simple to install with helm, and enables users to configure and manage instances of Prometheus using simple declarative configuration that will, in response, create, configure, and manage Prometheus monitoring instances.</p> <p>Once installed the Prometheus Operator provides the following features:</p> <ul> <li> <p>Create/Destroy: Easily launch a Prometheus instance for your Kubernetes   namespace, a specific application or team easily using the Operator.</p> </li> <li> <p>Simple Configuration: Configure the fundamentals of Prometheus like   versions, persistence, retention policies, and replicas from a native   Kubernetes resource.</p> </li> <li> <p>Target Services via Labels: Automatically generate monitoring target   configurations based on familiar Kubernetes label queries; no need to learn   a Prometheus specific configuration language.</p> </li> </ul>"}, {"location": "devops/prometheus/prometheus_operator/#how-it-works", "title": "How it works", "text": "<p>The core idea of the Operator is to decouple deployment of Prometheus instances from the configuration of which entities they are monitoring.</p> <p>The Operator acts on the following custom resource definitions (CRDs):</p> <ul> <li>Prometheus: Defines the desired Prometheus deployment. The Operator ensures   at all times that a deployment matching the resource definition is running.   This entails aspects like the data retention time, persistent volume claims,   number of replicas, the Prometheus version, and Alertmanager instances to send   alerts to.</li> <li>ServiceMonitor: Specifies how metrics can be retrieved from a set of   services exposing them in a common way. The Operator configures the Prometheus   instance to monitor all services covered by included ServiceMonitors and keeps   this configuration synchronized with any changes happening in the cluster.</li> <li>PrometheusRule: Defines a desired Prometheus rule file, which can be loaded   by a Prometheus instance containing Prometheus alerting and recording rules.</li> <li>Alertmanager: Defines a desired Alertmanager deployment. The Operator   ensures at all times that a deployment matching the resource definition is   running.</li> </ul> <p></p>"}, {"location": "devops/prometheus/prometheus_operator/#links", "title": "Links", "text": "<ul> <li>Homepage</li> <li>CoreOS Prometheus operator presentation</li> <li>Sysdig Prometheus operator guide part 3</li> </ul>"}, {"location": "devops/prometheus/prometheus_troubleshooting/", "title": "Prometheus Troubleshooting", "text": "<p>Solutions for problems with Prometheus.</p>"}, {"location": "devops/prometheus/prometheus_troubleshooting/#service-monitor-not-being-recognized", "title": "Service monitor not being recognized", "text": "<p>Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for <code>Service Monitor Selector</code>.</p> <pre><code>kubectl get prometheus -n monitoring\nkubectl describe prometheus prometheus-operator-prometheus -n monitoring\n</code></pre> <p>The last one will return something like:</p> <pre><code>  Service Monitor Selector:\nMatch Labels:\nRelease:  prometheus-operator\n</code></pre> <p>Which means you need to label your service monitors with <code>release: prometheus-operator</code>, be careful if you use <code>Release: prometheus-operator</code> it won't work.</p>"}, {"location": "devops/prometheus/prometheus_troubleshooting/#failed-calling-webhook-prometheusrulemutatemonitoringcoreoscom", "title": "Failed calling webhook prometheusrulemutate.monitoring.coreos.com", "text": "<pre><code>  Error: UPGRADE FAILED: failed to create resource: Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-operator.monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: no endpoints available for service \"prometheus-operator-operator\"\n</code></pre> <p>Since version 0.30 of the operator, there is an admission webhook to prevent malformed rules from being added to the cluster. Without this validation, creating an invalid resource will cause Prometheus not to load it. If the container then restarts, it will go into a crashloop.</p> <p>For the webhook to work, the control plane needs to be able to access the webhook service. That means the addition of a firewall rule in EKS and GKE deployments. People have succeeded with GKE, but people struggling with EKS have decided to disable the webhook.</p> <p>To disable it, the following options have to be set:</p> <ul> <li><code>prometheusOperator.admissionWebhooks.enabled=false</code></li> <li><code>prometheusOperator.admissionWebhooks.patch.enabled=false</code></li> <li><code>prometheusOperator.tlsProxy.enabled=false</code></li> </ul> <p>If you have deployed your release with the webhook enabled, you also need to remove all the resources that match the following:</p> <pre><code>kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io\nkubectl get MutatingWebhookConfiguration\n</code></pre> <p>Before executing <code>helmfile apply</code> again.</p>"}, {"location": "linux/brew/", "title": "Brew", "text": "<p>Complementary package manager to manage the programs that aren't in the Debian repositories.</p>"}, {"location": "linux/brew/#usage", "title": "Usage", "text": "<p>TBC</p>"}, {"location": "linux/brew/#references", "title": "References", "text": "<ul> <li>Homebrew formula for a Go app</li> </ul>"}, {"location": "linux/cookiecutter/", "title": "Cookiecutter", "text": "<p>Cookiecutter is a command-line utility that creates projects from cookiecutters (project templates).</p>"}, {"location": "linux/cookiecutter/#install", "title": "Install", "text": "<pre><code>pip install cookiecutter\n</code></pre>"}, {"location": "linux/cookiecutter/#use", "title": "Use", "text": "<p>DEPRECATION: use cruft instead</p> <p>You may want to use cruft to generate your templates instead, as it will help you maintain the project with the template updates. Something that it's not easy with cookiecutter alone</p> <pre><code>cookiecutter {{ path_or_url_to_cookiecutter_template }}\n</code></pre>"}, {"location": "linux/cookiecutter/#user-config", "title": "User config", "text": "<p>If you use Cookiecutter a lot, you\u2019ll find it useful to have a user config file. By default Cookiecutter tries to retrieve settings from a <code>.cookiecutterrc</code> file in your home directory.</p> <p>Example user config:</p> <pre><code>default_context:\nfull_name: \"Audrey Roy\"\nemail: \"audreyr@example.com\"\ngithub_username: \"audreyr\"\ncookiecutters_dir: \"/home/audreyr/my-custom-cookiecutters-dir/\"\nreplay_dir: \"/home/audreyr/my-custom-replay-dir/\"\nabbreviations:\npython: https://github.com/audreyr/cookiecutter-pypackage.git\ngh: https://github.com/{0}.git\nbb: https://bitbucket.org/{0}\n</code></pre> <p>Possible settings are:</p> <code>default_context</code> A list of key/value pairs that you want injected as context whenever you generate a project with Cookiecutter. These values are treated like the defaults in <code>cookiecutter.json</code>, upon generation of any project. <code>cookiecutters_dir</code> Directory where your cookiecutters are cloned to when you use Cookiecutter with a repo argument. <code>replay_dir</code> Directory where Cookiecutter dumps context data to, which you can fetch later on when using the replay feature. <code>abbreviations</code> A list of abbreviations for cookiecutters. Abbreviations can be simple aliases for a repo name, or can be used as a prefix, in the form <code>abbr:suffix</code>. Any suffix will be inserted into the expansion in place of the text <code>{0}</code>, using standard Python string formatting. With the above aliases, you could use the <code>cookiecutter-pypackage</code> template simply by saying cookiecutter <code>python</code>."}, {"location": "linux/cookiecutter/#write-your-own-cookietemplates", "title": "Write your own cookietemplates", "text": ""}, {"location": "linux/cookiecutter/#create-files-or-directories-with-conditions", "title": "Create files or directories with conditions", "text": "<p>For files use a filename like <code>'{{ \".vault_pass.sh\" if cookiecutter.vault_pass_entry != \"None\" else \"\" }}'</code>.</p> <p>For directories I haven't yet found a nice way to do it (as the above will fail), check the issue or the hooks documentation for more information.</p> <p>File: <code>post_gen_project.py</code></p> <pre><code>import os\nimport sys\n\nREMOVE_PATHS = [\n    '{% if cookiecutter.packaging != \"pip\" %} requirements.txt {% endif %}',\n    '{% if cookiecutter.packaging != \"poetry\" %} poetry.lock {% endif %}',\n]\n\nfor path in REMOVE_PATHS:\n    path = path.strip()\n    if path and os.path.exists(path):\n        if os.path.isdir(path):\n            os.rmdir(path)\n        else:\n            os.unlink(path)\n</code></pre>"}, {"location": "linux/cookiecutter/#add-some-text-to-a-file-if-a-condition-is-met", "title": "Add some text to a file if a condition is met", "text": "<p>Use jinja2 conditionals. Note the <code>-</code> at the end of the conditional opening, play with <code>{%- ... -%}</code> and <code>{% ... %}</code> for different results on line appending.</p> <pre><code>{% if cookiecutter.install_docker == 'yes' -%}\n- src: git+ssh://mywebpage.org/ansible-roles/docker.git\nversion: 1.0.3\n{%- else -%}\n- src: git+ssh://mywebpage.org/ansible-roles/other-role.git\nversion: 1.0.2\n{%- endif %}\n</code></pre>"}, {"location": "linux/cookiecutter/#initialize-git-repository-on-the-created-cookiecutter", "title": "Initialize git repository on the created cookiecutter", "text": "<p>Added the following to the post generation hooks.</p> <p>File: hooks/post_gen_project.py</p> <pre><code>import subprocess\n\nsubprocess.call(['git', 'init'])\nsubprocess.call(['git', 'add', '*'])\nsubprocess.call(['git', 'commit', '-m', 'Initial commit'])\n</code></pre>"}, {"location": "linux/cookiecutter/#prevent-cookiecutter-from-processing-some-files", "title": "Prevent cookiecutter from processing some files", "text": "<p>By default cookiecutter will try to process every file as a Jinja template. This behaviour produces wrong results if you have Jinja templates that are meant to be taken as literal. Starting with cookiecutter 1.1, you can tell cookiecutter to only copy some files without interpreting them as Jinja templates.</p> <p>Add a <code>_copy_without_render</code> key in the cookiecutter config file (<code>cookiecutter.json</code>). It takes a list of regular expressions. If a filename matches the regular expressions it will be copied and not processed as a Jinja template.</p> <pre><code>{\n\"project_slug\": \"sample\",\n\"_copy_without_render\": [\n\"*.js\",\n\"not_rendered_dir/*\",\n\"rendered_dir/not_rendered_file.ini\"\n]\n}\n</code></pre>"}, {"location": "linux/cookiecutter/#prevent-additional-whitespaces-when-jinja-condition-is-not-met", "title": "Prevent additional whitespaces when jinja condition is not met.", "text": "<p>Jinja2 has a whitespace control that can be used to manage the whitelines existent between the Jinja blocks. The problem comes when a condition is not met in an <code>if</code> block, in that case, Jinja adds a whitespace which will break most linters.</p> <p>This is the solution I've found out that works as expected.</p> <pre><code>### Multienvironment\n\nThis playbook has support for the following environments:\n\n{% if cookiecutter.production_environment == \"True\" -%}\n* Production\n{% endif %}\n{%- if cookiecutter.staging_environment == \"True\" -%}\n* Staging\n{% endif %}\n{%- if cookiecutter.development_environment == \"True\" -%}\n* Development\n{% endif %}\n### Tags\n</code></pre>"}, {"location": "linux/cookiecutter/#testing-your-own-cookiecutter-templates", "title": "Testing your own cookiecutter templates", "text": "<p>The pytest-cookies plugin comes with a <code>cookies</code> fixture which is a wrapper for the cookiecutter API for generating projects. It helps you verify that your template is working as expected and takes care of cleaning up after running the tests.</p>"}, {"location": "linux/cookiecutter/#install_1", "title": "Install", "text": "<pre><code>pip install pytest-cookies\n</code></pre>"}, {"location": "linux/cookiecutter/#usage", "title": "Usage", "text": "<p>@pytest.fixture def context():     return {         \"playbook_name\": \"My Test Playbook\",     }</p> <p>The <code>cookies.bake()</code> method generates a new project from your template based on the default values specified in cookiecutter.json:</p> <pre><code>def test_bake_project(cookies):\n    result = cookies.bake(extra_context={'repo_name': 'hello world'})\n\n    assert result.exit_code == 0\n    assert result.exception is None\n    assert result.project.basename == 'hello world'\n    assert result.project.isdir()\n</code></pre> <p>It accepts the <code>extra_context</code> keyword argument that is passed to cookiecutter. The given dictionary will override the default values of the template context, allowing you to test arbitrary user input data.</p> <p>The cookiecutter-django has a nice test file using this fixture.</p>"}, {"location": "linux/cookiecutter/#mocking-the-contents-of-the-cookiecutter-hooks", "title": "Mocking the contents of the cookiecutter hooks", "text": "<p>Sometimes it's interesting to add interactions with external services in the cookiecutter hooks, for example to activate a CI pipeline.</p> <p>If you want to test the cookiecutter template you need to mock those external interactions. But it's difficult to mock the contents of the hooks because their contents aren't run by the <code>cookies.bake()</code> code. Instead it delegates in cookiecutter to run them, which opens a <code>subprocess</code> to run them, so the mocks don't work.</p> <p>The alternative is setting an environmental variable in your tests to skip those steps:</p> <p>File: tests/conftest.py</p> <pre><code>import os\n\nos.environ[\"COOKIECUTTER_TESTING\"] = \"true\"\n</code></pre> <p>File: hooks/pre_gen_project.py</p> <pre><code>def main():\n    # ... pre_hook content ...\n\n\nif __name__ == \"__main__\":\n\n    if os.environ.get(\"COOKIECUTTER_TESTING\") != \"true\":\n        main()\n</code></pre> <p>If you want to test the content of <code>main</code>, you can now mock each of the external interactions. But you'll face the problem that these files are jinja2 templates of python files, so it's tricky to test them, due to syntax errors.</p>"}, {"location": "linux/cookiecutter/#debug-failing-template-generation", "title": "Debug failing template generation", "text": "<p>Sometimes the generation of the templates will fail in the tests, I've found that the easier way to debug why is to inspect the result object of the <code>result = cookies.bake()</code> statement with pdb.</p> <p>It has an <code>exception</code> method with <code>lineno</code> argument and <code>source</code>. With that information I've been able to locate the failing line. It also has a <code>filename</code> attribute but it doesn't seem to work for me.</p>"}, {"location": "linux/cookiecutter/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "linux/cruft/", "title": "Cruft", "text": "<p>cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates.</p> <p>Many project template utilities exist that automate the copying and pasting of code to create new projects. This seems great! However, once created, most leave you with that copy-and-pasted code to manage through the life of your project.</p>"}, {"location": "linux/cruft/#key-features", "title": "Key Features", "text": "Cookiecutter Compatible cruft utilizes Cookiecutter as its template expansion engine. Meaning it retains full compatibility with all existing Cookiecutter templates. Template Validation cruft can quickly validate whether or not a project is using the latest version of a template using <code>cruft check</code>. This check can easily be added to CI pipelines to ensure your projects stay in-sync. Automatic Template Updates cruft automates the process of updating code to match the latest version of a template, making it easy to utilize template improvements across many projects."}, {"location": "linux/cruft/#installation", "title": "Installation", "text": "<pre><code>pip install cruft\n</code></pre>"}, {"location": "linux/cruft/#usage", "title": "Usage", "text": ""}, {"location": "linux/cruft/#creating-a-new-project", "title": "Creating a New Project", "text": "<p>To create a new project using cruft run <code>cruft create PROJECT_URL</code> from the command line.</p> <p>cruft will then ask you any necessary questions to create your new project. It will use your answers to expand the provided template, and then return the directory it placed the expanded project.</p> <p>Behind the scenes, cruft uses Cookiecutter to do the project expansion. The only difference in the resulting output is a <code>.cruft.json</code> file that contains the git hash of the template used as well as the parameters specified.</p>"}, {"location": "linux/cruft/#updating-a-project", "title": "Updating a Project", "text": "<p>To update an existing project, that was created using cruft, run <code>cruft update</code> in the root of the project.</p> <p>If there are any updates, cruft will have you review them before applying. If you accept the changes cruft will apply them to your project and update the <code>.cruft.json</code> file for you.</p> <p>Sometimes certain files just aren't good fits for updating. Such as test cases or <code>__init__</code> files. You can tell cruft to always skip updating these files on a project by project basis by added them to a skip section within your .cruft.json file:</p> <pre><code>{\n\"template\": \"https://github.com/timothycrosley/cookiecutter-python\",\n\"commit\": \"8a65a360d51250221193ed0ec5ed292e72b32b0b\",\n\"skip\": [\n\"cruft/__init__.py\",\n\"tests\"\n],\n...\n}\n</code></pre> <p>Or, if you have toml installed, you can add skip files directly to a <code>tool.cruft</code> section of your <code>pyproject.toml</code> file:</p> <pre><code>[tool.cruft]\nskip = [\"cruft/__init__.py\", \"tests\"]\n</code></pre>"}, {"location": "linux/cruft/#checking-a-project", "title": "Checking a Project", "text": "<p>Checking to see if a project is missing a template update is as easy as running <code>cruft check</code>. If the project is out-of-date an error and exit code 1 will be returned.</p> <p><code>cruft check</code> can be added to CI pipelines to ensure projects don't unintentionally drift.</p>"}, {"location": "linux/cruft/#linking-an-existing-project", "title": "Linking an Existing Project", "text": "<p>Have an existing project that you created from a template in the past using Cookiecutter directly? You can link it to the template that was used to create it using: <code>cruft link TEMPLATE_REPOSITORY</code>.</p> <p>You can then specify the last commit of the template the project has been updated to be consistent with, or accept the default of using the latest commit from the template.</p>"}, {"location": "linux/cruft/#compute-the-diff", "title": "Compute the diff", "text": "<p>With time, your boilerplate may end up being very different from the actual cookiecutter template. Cruft allows you to quickly see what changed in your local project compared to the template. It is as easy as running <code>cruft diff</code>. If any local file differs from the template, the diff will appear in your terminal in a similar fashion to <code>git diff</code>.</p> <p>The <code>cruft diff</code> command optionally accepts an <code>--exit-code</code> flag that will make cruft exit with a non-0 code should any diff is found. You can combine this flag with the <code>skip</code> section of your <code>.cruft.json</code> to make stricter CI checks that ensures any improvement to the template is always submitted upstream.</p>"}, {"location": "linux/cruft/#issues", "title": "Issues", "text": "<ul> <li>Save config in the     pyproject.toml: Update the     template once it's supported.</li> </ul>"}, {"location": "linux/cruft/#error-unable-to-interpret-changes-between-current-project-and-cookiecutter-template-as-unicode", "title": "Error: Unable to interpret changes between current project and cookiecutter template as unicode.", "text": "<p>Typically a result of hidden binary files in project folder. Maybe you have a hook that initializes the <code>.git</code> directory. Since <code>2.10.0</code> you can add a <code>skip</code> category inside the <code>.cruft.json</code>, so that it doesn't check that directory:</p> <pre><code>{\n\"template\": \"xxx\",\n\"commit\": \"xxx\",\n\"checkout\": null,\n\"context\": {\n\"cookiecutter\": {\n...\n}\n},\n\"directory\": null,\n\"skip\": [\n\".git\"\n]\n}\n</code></pre>"}, {"location": "linux/cruft/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Issues</li> </ul>"}, {"location": "linux/elasticsearch/", "title": "Commands for elasticsearch", "text": ""}, {"location": "linux/elasticsearch/#searching-documents", "title": "Searching documents", "text": "<p>We use HTTP requests to talk to ElasticSearch. A HTTP request is made up of several components such as the URL to make the request to, HTTP verbs (GET, POST etc) and headers. In order to succinctly and consistently describe HTTP requests the ElasticSearch documentation uses cURL command line syntax. This is also the standard practice to describe requests made to ElasticSearch within the user community.</p>"}, {"location": "linux/elasticsearch/#get-all-documents", "title": "Get all documents", "text": "<p>An example HTTP request using CURL syntax looks like this:</p> <pre><code>curl \\\n-H 'Content-Type: application/json' \\\n-XPOST \"https://localhost:9200/_search\" \\\n-d' { \"query\": { \"match_all\": {} }}'\n</code></pre>"}, {"location": "linux/elasticsearch/#get-documents-that-match-a-string", "title": "Get documents that match a string", "text": "<pre><code>curl \\\n-H 'Content-Type: application/json' \\\n-XPOST \"https://localhost:9200/_search\" \\\n-d' { \"query\": { \"query_string\": {\"query\": \"test company\"} }}'\n</code></pre>"}, {"location": "linux/elasticsearch/#backup", "title": "Backup", "text": "<p>It's better to use the <code>curator</code> tool</p>"}, {"location": "linux/elasticsearch/#create-snapshot", "title": "Create snapshot", "text": "<pre><code>curl {{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}?wait_for_completion=true\n</code></pre>"}, {"location": "linux/elasticsearch/#create-snapshot-of-selected-indices", "title": "Create snapshot of selected indices", "text": "<pre><code>curl {{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}?wait_for_completion=true\n\ncurl -XPUT 'localhost:9200/_snapshot/my_backup/snapshot_1?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"indices\": \"index_1,index_2\",\n    \"ignore_unavailable\": true,\n    \"include_global_state\": false\n}\n'\n</code></pre>"}, {"location": "linux/elasticsearch/#list-all-backups", "title": "List all backups", "text": "<p>Check for my-snapshot-repo</p> <pre><code>curl {{ url }}/_snapshot/{{ backup_path }}/*?pretty\n</code></pre>"}, {"location": "linux/elasticsearch/#restore-backup", "title": "Restore backup", "text": "<p>First you need to close the selected indices</p> <pre><code>curl -X POST {{ url }}/{{ indice_name }}/_close\n</code></pre> <p>Then restore</p> <pre><code>curl {{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?wait_for_completion=true\n</code></pre> <p>If you want to restore only one index, use:</p> <pre><code>curl -X POST \"{{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?pretty\" -H 'Content-Type: application/json' -d'\n{\n    \"indices\": \"{{ index_to_restore }}\",\n}'\n</code></pre>"}, {"location": "linux/elasticsearch/#delete-snapshot", "title": "Delete snapshot", "text": "<pre><code>curl -XDELETE {{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}\n</code></pre>"}, {"location": "linux/elasticsearch/#delete-snapshot-repository", "title": "Delete snapshot repository", "text": "<pre><code>curl -XDELETE {{ url }}/_snapshot/{{ backup_path }}\n</code></pre>"}, {"location": "linux/elasticsearch/#delete-snapshots-older-than-x", "title": "Delete snapshots older than X", "text": "<p>!!! note \"File: curator.yml\" ```yaml client: hosts: - 'a data node' port: 9200 url_prefix: use_ssl: False certificate: client_cert: client_key: ssl_no_validate: False http_auth: timeout: 30 master_only: False</p> <pre><code>logging:\nloglevel: INFO\nlogfile: D:\\CuratorLogs\\logs.txt\nlogformat: default\nblacklist: ['elasticsearch', 'urllib3']\n```\n</code></pre> <p>File: delete_old_snapshots.yml</p> <p><code>yaml     actions:     1:     action: delete_snapshots     description: &gt;-     Delete snapshots from the selected repository older than 100 days     (based on creation_date), for everything but 'citydirectory-' prefixed snapshots.     options:     repository: 'dcs-elastic-snapshot'     disable_action: False     filters:     - filtertype: pattern     kind: prefix     value: citydirectory-     exclude: True     - filtertype: age     source: creation_date     direction: older     unit: days     unit_count: 100</code></p>"}, {"location": "linux/elasticsearch/#information-gathering", "title": "Information gathering", "text": ""}, {"location": "linux/elasticsearch/#get-status-of-cluster", "title": "Get status of cluster", "text": "<pre><code>curl {{ url }}/_cluster/health?pretty\ncurl {{ url }}/_cat/nodes?v\ncurl {{ url }}/_cat/indices?v\ncurl {{ url }}/_cat/shards\n</code></pre> <p>If you've got red status, use the following command to choose the first unassigned shard that it finds and explains why it cannot be allocated to a node.</p> <pre><code>curl {{ url }}/_cluster/allocation/explain?v\n</code></pre>"}, {"location": "linux/elasticsearch/#get-settings", "title": "Get settings", "text": "<pre><code>curl {{ url }}/_settings\n</code></pre>"}, {"location": "linux/elasticsearch/#get-space-left", "title": "Get space left", "text": "<pre><code>curl {{ url }}/_nodes/stats/fs?pretty\n</code></pre>"}, {"location": "linux/elasticsearch/#list-plugins", "title": "List plugins", "text": "<pre><code>curl {{ url }}/_nodes/plugins?pretty\n</code></pre>"}, {"location": "linux/elasticsearch/#upload", "title": "Upload", "text": ""}, {"location": "linux/elasticsearch/#single-data-upload", "title": "Single data upload", "text": "<pre><code>curl -XPOST '{{ url }}/{{ path_to_table }}' -d '{{ json_input }}'\n</code></pre> <p>where json_input can be <code>{ \"field\" : \"value\" }</code></p>"}, {"location": "linux/elasticsearch/#bulk-upload-of-data", "title": "Bulk upload of data", "text": "<pre><code>curl -H 'Content-Type: application/x-ndjson' -XPOST \\\n'{{ url }}/{{ path_to_table }}/_bulk?pretty' --data-binary @{{ json_file }}\n</code></pre>"}, {"location": "linux/elasticsearch/#delete", "title": "Delete", "text": ""}, {"location": "linux/elasticsearch/#delete-data", "title": "Delete data", "text": "<pre><code>curl -XDELETE {{ url }}/{{ path_to_ddbb }}\n</code></pre>"}, {"location": "linux/elasticsearch/#reindex-an-index", "title": "Reindex an index", "text": "<p>If you encountered errors while reindexing <code>source_index</code> to <code>destination_index</code> it can be because the cluster hit a timeout on the scroll locks. As a work around, you can increase the timeout period to a reasonable value and then reindex. The default AWS values are search context of 5 minutes, socket timeout of 30 seconds, and batch size of 1,000.</p> <p>First clear the cache of the index with:</p> <pre><code>curl -X POST https://elastic.url/destination_index/_cache/clear\n</code></pre> <p>If the index is big, they suggest to disable replicas in your destination index by setting number_of_replicas to 0 and re-enable them once the reindex process is complete.</p> <p>To get the current state use:</p> <pre><code>curl https://elastic.url/destination_index/_settings\n</code></pre> <p>Then disable the replicas with:</p> <pre><code>curl -X PUT \\\nhttps://elastic.url/destination_index \\\n-H 'Content-Type: application/json' \\\n-d '{\"settings\": {\"refresh_interval\": -1, \"number_of_replicas\": 0}}\n</code></pre> <p>Now you can reindex the index with:</p> <pre><code>curl -X POST \\\nhttps://elastic.url/_reindex?wait_for_completion=false\\&amp;timeout=10m\\&amp;scroll=10h\\&amp;pretty=true \\\n-H 'Content-Type: application/json' \\\n-d '{\"source\": { \"remote\": { \"host\": \"https://elastic.url:443\", \"socket_timeout\": \"60m\" }, \"index\": \"source_index\" }, \"dest\": {\"index\": \"destination_index\"}}'\n</code></pre> <p>And check the evolution of the task with:</p> <pre><code>curl 'https://elastic.url/_tasks?detailed=true&amp;actions=*reindex&amp;group_by=parents&amp;pretty=true'\n</code></pre> <p>The output is quite verbose, so I use <code>vimdiff</code> to see the differences between instant states.</p> <p>If you see there are no tasks running, check the indices status to see if the reindex ended well.</p> <pre><code>curl https://elastic.url/_cat/indices\n</code></pre> <p>After the reindex process is complete, you can reset your desired replica count and remove the refresh interval setting.</p>"}, {"location": "linux/elasticsearch/#knn", "title": "KNN", "text": ""}, {"location": "linux/elasticsearch/#knn-sizing", "title": "KNN sizing", "text": "<p>Typically, in an Elasticsearch cluster, a certain portion of RAM is set aside for the JVM heap. The k-NN plugin allocates graphs to a portion of the remaining RAM. This portion\u2019s size is determined by the circuit_breaker_limit cluster setting. By default, the circuit breaker limit is set at 50%.</p> <p>The memory required for graphs is estimated to be `1.1 * (4 * dimension</p> <ul> <li>8 * M)` bytes/vector.</li> </ul> <p>To get the <code>dimension</code> and <code>m</code> use the <code>/index</code> elasticsearch endpoint. To get the number of vectors, use <code>/index/_count</code>. The number of vectors is the same as the number of documents.</p> <p>As an example, assume that we have 1 Million vectors with a dimension of 256 and M of 16, and the memory required can be estimated as:</p> <pre><code>1.1 * (4 *256 + 8 * 16) * 1,000,000 ~= 1.26 GB\n</code></pre> <p>!!! note \"Remember that having a replica will double the total number of vectors.\"</p> <p>I've seen some queries work with indices that required 120% of the available memory for the KNN.</p> <p>A good way to see if it fits, is warming up the knn vectors. If the process returns a timeout, you probably don't have enough memory.</p>"}, {"location": "linux/elasticsearch/#knn-warmup", "title": "KNN warmup", "text": "<p>The Hierarchical Navigable Small World (HNSW) graphs that are used to perform an approximate k-Nearest Neighbor (k-NN) search are stored as .hnsw files with other Apache Lucene segment files. In order for you to perform a search on these graphs using the k-NN plugin, these files need to be loaded into native memory.</p> <p>If the plugin has not loaded the graphs into native memory, it loads them when it receives a search request. This loading time can cause high latency during initial queries. To avoid this situation, users often run random queries during a warmup period. After this warmup period, the graphs are loaded into native memory and their production workloads can begin. This loading process is indirect and requires extra effort.</p> <p>As an alternative, you can avoid this latency issue by running the k-NN plugin warmup API operation on whatever indices you\u2019re interested in searching. This operation loads all the graphs for all of the shards (primaries and replicas) of all the indices specified in the request into native memory.</p> <p>After the process finishes, you can start searching against the indices with no initial latency penalties. The warmup API operation is idempotent, so if a segment\u2019s graphs are already loaded into memory, this operation has no impact on those graphs. It only loads graphs that aren\u2019t currently in memory.</p> <p>This request performs a warmup on three indices:</p> <pre><code>GET /_opendistro/_knn/warmup/index1,index2,index3?pretty\n{\n  \"_shards\" : {\n    \"total\" : 6,\n    \"successful\" : 6,\n    \"failed\" : 0\n  }\n}\n</code></pre> <p><code>total</code> indicates how many shards the k-NN plugin attempted to warm up. The response also includes the number of shards the plugin succeeded and failed to warm up.</p> <p>The call does not return until the warmup operation is complete or the request times out. If the request times out, the operation still continues on the cluster. To monitor the warmup operation, use the Elasticsearch <code>_tasks</code> API:</p> <pre><code>GET /_tasks\n</code></pre>"}, {"location": "linux/elasticsearch/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "linux/elasticsearch/#deal-with-the-aws-service-timeout", "title": "Deal with the AWS service timeout", "text": "<p>AWS' Elasticsearch service is exposed behind a load balancer that returns a timeout after 300 seconds. If the query you're sending takes longer you won't be able to retrieve the information.</p> <p>You can consider using Asynchronous search which requires Elasticsearch 7.10 or later. Asynchronous search lets you run search requests that run in the background. You can monitor the progress of these searches and get back partial results as they become available. After the search finishes, you can save the results to examine at a later time.</p> <p>If the query you're running is a KNN one, you can try:</p> <ul> <li> <p>Using the knn warmup api before running initial queries.</p> </li> <li> <p>Scaling up the instances: Amazon ES uses half of an instance's RAM for the   Java heap (up to a heap size of 32 GiB). By default, KNN uses up to 50% of the   remaining half, so an instance type with 64 GiB of RAM can accommodate 16 GiB   of graphs (64 * 0.5 * 0.5). Performance can suffer if graph memory usage   exceeds this value.</p> </li> <li> <p>In a less recommended approach, you can make more percentage of memory   available for KNN operations.</p> </li> </ul> <p>Open Distro for Elasticsearch lets you modify all KNN settings using the   <code>_cluster/settings</code> API. On Amazon ES, you can change all settings except   <code>knn.memory.circuit_breaker.enabled</code> and <code>knn.circuit_breaker.triggered</code>.</p> <p>You can change the circuit breaker settings as:</p> <pre><code>PUT /_cluster/settings\n{\n  \"persistent\" : {\n    \"knn.memory.circuit_breaker.limit\" : \"&lt;value%&gt;\"\n  }\n}\n</code></pre> <p>You could also do performance tuning your KNN request.</p>"}, {"location": "linux/elasticsearch/#fix-circuit-breakers-triggers", "title": "Fix Circuit breakers triggers", "text": "<p>The <code>elasticsearch_exporter</code> has a <code>elasticsearch_breakers_tripped</code> metric, which counts then number of Circuit Breakers triggered of the different kinds. The Grafana dashboard paints a count of all the triggers with a big red number, which may scare you at first.</p> <p>Lets first understand what are Circuit Breakers. Elasticsearch is built with Java and as such depends on the JVM heap for many operations and caching purposes. By default in AWS, each data node is assigned half the RAM to be used for heap for ES. In Elasticsearch the default Garbage Collector is Concurrent-Mark and Sweep (CMS). When the JVM Memory Pressure reaches 75%, this collector pauses some threads and attempts to reclaim some heap space. High heap usage occurs when the garbage collection process cannot keep up. An indicator of high heap usage is when the garbage collection is incapable of reducing the heap usage to around 30%.</p> <p>When a request reaches the ES nodes, circuit breakers estimate the amount of memory needed to load the required data. The cluster then compares the estimated size with the configured heap size limit. If the estimated size of your data is greater than the available heap size, the query is terminated. As a result, a CircuitBreakerException is thrown to prevent overloading the node.</p> <p>In essence, these breakers are present to prevent a request overloading a data node and consuming more heap space than that node can provide at that time. If these breakers weren't present, then the request will use up all the heap that the node can provide and this node will then restart due to OOM.</p> <p>Lets assume a data node has 16GB heap configured, When the parent circuit breaker is tripped, then a similar error is thrown:</p> <pre><code>\"error\": {\n\"root_cause\": [\n{\n\"type\": \"circuit_breaking_exception\",\n\"reason\": \"[parent] Data too large, data for [&lt;HTTP_request&gt;] would be [16355096754/15.2gb], which is larger than the limit of [16213167308/15gb], real usage: [15283269136/14.2gb], new bytes reserved: [1071827618/1022.1mb]\",\n}\n]\n}\n</code></pre> <p>The parent circuit breaker (a circuit breaker type) is responsible for the overall memory usage of your Elasticsearch cluster. When a parent circuit breaker exception occurs, the total memory used across all circuit breakers has exceeded the set limit. A parent breaker throws an exception when the cluster exceeds 95% of 16 GB, which is 15.2 GB of heap (in above example).</p> <p>A circuit breaking exception is generally caused by high JVM. When the JVM Memory Pressure is high, it indicates that a large portion of one or more data nodes configured heap is currently being used heavily, and as such, the frequency of the circuit breakers being tripped increases as there is not enough heap available at the time to process concurrent smaller or larger requests.</p> <p>It is worth noting that that the error can also be thrown by a certain request that would just consume all the available heap on a certain data node at the time such as an intensive search query.</p> <p>If you see numerous spikes to the high 90%, with occasionally spikes to 100%, it's not uncommon for the parent circuit breaker to be tripped in response to requests.</p> <p>To troubleshoot circuit breakers, you'll then have to address the High JVM issues, which can be caused by:</p> <ul> <li>Increase in the number of requests to the cluster. Check the IndexRate and   SearchRate metrics in to determine your current load.</li> <li>Aggregation, wildcards, and using wide time ranges in your queries.</li> <li>Unbalanced shard allocation across nodes or too many shards in a cluster.</li> <li>Index mapping explosions.</li> <li>Using the fielddata data structure to query data. Fielddata can consume a   large amount of heap space, and remains in the heap for the lifetime of a   segment. As a result, JVM memory pressure remains high on the cluster when   fielddata is used.</li> </ul> <p>Here's what happens as JVM memory pressure increases in AWS:</p> <ul> <li>At 75%: Amazon ES triggers the Concurrent Mark Sweep (CMS) garbage collector.   The CMS collector runs alongside other processes to keep pauses and   disruptions to a minimum. The garbage collection is a CPU-intensive process.   If JVM memory pressure stays at this percentage for a few minutes, then you   could encounter ClusterBlockException, JVM OutOfMemoryError, or other cluster   performance issues.</li> <li>Above 75%: If the CMS collector fails to reclaim enough memory and usage   remains above 75%, Amazon ES triggers a different garbage collection   algorithm. This algorithm tries to free up memory and prevent a JVM   OutOfMemoryError (OOM) exception by slowing or stopping processes.</li> <li>Above 92% for 30 minutes: Amazon ES blocks all write operations.</li> <li>Around 95%: Amazon ES kills processes that try to allocate memory. If a   critical process is killed, one or more cluster nodes might fail.</li> <li>At 100%: Amazon ES JVM is configured to exit and eventually restarts on   OutOfMemory (OOM).</li> </ul> <p>To resolve high JVM memory pressure, try the following tips:</p> <ul> <li> <p>Reduce incoming traffic to your cluster, especially if you have a heavy   workload.</p> </li> <li> <p>Consider scaling the cluster to obtain more JVM memory to support your   workload. As mentioned above each data node gets half the RAM allocated to be   used as Heap. Consider scaling to a data node type with more RAM and hence   more Available Heap. Thereby increasing the parent circuit breaker limit.</p> </li> <li> <p>If cluster scaling isn't possible, try reducing the number of shards by   deleting old or unused indices. Because shard metadata is stored in memory,   reducing the number of shards can reduce overall memory usage.</p> </li> <li> <p>Enable slow logs to identify faulty requests. Note: Before enabling   configuration changes, verify that JVM memory pressure is below 85%. This way,   you can avoid additional overhead to existing resources.</p> </li> <li> <p>Optimize search and indexing requests, and choose the correct number of   shards.</p> </li> <li> <p>Disable and avoid using fielddata. By default, fielddata is set to \"false\" on   a text field unless it's explicitly defined as otherwise in index mappings.</p> </li> </ul> <p>Field data is a potentially a huge consumer of JVM Heap space. This build up   of field data occurs when aggregations are run on fields that are of type   <code>text</code>. More on how you can periodically clear field data below.</p> <ul> <li>Change your index mapping type to a <code>keyword</code>, using reindex API. You can use   the <code>keyword</code> type as an alternative for performing aggregations and sorting   on text fields.</li> </ul> <p>As mentioned in above point, by aggregating on <code>keyword</code> type instead of   <code>text</code>, no field data has to be built on demand and hence won't consume   precious heap space. Look into the commonly aggregated fields in index   mappings and ensure they are not of type <code>text</code>.</p> <p>If they are, you can consider changing them to <code>keyword</code>. You will have to   create a new index with the desired mapping and then use the Reindex API to   transfer over the documents from the source index to the new index. Once   Re-index has completed then you can delete the old index.</p> <ul> <li> <p>Avoid aggregating on text fields to prevent increases in field data. When you   use more field data, more heap space is consumed. Use the cluster stats API   operation to check your field data.</p> </li> <li> <p>Clear the fielddata cache with the following API call:</p> </li> </ul> <pre><code>POST /index_name/_cache/clear?fielddata=true (index-level cache)\nPOST */_cache/clear?fielddata=true (cluster-level cache)\n</code></pre> <p>Generally speaking, if you notice your workload (search rate and index rate) remaining consistent during these high spikes and non of the above optimizations can be applied or if they have already been applied and the JVM is still high during these workload times, then it is an indication that the cluster needs to be scaled in terms of JVM resources to cope with this workload.</p> <p>You can't reset the 'tripped' count. This is a Node level metric and thus will be reset to <code>0</code> when the Elasticsearch Service is restarted on that Node. Since in AWS it's a managed service, unfortunately you will not have access to the underlaying EC2 instance to restart the ES Process.</p> <p>However the ES Process can be restarted on your end (on all nodes) in the following ways:</p> <ul> <li>Initiate a Configuration Change that causes a blue/green deployment : When you   initiate a configuration change, a subsequent blue/green deployment process is   launched in which we launch a new fleet that matches the desired   configuration. The old fleet continues to run and serve requests.   Simultaneously, data in the form of shards are then migrated from the old   fleet to the new fleet. Once all this data has been migrated the old fleet is   terminated and the new one takes over.</li> </ul> <p>During this process ES is restarted on the Nodes.</p> <p>Ensure that CPU Utilization and JVM Memory Pressure are below the recommended   80% thresholds to prevent any issues with this process as it uses clusters   resources to initiate and complete.</p> <p>You can scale the EBS Volumes attached to the data nodes by an arbitrary   amount such as 1GB, wait for the blue/green to complete and then scale it   back.</p> <ul> <li>Wait for a new service software release and update the service software of the   Cluster.</li> </ul> <p>This will also cause a blue/green and hence ES process will be restarted on   the nodes.</p>"}, {"location": "linux/elasticsearch/#recover-from-yellow-state", "title": "Recover from yellow state", "text": "<p>A yellow cluster represents that some of the replica shards in the cluster are unassigned. I can see that around 14 replica shards are unassigned.</p> <p>You can confirm the state of the cluster with the following commands</p> <pre><code>curl &lt;domain-endpoint&gt;_cluster/health?pretty\ncurl -X GET &lt;domain-endpoint&gt;/_cat/shards | grep UNASSIGNED\ncurl -X GET &lt;domain-endpoint&gt;/_cat/indices | grep yellow\n</code></pre> <p>If you have metrics of the JVMMemoryPressure of the nodes, check if the memory of a node reached 100% around the time the cluster reached yellow state.</p> <p>One can generally confirm the reason for a cluster going yellow by looking at the output of the following API call:</p> <pre><code>curl -X GET &lt;domain-endpoint&gt;/_cluster/allocation/explain | jq\n</code></pre> <p>If it shows a <code>CircuitBreakerException</code>, it confirms that a spike in the JVM metric caused the node to go down. Check the Fix Circuit breaker triggers section above to see how to solve that case.</p>"}, {"location": "linux/elasticsearch/#reallocate-unassigned-shards", "title": "Reallocate unassigned shards", "text": "<p>Elasticsearch makes 5 attempts to assign the shard but if it fails to be assigned after 5 attempts, the shards will remain unassigned. There is a solution to this issue in order to bring the cluster to green state.</p> <p>You can disable the replicas on the failing index and then enable replicas back.</p> <ul> <li>Disable Replica</li> </ul> <pre><code>curl -X PUT \"&lt;ES_endpoint&gt;/&lt;index_name&gt;/_settings\" -H 'Content-Type: application/json' -d'\n{\n    \"index\" : {\n        \"number_of_replicas\" : 0\n    }\n}'\n</code></pre> <ul> <li>Enable the Replica back:</li> </ul> <pre><code>curl -X PUT \"&lt;ES_endpoint&gt;/&lt;index_name&gt;/_settings\" -H 'Content-Type: application/json' -d'\n{\n    \"index\" : {\n        \"number_of_replicas\" : 1\n    }\n}'\n</code></pre> <p>Please note that it will take some time for the shards to be completely assigned and hence you might see intermittent cluster status as YELLOW.</p>"}, {"location": "linux/fail2ban/", "title": "Fail2ban", "text": ""}, {"location": "linux/fail2ban/#usage", "title": "Usage", "text": ""}, {"location": "linux/fail2ban/#unban-ip", "title": "Unban IP", "text": "<pre><code>fail2ban-client set {{ jail }} unbanip {{ ip }}\n</code></pre> <p>Where <code>jail</code> can be <code>ssh</code>.</p>"}, {"location": "linux/google_chrome/", "title": "Install Google Chrome", "text": "<p>Although I hate it, there are web pages that don't work on Firefox or Chromium. In those cases I install <code>google-chrome</code> and uninstall as soon as I don't need to use that service.</p>"}, {"location": "linux/google_chrome/#installation", "title": "Installation", "text": ""}, {"location": "linux/google_chrome/#debian", "title": "Debian", "text": "<ul> <li> <p>Import the GPG key, and use the following command.   <pre><code>sudo wget -O- https://dl.google.com/linux/linux_signing_key.pub | gpg --dearmor &gt; /usr/share/keyrings/google-chrome.gpg\n</code></pre></p> </li> <li> <p>Once the GPG import is complete, you will need to import the Google Chrome repository.</p> </li> </ul> <pre><code>echo 'deb [arch=amd64 signed-by=/usr/share/keyrings/google-chrome.gpg] http://dl.google.com/linux/chrome/deb/ stable main' | sudo tee /etc/apt/sources.list.d/google-chrome.list\n</code></pre> <ul> <li>Install the program:   <pre><code>apt-get update\napt-get install google-chrome-stable\n</code></pre></li> </ul>"}, {"location": "linux/haproxy/", "title": "HAProxy", "text": "<p>HAProxy is free, open source software that provides a high availability load balancer and proxy server for TCP and HTTP-based applications that spreads requests across multiple servers. It is written in C and has a reputation for being fast and efficient (in terms of processor and memory usage).</p>"}, {"location": "linux/haproxy/#use-haproxy-as-a-reverse-proxy", "title": "Use HAProxy as a reverse proxy", "text": "<p>reverse proxy is a type of proxy server that retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client, appearing as if they originated from the server itself. Unlike a forward proxy, which is an intermediary for its associated clients to contact any server, a reverse proxy is an intermediary for its associated servers to be contacted by any client. In other words, a proxy is associated with the client(s), while a reverse proxy is associated with the server(s); a reverse proxy is usually an internal-facing proxy used as a 'front-end' to control and protect access to a server on a private network.</p> <p>It can be done at Web server level (Nginx, Apache, ...) or at load balancer level.</p> <p>This HAProxy post shows how to translate Apache's proxy pass directives to the HAProxy configuration.</p> <pre><code>    frontend ft_global\n     acl host_dom.com    req.hdr(Host) dom.com\n     acl path_mirror_foo path -m beg   /mirror/foo/\n     use_backend bk_myapp if host_dom.com path_mirror_foo\n    backend bk_myapp\n    [...]\n    # external URL                  =&gt; internal URL\n    # http://dom.com/mirror/foo/bar =&gt; http://bk.dom.com/bar\n     # ProxyPass /mirror/foo/ http://bk.dom.com/bar\n     http-request set-header Host bk.dom.com\n     reqirep  ^([^ :]*)\\ /mirror/foo/(.*)     \\1\\ /\\2\n     # ProxyPassReverse /mirror/foo/ http://bk.dom.com/bar\n     # Note: we turn the urls into absolute in the mean time\n     acl hdr_location res.hdr(Location) -m found\n     rspirep ^Location:\\ (https?://bk.dom.com(:[0-9]+)?)?(/.*) Location:\\ /mirror/foo3 if hdr_location\n     # ProxyPassReverseCookieDomain bk.dom.com dom.com\n     acl hdr_set_cookie_dom res.hdr(Set-cookie) -m sub Domain= bk.dom.com\n     rspirep ^(Set-Cookie:.*)\\ Domain=bk.dom.com(.*) \\1\\ Domain=dom.com\\2 if hdr_set_cookie_dom\n     # ProxyPassReverseCookieDomain / /mirror/foo/\n     acl hdr_set_cookie_path res.hdr(Set-cookie) -m sub Path=\n     rspirep ^(Set-Cookie:.*)\\ Path=(.*) \\1\\ Path=/mirror/foo2 if hdr_set_cookie_path\n</code></pre> <p>Other useful examples can be retrieved from drmalex07  or ferdinandosimonetti gists.</p>"}, {"location": "linux/haproxy/#references", "title": "References", "text": "<ul> <li>Guidelines for HAProxy termination in AWS</li> </ul>"}, {"location": "linux/hypothesis/", "title": "Hypothesis", "text": "<p>Hypothesis is an open-source software project that aims to collect comments about statements made in any web-accessible content, and filter and rank those comments to assess each statement's credibility.</p> <p>It offers an online web application where registered users share highlights and annotations over any webpage.</p> <p>As of 2020-06-11, although the service can be self-hosted, it's not yet easy to do so.</p>"}, {"location": "linux/hypothesis/#install", "title": "Install", "text": ""}, {"location": "linux/hypothesis/#client", "title": "Client", "text": "<p>If you're using Chrome or any derivative there is an official extension.</p> <p>Unfortunately if you use Firefox the extension is still being developed #310 although an unofficial release works just fine. Alternatively you can use the Hypothesis bookmarklet.</p> <p>The only problem is that both the extensions and the bookmarklet only works for the official service. In theory you can tweak the extension build process to use your custom settings. Though there is yet no documentation on this topic.</p> <p>I've thought of opening them a bug regarding this issue, but their github issues are only for bug reports, they use a google group to track the feature requests, I don't have an easy way to post there, so if you follow this path, please contact me.</p>"}, {"location": "linux/hypothesis/#server", "title": "Server", "text": "<p>The infrastructure can be deployed with Docker-compose.</p> <pre><code>version: '3'\nservices:\npostgres:\nimage: postgres:11.5-alpine\nports:\n- 5432\n# - '5432:5432'\nelasticsearch:\nimage: hypothesis/elasticsearch:latest\nports:\n- 9200\n#- '9200:9200'\nenvironment:\n- discovery.type=single-node\nrabbit:\nimage: rabbitmq:3.6-management-alpine\nports:\n- 5672\n- 15672\n#- '5672:5672'\n#- '15672:15672'\nweb:\nimage: hypothesis/hypothesis:latest\nenvironment:\n- APP_URL=http://localhost:5000\n- AUTHORITY=localhost\n- BROKER_URL=amqp://guest:guest@rabbit:5672//\n- CLIENT_OAUTH_ID\n- CLIENT_URL=http://localhost:3001/hypothesis\n- DATABASE_URL=postgresql://postgres@postgres/postgres\n- ELASTICSEARCH_URL=http://elasticsearch:9200\n- NEW_RELIC_APP_NAME=h (dev)\n- NEW_RELIC_LICENSE_KEY\n- SECRET_KEY=notasecret\nports:\n- '5000:5000'\ndepends_on:\n- postgres\n- elasticsearch\n- rabbit\n</code></pre> <pre><code>docker-compose up\n</code></pre> <p>Initialize the database and create the admin user.</p> <pre><code>docker-compose exec web /bin/sh\nhypothesis init\n\nhypothesis user add\nhypothesis user admin &lt;username&gt;\n</code></pre> <p>The service is available at http://localhost:5000.</p> <p>To check the latest developments of the Docker compose deployment follow the issue #4899.</p> <p>They also provide the tools they use to deploy the production service into AWS.</p>"}, {"location": "linux/hypothesis/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>FAQ</li> <li>Bug tracker</li> <li>Feature request tracker</li> </ul>"}, {"location": "linux/hypothesis/#server-deployment-open-issues", "title": "Server deployment open issues", "text": "<ul> <li>Self-hosting Docker compose</li> <li>Create admin user when using Docker     compose</li> <li>Steps required to run both h and serve the client from internal server</li> <li>How to deploy h on VM</li> </ul>"}, {"location": "linux/mkdocs/", "title": "Mkdocs", "text": "<p>MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file.</p> <p>Note: I've automated the creation of the mkdocs site in this cookiecutter template.</p>"}, {"location": "linux/mkdocs/#installation", "title": "Installation", "text": "<ul> <li>Install the basic packages.</li> </ul> <pre><code>pip install \\\nmkdocs \\\nmkdocs-material \\\nmkdocs-autolink-plugin \\\nmkdocs-minify-plugin \\\npymdown-extensions \\\nmkdocs-git-revision-date-localized-plugin\n</code></pre> <ul> <li>Create the <code>docs</code> repository.</li> </ul> <pre><code>mkdocs new docs\n</code></pre> <ul> <li>Although there are   several themes, I   usually use the material one. I   won't dive into the different options, just show a working template of the   <code>mkdocs.yaml</code> file.</li> </ul> <pre><code>site_name: {{site_name: null}: null}\nsite_author: {{your_name: null}: null}\nsite_url: {{site_url: null}: null}\nnav:\n- Introduction: index.md\n- Basic Usage: basic_usage.md\n- Configuration: configuration.md\n- Update: update.md\n- Advanced Usage:\n- Projects: projects.md\n- Tags: tags.md\n\nplugins:\n- search\n- autolinks\n- git-revision-date-localized:\ntype: timeago\n- minify:\nminify_html: true\n\nmarkdown_extensions:\n- admonition\n- meta\n- toc:\npermalink: true\nbaselevel: 2\n- pymdownx.arithmatex\n- pymdownx.betterem:\nsmart_enable: all\n- pymdownx.caret\n- pymdownx.critic\n- pymdownx.details\n- pymdownx.emoji:\nemoji_generator: !%21python/name:pymdownx.emoji.to_svg\n- pymdownx.inlinehilite\n- pymdownx.magiclink\n- pymdownx.mark\n- pymdownx.smartsymbols\n- pymdownx.superfences\n- pymdownx.tasklist:\ncustom_checkbox: true\n- pymdownx.tilde\n\ntheme:\nname: material\ncustom_dir: theme\nlogo: images/logo.png\npalette:\nprimary: blue grey\naccent: light blue\n\nextra_css:\n- stylesheets/extra.css\n- stylesheets/links.css\n\nrepo_name: {{repository_name: null}: null} # for example: 'lyz-code/pydo'\nrepo_url: {{repository_url: null}: null} # for example: 'https://github.com/lyz-code/pydo'\n</code></pre> <ul> <li> <p>Configure your logo   by saving it into <code>docs/images/logo.png</code>.</p> </li> <li> <p>I like to show a small image above each link so you know where is it pointing   to. To do so add the content of   this directory to   <code>theme</code>. and   these   files under <code>docs/stylesheets</code>.</p> </li> <li> <p>Initialize the git repository and create the first commit.</p> </li> <li> <p>Start the server to see everything is alright.</p> </li> </ul> <pre><code>mkdocs serve\n</code></pre>"}, {"location": "linux/mkdocs/#material-theme-customizations", "title": "Material theme customizations", "text": ""}, {"location": "linux/mkdocs/#color-palette-toggle", "title": "Color palette toggle", "text": "<p>Since 7.1.0, you can have a light-dark mode on the site using a toggle in the upper bar.</p> <p>To enable it add to your <code>mkdocs.yml</code>:</p> <pre><code>theme:\npalette:\n\n# Light mode\n- media: '(prefers-color-scheme: light)'\nscheme: default\nprimary: blue grey\naccent: light blue\ntoggle:\nicon: material/toggle-switch-off-outline\nname: Switch to dark mode\n\n# Dark mode\n- media: '(prefers-color-scheme: dark)'\nscheme: slate\nprimary: blue grey\naccent: light blue\ntoggle:\nicon: material/toggle-switch\nname: Switch to light mode\n</code></pre> <p>Changing your desired colors for each mode</p>"}, {"location": "linux/mkdocs/#back-to-top-button", "title": "Back to top button", "text": "<p>Since 7.1.0, a back-to-top button can be shown when the user, after scrolling down, starts to scroll up again. It's rendered in the lower right corner of the viewport. Add the following lines to mkdocs.yml:</p> <pre><code>theme:\nfeatures:\n- navigation.top\n</code></pre>"}, {"location": "linux/mkdocs/#add-a-github-pages-hook", "title": "Add a github pages hook.", "text": "<ul> <li>Save your <code>requirements.txt</code>.</li> </ul> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <ul> <li>Create the <code>.github/workflows/gh-pages.yml</code> file with the following contents.</li> </ul> <pre><code>name: Github pages\n\non:\npush:\nbranches:\n- master\n\njobs:\ndeploy:\nruns-on: ubuntu-18.04\nsteps:\n- uses: actions/checkout@v2\nwith:\n# Number of commits to fetch. 0 indicates all history.\n# Default: 1\nfetch-depth: 0\n\n- name: Setup Python\nuses: actions/setup-python@v1\nwith:\npython-version: '3.7'\narchitecture: x64\n\n- name: Cache dependencies\nuses: actions/cache@v1\nwith:\npath: ~/.cache/pip\nkey: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}\nrestore-keys: |\n${{ runner.os }}-pip-\n\n- name: Install dependencies\nrun: |\npython3 -m pip install --upgrade pip\npython3 -m pip install -r ./requirements.txt\n\n- run: |\ncd docs\nmkdocs build\n\n- name: Deploy\nuses: peaceiris/actions-gh-pages@v3\nwith:\ndeploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }}\npublish_dir: ./docs/site\n</code></pre> <ul> <li> <p>Create an   SSH deploy key</p> </li> <li> <p>Activate <code>GitHub Pages</code> repository configuration with <code>gh-pages branch</code>.</p> </li> <li> <p>Make a new commit and push to check it's working.</p> </li> </ul>"}, {"location": "linux/mkdocs/#create-mermaidjs-diagrams", "title": "Create MermaidJS diagrams", "text": "<p>Even though the Material theme supports mermaid diagrams it's only giving it for the paid users. The funding needs to reach 5000$ so it's released to the general public.</p> <p>The alternative is to use the mkdocs-mermaid2-plugin plugin, which can't be used with <code>mkdocs-minify-plugin</code> and doesn't adapt to dark mode.</p> <p>To install it:</p> <ul> <li> <p>Download the package: <code>pip install mkdocs-mermaid2-plugin</code>.</p> </li> <li> <p>Enable the plugin in <code>mkdocs.yml</code>.</p> </li> </ul> <pre><code>plugins:\n# Not compatible with mermaid2\n# - minify:\n#    minify_html: true\n- mermaid2:\narguments:\nsecurityLevel: loose\nmarkdown_extensions:\n- pymdownx.superfences:\n# make exceptions to highlighting of code:\ncustom_fences:\n- name: mermaid\nclass: mermaid\nformat: !%21python/name:mermaid2.fence_mermaid\n</code></pre> <p>Check the MermaidJS article to see how to create the diagrams.</p>"}, {"location": "linux/mkdocs/#plugin-development", "title": "Plugin development", "text": "<p>Like MkDocs, plugins must be written in Python. It is expected that each plugin would be distributed as a separate Python module. At a minimum, a MkDocs Plugin must consist of a BasePlugin subclass and an entry point which points to it.</p> <p>The BasePlugin class is meant to have <code>on_&lt;event_name&gt;</code> methods that run actions on the MkDocs defined events.</p> <p>The same object is called at the different events, so you can save objects from one event to the other in the object attributes.</p> <p>Keep in mind that the order of execution of the plugins follows the ordering of the list of the <code>mkdocs.yml</code> file where they are defined.</p>"}, {"location": "linux/mkdocs/#interesting-objects", "title": "Interesting objects", "text": ""}, {"location": "linux/mkdocs/#files", "title": "Files", "text": "<p><code>mkdocs.structure.files.Files</code> contains a list of File objects under the <code>._files</code> attribute and allows you to <code>append</code> files to the collection. As well as extracting the different file types:</p> <ul> <li><code>documentation_pages</code>: Iterable of markdown page file objects.</li> <li><code>static_pages</code>: Iterable of static page file objects.</li> <li><code>media_files</code>: Iterable of all files that are not documentation or static   pages.</li> <li><code>javascript_files</code>: Iterable of javascript files.</li> <li><code>css_files</code>: Iterable of css files.</li> </ul> <p>It is initialized with a list of <code>File</code> objects.</p>"}, {"location": "linux/mkdocs/#file", "title": "File", "text": "<p><code>mkdocs.structure.files.File</code> objects points to the source and destination locations of a file. It has the following interesting attributes:</p> <ul> <li><code>name</code>: Name of the file without the extension.</li> <li><code>src_path</code> or <code>abs_src_path</code>: Relative or absolute path to the original path,   for example the markdown file.</li> <li><code>dest_path</code> or <code>abs_dest_path</code>: Relative or absolute path to the destination   path, for example the html file generated from the markdown one.</li> <li><code>url</code>: Url where the file is going to be exposed.</li> </ul> <p>It is initialized with the arguments:</p> <ul> <li><code>path</code>: Must be a path that exists relative to <code>src_dir</code>.</li> <li><code>src_dir</code>: Absolute path on the local file system to the directory where the   docs are.</li> <li><code>dest_dir</code>: Absolute path on the local file system to the directory where the   site is going to be built.</li> <li><code>use_directory_urls</code>: If <code>False</code>, a Markdown file is mapped to an HTML file of   the same name (the file extension is changed to <code>.html</code>). If True, a Markdown   file is mapped to an HTML index file (<code>index.html</code>) nested in a directory   using the \"name\" of the file in <code>path</code>. The <code>use_directory_urls</code> argument has   no effect on non-Markdown files. By default MkDocs uses <code>True</code>.</li> </ul>"}, {"location": "linux/mkdocs/#navigation", "title": "Navigation", "text": "<p><code>mkdocs.structure.nav.Navigation</code> objects hold the information to build the navigation of the site. It has the following interesting attributes:</p> <ul> <li><code>items</code>: Nested List with full navigation of Sections, SectionPages, Pages,   and Links.</li> <li><code>pages</code>: Flat List of subset of Pages in nav, in order.</li> </ul> <p>The <code>Navigation</code> object has no <code>__eq__</code> method, so when testing, instead of trying to build a similar <code>Navigation</code> object and compare them, you need to assert that the contents of the object are what you expect.</p>"}, {"location": "linux/mkdocs/#page", "title": "Page", "text": "<p><code>mkdocs.structure.pages.Page</code> models each page of the site.</p> <p>To initialize it you need the <code>title</code>, the <code>File</code> object of the page, and the MkDocs <code>config</code> object.</p>"}, {"location": "linux/mkdocs/#section", "title": "Section", "text": "<p><code>mkdocs.structure.nav.Section</code> object models a section of the navigation of a MkDocs site.</p> <p>To initialize it you need the <code>title</code> of the section and the <code>children</code> which are the elements that belong to the section. If you don't yet know the children, pass an empty list <code>[]</code>.</p>"}, {"location": "linux/mkdocs/#sectionpage", "title": "SectionPage", "text": "<p><code>mkdocs_section_index.SectionPage</code> , part of the mkdocs-section-index plugin, models Section objects that have an associated Page, allowing you to have nav sections that when clicked, load the Page and not only opens the menu for the children elements.</p> <p>To initialize it you need the <code>title</code> of the section, the <code>File</code> object of the page, , the MkDocs <code>config</code> object, and the <code>children</code> which are the elements that belong to the section. If you don't yet know the children, pass an empty list <code>[]</code>.</p>"}, {"location": "linux/mkdocs/#events", "title": "Events", "text": ""}, {"location": "linux/mkdocs/#on_config", "title": "on_config", "text": "<p>The config event is the first event called on build and is run immediately after the user configuration is loaded and validated. Any alterations to the config should be made here.</p> <p>Parameters:</p> <ul> <li><code>config</code>: global configuration object</li> </ul> <p>Returns:</p> <ul> <li>global configuration object</li> </ul>"}, {"location": "linux/mkdocs/#on_files", "title": "on_files", "text": "<p>The <code>files</code> event is called after the files collection is populated from the <code>docs_dir</code>. Use this event to add, remove, or alter files in the collection. Note that Page objects have not yet been associated with the file objects in the collection. Use Page Events to manipulate page specific data.</p> <p>Parameters:</p> <ul> <li><code>files</code>: global files collection</li> <li><code>config</code>: global configuration object</li> </ul> <p>Returns:</p> <ul> <li>global files collection</li> </ul>"}, {"location": "linux/mkdocs/#on_nav", "title": "on_nav", "text": "<p>The <code>nav</code> event is called after the site navigation is created and can be used to alter the site navigation.</p> <p>Warning: Read the following section if you want to add new files.</p> <p>Parameters:</p> <ul> <li><code>nav</code>: global navigation object.</li> <li><code>config</code>: global configuration object.</li> <li><code>files</code>: global files collection.</li> </ul> <p>Returns:</p> <ul> <li>global navigation object</li> </ul>"}, {"location": "linux/mkdocs/#adding-new-files", "title": "Adding new files", "text": "<p>Note: \"TL;DR: Add them in the <code>on_config</code> event.\"</p> <p>To add new files to the repository you will need two phases:</p> <ul> <li>Create the markdown article pages.</li> <li>Add them to the navigation.</li> </ul> <p>My first idea as a MkDocs user, and newborn plugin developer was to add the navigation items to the <code>nav</code> key in the <code>config</code> object, as it's more easy to add items to a dictionary I'm used to work with than to dive into the code and understand how MkDocs creates the navigation. As I understood from the docs, the files should be created in the <code>on_files</code> event. the problem with this approach is that the only event that allows you to change the <code>config</code> is the <code>on_config</code> event, which is before the <code>on_files</code> one, so you can't build the navigation this way after you've created the files.</p> <p>Next idea was to add the items in the <code>on_nav</code> event, that means creating yourself the <code>Section</code>, <code>Pages</code>, <code>SectionPages</code> or <code>Link</code> objects and append them to the <code>nav.items</code>. The problem is that MkDocs initializes and processes the <code>Navigation</code> object in the <code>get_navigation</code> function. If you want to add items with a plugin in the <code>on_nav</code> event, you need to manually run all the post processing functions such as building the <code>pages</code> attribute, by running the <code>_get_by_type</code>, <code>_add_previous_and_next_links</code> or <code>_add_parent_links</code> yourself. Additionally, when building the site you'll get the <code>The following pages exist in the docs directory, but are not included in the \"nav\" configuration</code> error, because that check is done before all plugins change the navigation in the <code>on_nav</code> object.</p> <p>The last approach is to build the files and tweak the navigation in the <code>on_config</code> event. This approach has the next advantages:</p> <ul> <li>You need less knowledge of how MkDocs works.</li> <li>You don't need to create the <code>File</code> or <code>Files</code> objects.</li> <li>You don't need to create the <code>Page</code>, <code>Section</code>, <code>SectionPage</code> objects.</li> <li>More robust as you rely on existent MkDocs functionality.</li> </ul>"}, {"location": "linux/mkdocs/#testing", "title": "Testing", "text": "<p>I haven't found any official documentation on how to test MkDocs plugins, in the issues they suggest you look at how they test it in the search plugin. I've looked at other plugins such as mkdocs_blog and used the next way to test mkdocs-newsletter.</p> <p>I see the plugin definition as an entrypoint to the functionality of our program, that's why I feel the definition should be in <code>src/mkdocs_newsletter/entrypoints/mkdocs_plugin.py</code>. As any entrypoint, the best way to test them are in end-to-end tests.</p> <p>You need to have a working test site in <code>tests/assets/test_data</code>, with it's <code>mkdocs.yml</code> file that loads your plugin and some fake articles.</p> <p>To prepare the test we can define the next fixture that prepares the building of the site:</p> <p>File: <code>tests/conftest.py</code>:</p> <pre><code>import os\nimport shutil\n\nfrom mkdocs import config\nfrom mkdocs.config.base import Config\n\n\n@pytest.fixture(name=\"config\")\ndef config_(tmp_path: Path) -&gt; Config:\n\"\"\"Load the mkdocs configuration.\"\"\"\n    repo_path = tmp_path / \"test_data\"\n    shutil.copytree(\"tests/assets/test_data\", repo_path)\n    mkdocs_config = config.load_config(os.path.join(repo_path, \"mkdocs.yml\"))\n    mkdocs_config[\"site_dir\"] = os.path.join(repo_path, \"site\")\n    return mkdocs_config\n</code></pre> <p>It does the next steps:</p> <ul> <li>Copy the fake MkDocs site to a temporal directory</li> <li>Prepare the MkDocs <code>Config</code> object to build the site.</li> </ul> <p>Now we can use it in the e2e tests:</p> <p>File: <code>tests/e2e/test_plugin.py</code>:</p> <pre><code>def test_plugin_builds_newsletters(full_repo: Repo, config: Config) -&gt; None:\n    build.build(config)  # act\n\n    newsletter_path = f\"{full_repo.working_dir}/site/newsletter/2021_02/index.html\"\n    with open(newsletter_path, \"r\") as newsletter_file:\n        newsletter = newsletter_file.read()\n    assert \"&lt;title&gt;February of 2021 - The Blue Book&lt;/title&gt;\" in newsletter\n</code></pre> <p>That test is meant to ensure that our plugin works with the MkDocs ecosystem, so the assertions should be done against the created html files.</p> <p>If your functionality can't be covered by the happy path of the end-to-end test, it's better to create unit tests to make sure that they work as you want.</p> <p>You can see a full example here.</p>"}, {"location": "linux/mkdocs/#issues", "title": "Issues", "text": "<p>Once they are closed:</p> <ul> <li>Mkdocs Deprecation warning,   once it's solved remove the warning filter on mkdocs-newsletter   <code>pyproject.toml</code>.</li> <li>Mkdocs-Material Deprecation warning,   once it's solved remove the warning filter on mkdocs-newsletter   <code>pyproject.toml</code>.</li> </ul>"}, {"location": "linux/mkdocs/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Homepage.</li> <li>Material theme configuration guide</li> </ul>"}, {"location": "linux/mkdocs/#plugin-development_1", "title": "Plugin development", "text": "<ul> <li>User guide</li> <li>List of events</li> <li>Plugin testing example</li> </ul>"}, {"location": "linux/monica/", "title": "Monica", "text": "<p>Monica is an open-source web application to organize the interactions with your loved ones. They call it a PRM, or Personal Relationship Management. Think of it as a CRM (a popular tool used by sales teams in the corporate world) for your friends or family.</p> <p>Monica allows people to keep track of everything that's important about their friends and family. Like the activities done with them. When you last called someone. What you talked about. It will help you remember the name and the age of the kids. It can also remind you to call someone you haven't talked to in a while.</p> <p>They have pricing plans for their hosted service, but the self-hosted solution has all the features.</p> <p>It also has a nice API to interact with.</p>"}, {"location": "linux/monica/#install", "title": "Install", "text": "<p>They provide a very throughout documented Docker installation.</p> <p>If you just want to test it, use this docker compose</p> <p>File: docker-compose.yml</p> <pre><code>version: \"3.4\"\n\nservices:\n  app:\n    image: monicahq/monicahq\n    depends_on:\n      - db\n    ports:\n      - 8080:80\n    environment:\n      # generate with `pwgen -s 32 1` for instance:\n      - APP_KEY=DoKMvhGu795QcMBP1I5sw8uk85MMAPS9\n      - DB_HOST=db\n    volumes:\n      - data:/var/www/monica/storage\n    restart: always\n\n  db:\n    image: mysql:5.7\n    environment:\n      - MYSQL_RANDOM_ROOT_PASSWORD=true\n      - MYSQL_DATABASE=monica\n      - MYSQL_USER=homestead\n      - MYSQL_PASSWORD=secret\n    volumes:\n      - mysql:/var/lib/mysql\n    restart: always\n\nvolumes:\n  data:\n    name: data\n  mysql:\n    name: mysql\n</code></pre> <p>Once you install your own, you may want to:</p> <ul> <li>Change the <code>APP_KEY</code></li> <li>Change the database credentials. In the application docker are loaded as     <code>DB_USERNAME</code>, <code>DB_HOST</code> and <code>DB_PASSWORD</code>.</li> <li>Set up the environment and the application url with <code>APP_ENV=production</code> and     <code>APP_URL</code>.</li> <li> <p>Set up the email configuration</p> <pre><code>MAIL_MAILER: smtp\nMAIL_HOST: smtp.service.com # ex: smtp.sendgrid.net\nMAIL_PORT: 587 # is using tls, as you should\nMAIL_USERNAME: my_service_username # ex: apikey\nMAIL_PASSWORD: my_service_password # ex: SG.Psuoc6NZTrGHAF9fdsgsdgsbvjQ.JuxNWVYmJ8LE0\nMAIL_ENCRYPTION: tls\nMAIL_FROM_ADDRESS: no-reply@xxx.com # ex: email you want the email to be FROM\nMAIL_FROM_NAME: Monica # ex: name of the sender\n</code></pre> </li> </ul> <p>Here is an example of all the possible configurations.</p> <p>They also share other configuration examples where you can take ideas of alternate setups.</p> <p>If you don't want to use docker, check the other installation documentation.</p>"}, {"location": "linux/monica/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Git</li> <li>Docs</li> <li>Blog</li> </ul>"}, {"location": "linux/nodejs/", "title": "Nodejs", "text": "<p>Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine.</p>"}, {"location": "linux/nodejs/#install", "title": "Install", "text": "<p>The debian base repositories are really outdated, so add the NodeSource repository</p> <pre><code>curl -fsSL https://deb.nodesource.com/setup_16.x | bash -\napt-get install -y nodejs npm\nnodejs --version\n</code></pre>"}, {"location": "linux/nodejs/#links", "title": "Links", "text": "<ul> <li>Home</li> </ul>"}, {"location": "linux/rm/", "title": "rm", "text": "<p>rm definition</p> <p>In computing, rm (short for remove) is a basic command on Unix and Unix-like operating systems used to remove objects such as computer files, directories and symbolic links from file systems and also special files such as device nodes, pipes and sockets</p>"}, {"location": "linux/rm/#debugging", "title": "Debugging", "text": ""}, {"location": "linux/rm/#cannot-remove-file-structure-needs-cleaning", "title": "Cannot remove file: \u201cStructure needs cleaning\u201d", "text": "<p>From Victoria Stuart and Depressed Daniel answer</p> <p>You first need to:</p> <ul> <li>Umount the partition.</li> <li>Do a sector level backup of your disk.</li> <li> <p>If your filesystem is ext4 run:   <pre><code>fsck.ext4 {{ device }}\n</code></pre>   Accept all suggested fixes.</p> </li> <li> <p>Mount again the partition.</p> </li> </ul>"}, {"location": "linux/syncthing/", "title": "Syncthing", "text": "<p>Syncthing is a continuous file synchronization program. It synchronizes files between two or more computers in real time, safely protected from prying eyes. Your data is your data alone and you deserve to choose where it is stored, whether it is shared with some third party, and how it's transmitted over the internet.</p>"}, {"location": "linux/syncthing/#installation", "title": "Installation", "text": ""}, {"location": "linux/syncthing/#debian-or-ubuntu", "title": "Debian or Ubuntu", "text": "<pre><code># Add the release PGP keys:\ncurl -s https://syncthing.net/release-key.txt | sudo apt-key add -\n\n# Add the \"stable\" channel to your APT sources:\necho \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list\n\n# Update and install syncthing:\nsudo apt-get update\nsudo apt-get install syncthing\n</code></pre>"}, {"location": "linux/syncthing/#docker", "title": "Docker", "text": "<p>Use Linuxserver Docker</p>"}, {"location": "linux/syncthing/#configuration", "title": "Configuration", "text": "<p>If you're only going to use syncthing in an internal network, or you're going to fix the IPs of the devices you can disable the Global Discovery and Relaying connections so that you don't leak the existence of your services to the syncthing servers.</p>"}, {"location": "linux/syncthing/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "linux/syncthing/#syncthing-over-tor", "title": "Syncthing over Tor", "text": "<p>There are many posts on this topic (1, 2) but I wasn't able to connect two clients through Tor. Here are the steps I took in case anyone is interested. If you make it work, please contact me.</p> <p>Suggest to use a relay, go to relays.syncthing.net to see the public ones. You need to add the required servers to the <code>Sync Protocol Listen Address</code> field, under <code>Actions</code> and <code>Settings</code>. The syntax is:</p> <pre><code>relay://&lt;host name|IP&gt;[:port]/?id=&lt;relay device ID&gt;\n</code></pre> <p>The only way I've found to get the <code>relay device ID</code> is setting a fake one, and getting the correct one from the logs of syncthing. It will say that <code>the fingerprint ( what you put ) doesn't match ( actual fingerprint )</code>.</p>"}, {"location": "linux/syncthing/#steps", "title": "Steps", "text": "<ul> <li> <p>Configure the client:</p> <pre><code>export all_proxy=socks5://127.0.0.1:9058\nexport ALL_PROXY_NO_FALLBACK=1\nsyncthing --home /tmp/syncthing_1\n</code></pre> </li> <li> <p>Allow the connection to the local server:</p> <pre><code>sudo iptables -I OUTPUT -o lo -p tcp --dport 8384 -j ACCEPT\n</code></pre> </li> <li> <p>If you're using Tails and Tor Browser, you'll need to set the <code>about:config</code>     setting <code>network.proxy.allow_hijacking_localhost</code> to <code>false</code>. Otherwise you     won't be able to access the user interface.</p> </li> </ul>"}, {"location": "linux/syncthing/#issues", "title": "Issues", "text": "<ul> <li>Wifi run condition needs location to be turned     on: update and     check that you no longer need it.</li> </ul>"}, {"location": "linux/syncthing/#links", "title": "Links", "text": "<ul> <li>Home</li> <li>Getting Started</li> </ul>"}, {"location": "linux/wireguard/", "title": "Wireguard", "text": "<p>Wireguard is an simple yet fast and modern VPN that utilizes state-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN. WireGuard is a general purpose VPN for running on embedded interfaces and super computers alike. Initially released for the Linux kernel, it's now cross-platform (Windows, macOS, BSD, iOS, Android) and widely deployable. Although it's under heavy development, it already might be the most secure, easiest to use, and simplest VPN solution in the industry.</p> <p>Features:</p> <ul> <li> <p>Simple and easy to use: WireGuard aims to be as easy to configure and deploy     as SSH. A VPN connection is made by exchanging public keys \u2013 exactly like     exchanging SSH keys \u2013 and all the rest is transparently handled by     WireGuard. It's even capable of roaming between IP addresses, like     Mosh. There is no need to manage connections, worry about state,     manage daemons, or worry about what's under the hood. WireGuard presents a     basic yet powerful interface.</p> </li> <li> <p>Cryptographically Sound: WireGuard uses state-of-the-art cryptography, such as     the Noise protocol framework, Curve25519, ChaCha20, Poly1305, BLAKE2,     SipHash24, HKDF, and secure trusted constructions. It makes conservative and     reasonable choices and has been reviewed by cryptographers.</p> </li> <li> <p>Minimal Attack Surface: WireGuard is designed with ease-of-implementation and     simplicity in mind. It's meant to be implemented in very few lines of code,     and auditable for security vulnerabilities. Compared to behemoths like     *Swan/IPsec or OpenVPN/OpenSSL, in which auditing the gigantic codebases is     an overwhelming task even for large teams of security experts, WireGuard is     meant to be comprehensively reviewable by single individuals.</p> </li> <li> <p>High Performance: A combination of extremely high-speed cryptographic     primitives and the fact that WireGuard lives inside the Linux kernel means     that secure networking can be very high-speed. It is suitable for both small     embedded devices like smartphones and fully loaded backbone routers.</p> </li> <li> <p>Well Defined &amp; Thoroughly Considered: WireGuard is the result of a lengthy and     thoroughly considered academic process, resulting in the technical     whitepaper, an academic research paper which clearly defines the protocol     and the intense considerations that went into each decision.</p> </li> </ul> <p>Plus it's created by the same guy as <code>pass</code>, which uses Gentoo, I like this guy.</p>"}, {"location": "linux/wireguard/#conceptual-overview", "title": "Conceptual Overview", "text": "<p>WireGuard securely encapsulates IP packets over UDP. You add a WireGuard interface, configure it with your private key and your peers' public keys, and then you send packets across it. All issues of key distribution and pushed configurations are out of scope of WireGuard; these are issues much better left for other layers. It mimics the model of SSH and Mosh; both parties have each other's public keys, and then they're simply able to begin exchanging packets through the interface.</p>"}, {"location": "linux/wireguard/#simple-network-interface", "title": "Simple Network Interface", "text": "<p>WireGuard works by adding network interfaces, called wg0 (or wg1, wg2, wg3, etc). This network interface can then be configured normally using the ordinary networking utilities. The specific WireGuard aspects of the interface are configured using the <code>wg</code> tool. This interface acts as a tunnel interface.</p> <p>WireGuard associates tunnel IP addresses with public keys and remote endpoints. When the interface sends a packet to a peer, it does the following:</p> <ul> <li>This packet is meant for 192.168.30.8. Which peer is that? Let me look...     Okay, it's for peer ABCDEFGH. (Or if it's not for any configured peer, drop     the packet.)</li> <li>Encrypt entire IP packet using peer ABCDEFGH's public key.</li> <li>What is the remote endpoint of peer ABCDEFGH? Let me look... Okay, the     endpoint is UDP port 53133 on host 216.58.211.110.</li> <li>Send encrypted bytes from step 2 over the Internet to 216.58.211.110:53133     using UDP.</li> </ul> <p>When the interface receives a packet, this happens:</p> <ul> <li>I just got a packet from UDP port 7361 on host 98.139.183.24. Let's decrypt     it!</li> <li>It decrypted and authenticated properly for peer LMNOPQRS. Okay, let's     remember that peer LMNOPQRS's most recent Internet endpoint is     98.139.183.24:7361 using UDP.</li> <li>Once decrypted, the plain-text packet is from 192.168.43.89. Is peer LMNOPQRS     allowed to be sending us packets as 192.168.43.89?</li> <li>If so, accept the packet on the interface. If not, drop it.</li> </ul> <p>Behind the scenes there is much happening to provide proper privacy, authenticity, and perfect forward secrecy, using state-of-the-art cryptography.</p>"}, {"location": "linux/wireguard/#cryptokey-routing", "title": "Cryptokey Routing", "text": "<p>At the heart of WireGuard is a concept called Cryptokey Routing, which works by associating public keys with a list of tunnel IP addresses that are allowed inside the tunnel. Each network interface has a private key and a list of peers. Each peer has a public key.</p> <p>For example, a server computer might have this configuration:</p> <pre><code>[Interface]\nPrivateKey = yAnz5TF+lXXJte14tji3zlMNq+hd2rYUIgJBgB3fBmk=\nListenPort = 51820\n\n[Peer]\nPublicKey = xTIBA5rboUvnH4htodjb6e697QjLERt1NAB4mZqp8Dg=\nAllowedIPs = 10.192.122.3/32, 10.192.124.1/24\n\n[Peer]\nPublicKey = TrMvSoP4jYQlY6RIzBgbssQqY3vxI2Pi+y71lOWWXX0=\nAllowedIPs = 10.192.122.4/32, 192.168.0.0/16\n\n[Peer]\nPublicKey = gN65BkIKy1eCE9pP1wdc8ROUtkHLF2PfAqYdyYBz6EA=\nAllowedIPs = 10.10.10.230/32\n</code></pre> <p>And a client computer might have this simpler configuration:</p> <pre><code>[Interface]\nPrivateKey = gI6EdUSYvn8ugXOt8QQD6Yc+JyiZxIhp3GInSWRfWGE=\nListenPort = 21841\n\n[Peer]\nPublicKey = HIgo9xNzJMWLKASShiTqIybxZ0U3wGLiUeJ1PKf8ykw=\nEndpoint = 192.95.5.69:51820\nAllowedIPs = 0.0.0.0/0\n</code></pre> <p>In the server configuration, each peer (a client) will be able to send packets to the network interface with a source IP matching his corresponding list of allowed IPs. For example, when a packet is received by the server from peer gN65BkIK..., after being decrypted and authenticated, if its source IP is 10.10.10.230, then it's allowed onto the interface; otherwise it's dropped.</p> <p>In the server configuration, when the network interface wants to send a packet to a peer (a client), it looks at that packet's destination IP and compares it to each peer's list of allowed IPs to see which peer to send it to. For example, if the network interface is asked to send a packet with a destination IP of 10.10.10.230, it will encrypt it using the public key of peer gN65BkIK..., and then send it to that peer's most recent Internet endpoint.</p> <p>In the client configuration, its single peer (the server) will be able to send packets to the network interface with any source IP (since 0.0.0.0/0 is a wildcard). For example, when a packet is received from peer HIgo9xNz..., if it decrypts and authenticates correctly, with any source IP, then it's allowed onto the interface; otherwise it's dropped.</p> <p>In the client configuration, when the network interface wants to send a packet to its single peer (the server), it will encrypt packets for the single peer with any destination IP address (since 0.0.0.0/0 is a wildcard). For example, if the network interface is asked to send a packet with any destination IP, it will encrypt it using the public key of the single peer HIgo9xNz..., and then send it to the single peer's most recent Internet endpoint.</p> <p>In other words, when sending packets, the list of allowed IPs behaves as a sort of routing table, and when receiving packets, the list of allowed IPs behaves as a sort of access control list.</p> <p>This is what we call a Cryptokey Routing Table: the simple association of public keys and allowed IPs.</p> <p>Because all packets sent on the WireGuard interface are encrypted and authenticated, and because there is such a tight coupling between the identity of a peer and the allowed IP address of a peer, system administrators do not need complicated firewall extensions, such as in the case of IPsec, but rather they can simply match on \"is it from this IP? on this interface?\", and be assured that it is a secure and authentic packet. This greatly simplifies network management and access control, and provides a great deal more assurance that your iptables rules are actually doing what you intended for them to do.</p>"}, {"location": "linux/wireguard/#built-in-roaming", "title": "Built-in Roaming", "text": "<p>The client configuration contains an initial endpoint of its single peer (the server), so that it knows where to send encrypted data before it has received encrypted data. The server configuration doesn't have any initial endpoints of its peers (the clients). This is because the server discovers the endpoint of its peers by examining from where correctly authenticated data originates. If the server itself changes its own endpoint, and sends data to the clients, the clients will discover the new server endpoint and update the configuration just the same. Both client and server send encrypted data to the most recent IP endpoint for which they authentically decrypted data. Thus, there is full IP roaming on both ends.</p>"}, {"location": "linux/zfs/", "title": "ZFS", "text": "<p>OpenZFS is a file system with volume management capabilities designed specifically for storage servers.</p> <p>Some neat features of ZFS include:</p> <ul> <li>Aggregating multiple physical disks into a single filesystem.</li> <li>Automatically repairing data corruption.</li> <li>Creating point-in-time snapshots of data on disk.</li> <li>Optionally encrypting or compressing data on disk.</li> </ul>"}, {"location": "linux/zfs/#usage", "title": "Usage", "text": ""}, {"location": "linux/zfs/#mount-a-pool-as-readonly", "title": "Mount a pool as readonly", "text": "<pre><code>zpool import -o readonly=on {{ pool_name }}\n</code></pre>"}, {"location": "linux/zfs/#mount-a-zfs-snapshot-in-a-directory-as-readonly", "title": "Mount a ZFS snapshot in a directory as readonly", "text": "<pre><code>mount -t zfs {{ pool_name }}/{{ snapshot_name }} {{ mount_path }} -o ro\n</code></pre>"}, {"location": "linux/zfs/#list-pools", "title": "List pools", "text": "<pre><code>zpool list\n</code></pre>"}, {"location": "linux/zfs/#list-the-filesystems", "title": "List the filesystems", "text": "<pre><code>zfs list\n</code></pre>"}, {"location": "linux/zfs/#get-read-and-write-stats-from-pool", "title": "Get read and write stats from pool", "text": "<pre><code>zpool iostat -v {{ pool_name }} {{ refresh_time_in_seconds }}\n</code></pre>"}, {"location": "linux/zfs/#get-all-properties-of-a-pool", "title": "Get all properties of a pool", "text": "<pre><code>zpool get all {{ pool_name }}\n</code></pre>"}, {"location": "linux/zfs/#get-all-properties-of-a-filesystem", "title": "Get all properties of a filesystem", "text": "<pre><code>zfs get all {{ pool_name }}\n</code></pre>"}, {"location": "linux/zfs/#installation", "title": "Installation", "text": ""}, {"location": "linux/zfs/#install-the-required-programs", "title": "Install the required programs", "text": "<p>OpenZFS is not in the mainline kernel for license issues (fucking capitalism...) so it's not yet suggested to use it for the root of your filesystem. </p> <p>To install it in a Debian device:</p> <ul> <li>ZFS packages are included in the <code>contrib</code> repository, but the <code>backports</code> repository often provides newer releases of ZFS. You can use it as follows.</li> </ul> <p>Add the backports repository:</p> <pre><code>vi /etc/apt/sources.list.d/bullseye-backports.list\n</code></pre> <pre><code>deb http://deb.debian.org/debian bullseye-backports main contrib\ndeb-src http://deb.debian.org/debian bullseye-backports main contrib\n</code></pre> <pre><code>vi /etc/apt/preferences.d/90_zfs\n</code></pre> <pre><code>Package: src:zfs-linux\nPin: release n=bullseye-backports\nPin-Priority: 990\n</code></pre> <ul> <li>Install the packages:</li> </ul> <pre><code>apt update\napt install dpkg-dev linux-headers-generic linux-image-generic\napt install zfs-dkms zfsutils-linux\n</code></pre> <p>BE CAREFUL: if root doesn't have <code>sbin</code> in the <code>PATH</code> you will get an error of loading the zfs module as it's not signed. If this happens to you reinstall or try the debugging I did (which didn't work).</p>"}, {"location": "linux/zfs/#create-your-pool", "title": "Create your pool", "text": "<p>First read the ZFS storage planning article and then create your <code>main</code> pool with a command similar to:</p> <pre><code>zpool create \\\n-o ashift=12 \\ \n-o autoexpand=on \\ \n-o compression=lz4 \\\nmain raidz /dev/sda /dev/sdb /dev/sdc /dev/sdd \\\nlog mirror \\\n/dev/disk/by-id/nvme-eui.e823gqkwadgp32uhtpobsodkjfl2k9d0-part4 \\\n/dev/disk/by-id/nvme-eui.a0sdgohosdfjlkgjwoqkegdkjfl2k9d0-part4 \\\ncache \\\n/dev/disk/by-id/nvme-eui.e823gqkwadgp32uhtpobsodkjfl2k9d0-part5 \\\n/dev/disk/by-id/nvme-eui.a0sdgohosdfjlkgjwoqkegdkjfl2k9d0-part5 \\\n</code></pre> <p>Where:</p> <ul> <li><code>-o ashift=12</code>: Adjusts the disk sector size to the disks in use.</li> <li><code>-o canmount=off</code>:  Don't mount the main pool, we'll mount the filesystems.</li> <li><code>-o compression=lz4</code>: Enable compression by default</li> <li><code>/dev/sda /dev/sdb /dev/sdc /dev/sdd</code> are the rotational data disks configured in RAIDZ1</li> <li>We set two partitions in mirror for the ZLOG</li> <li>We set two partitions in stripe for the L2ARC</li> </ul> <p>If you don't want the main pool to be mounted use <code>zfs set mountpoint=none main</code>.</p>"}, {"location": "linux/zfs/#create-your-filesystems", "title": "Create your filesystems", "text": "<p>Once we have the pool you can create the different filesystems. If you want to use encryption with a key follow the next steps:</p> <pre><code>mkdir /etc/zfs/keys\nchmod 700 /etc/zfs/keys\ndd if=/dev/random of=/etc/zfs/keys/home.key bs=1 count=32\n</code></pre> <p>Then create the filesystem:</p> <pre><code>zfs create \\ \n-o mountpoint=/home/lyz \\\n-o encryption=on \\\n-o keyformat=raw \\\n-o keylocation=file:///etc/zfs/keys/home.key \\\nmain/lyz\n</code></pre> <p>I'm assuming that <code>compression</code> was set in the pool.</p> <p>You can check the created filesystems with <code>zfs list</code></p>"}, {"location": "linux/zfs/#enable-the-autoloading-of-datasets-on-boot", "title": "Enable the autoloading of datasets on boot", "text": "<p>It is possible to automatically unlock a pool dataset on boot time by using a systemd unit. For example create the following service to unlock any specific dataset:</p> <pre><code>/etc/systemd/system/zfs-load-key.service\n</code></pre> <pre><code>[Unit]\nDescription=Load encryption keys\nDefaultDependencies=no\nAfter=zfs-import.target\nBefore=zfs-mount.service\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nExecStart=/usr/bin/zfs load-key -a\nStandardInput=tty-force\n\n[Install]\nWantedBy=zfs-mount.service\n</code></pre> <pre><code>systemctl start zfs-load-key.service\nsystemctl enable zfs-load-key.service\nreboot\n</code></pre>"}, {"location": "linux/zfs/#configure-nfs", "title": "Configure NFS", "text": "<p>With ZFS you can share a specific dataset via NFS. If for whatever reason the dataset does not mount, then the export will not be available to the application, and the NFS client will be blocked.</p> <p>You still must install the necessary daemon software to make the share available. For example, if you wish to share a dataset via NFS, then you need to install the NFS server software, and it must be running. Then, all you need to do is flip the sharing NFS switch on the dataset, and it will be immediately available.</p>"}, {"location": "linux/zfs/#install-nfs", "title": "Install NFS", "text": "<p>To share a dataset via NFS, you first need to make sure the NFS daemon is running. On Debian and Ubuntu, this is the <code>nfs-kernel-server</code> package. </p> <pre><code>sudo apt-get install nfs-kernel-server\n</code></pre> <p>Further, with Debian and Ubuntu, the NFS daemon will not start unless there is an export in the <code>/etc/exports</code> file. So, you have two options: you can create a dummy export, only available to localhost, or you can edit the init script to start without checking for a current export. I prefer the former. Let's get that setup:</p> <pre><code>echo '/tmp localhost(ro)' &gt;&gt; /etc/exports\n$ sudo /etc/init.d/nfs-kernel-server start\n$ showmount -e hostname.example.com\nExport list for hostname.example.com:\n/mnt localhost\n</code></pre> <p>With our NFS daemon running, we can now start sharing ZFS datasets. The <code>sharenfs</code> property can be <code>on</code>, <code>off</code> or <code>opts</code>, where <code>opts</code> are valid NFS export options. So, if you want to share the <code>pool/srv</code> dataset, which is mounted to <code>/srv</code> to the <code>10.80.86.0/24</code> network you could run:</p> <pre><code># zfs set sharenfs=\"rw=@10.80.86.0/24\" pool/srv\n# zfs share pool/srv\n# showmount -e hostname.example.com\nExport list for hostname.example.com:\n/srv 10.80.86.0/24\n/mnt localhost\n</code></pre> <p>If you need to share to multiple subnets, you would do something like:</p> <pre><code>sudo zfs set sharenfs=\"rw=@192.168.0.0/24,rw=@10.0.0.0/24\" pool-name/dataset-name\n</code></pre> <p>If you need <code>root</code> to be able to write to the directory enable the <code>no_root_squash</code> NFS option</p> <p>root_squash \u2014 Prevents root users connected remotely from having root privileges and assigns them the user ID for the user nfsnobody. This effectively \"squashes\" the power of the remote root user to the lowest local user, preventing unauthorized alteration of files on the remote server. Alternatively, the no_root_squash option turns off root squashing. To squash every remote user, including root, use the all_squash option. To specify the user and group IDs to use with remote users from a particular host, use the anonuid and anongid options, respectively. In this case, a special user account can be created for remote NFS users to share and specify (anonuid=,anongid=), where is the user ID number and is the group ID number.</p> <p>You should now be able to mount the NFS export from an NFS client. Install the client with:</p> <pre><code>sudo apt-get install nfs-common\n</code></pre> <p>And then mount it with:</p> <pre><code>mount -t nfs hostname.example.com:/srv /mnt\n</code></pre> <p>To permanently mount it you need to add it to your <code>/etc/fstab</code>, check this section for more details.</p>"}, {"location": "linux/zfs/#backup", "title": "Backup", "text": "<p>Please remember that RAID is not a backup, it guards against one kind of hardware failure. There's lots of failure modes that it doesn't guard against though:</p> <ul> <li>File corruption</li> <li>Human error (deleting files by mistake)</li> <li>Catastrophic damage (someone dumps water onto the server)</li> <li>Viruses and other malware</li> <li>Software bugs that wipe out data</li> <li>Hardware problems that wipe out data or cause hardware damage (controller malfunctions, firmware bugs, voltage spikes, ...)</li> </ul> <p>That's why you still need to make backups.</p> <p>ZFS has the builtin feature to make snapshots of the pool. A snapshot is a first class read-only filesystem. It is a mirrored copy of the state of the filesystem at the time you took the snapshot. They are persistent across reboots, and they don't require any additional backing store; they use the same storage pool as the rest of your data. </p> <p>If you remember ZFS's awesome nature of copy-on-write filesystems, you will remember the discussion about Merkle trees. A ZFS snapshot is a copy of the Merkle tree in that state, except we make sure that the snapshot of that Merkle tree is never modified.</p> <p>Creating snapshots is near instantaneous, and they are cheap. However, once the data begins to change, the snapshot will begin storing data. If you have multiple snapshots, then multiple deltas will be tracked across all the snapshots. However, depending on your needs, snapshots can still be exceptionally cheap.</p>"}, {"location": "linux/zfs/#zfs-snapshot-lifecycle-management", "title": "ZFS snapshot lifecycle management", "text": "<p>ZFS doesn't though have a clean way to manage the lifecycle of those snapshots. There are many tools to fill the gap:</p> <ul> <li><code>sanoid</code>: Made in Perl, 2.4k stars, last commit April 2022, last release April 2021</li> <li>zfs-auto-snapshot: Made in Bash, 767 stars, last commit/release on September 2019</li> <li>pyznap: Made in Python, 176 stars, last commit/release on September 2020</li> <li>Custom scripts.</li> </ul> <p>It seems that the state of the art of ZFS backups is not changing too much in the last years, possibly because the functionality is covered so there is no need for further development. So I'm going to manage the backups with <code>sanoid</code> despite it being done in Perl because it's the most popular, it looks simple but flexible for complex cases, and it doesn't look I'd need to tweak the code.</p>"}, {"location": "linux/zfs/#restore-a-backup", "title": "Restore a backup", "text": "<p>You can list the available snapshots of a filesystem with <code>zfs list -t snapshot {{ pool_or_filesystem_name }}</code>, if you don't specify the <code>pool_or_filesystem_name</code> it will show all available snapshots.</p> <p>You have two ways to restore a backup:</p> <ul> <li>Mount the snapshot in a directory and manually copy the needed files:</li> </ul> <pre><code>mount -t zfs main/lyz@autosnap_2023-02-17_13:15:06_hourly /mnt\n</code></pre> <p>To umount the snapshot run <code>umount /mnt</code>.</p> <ul> <li>Rolling back the filesystem to the snapshot state: Rolling back to a previous snapshot will discard any data changes between that snapshot and the current time. Further, by default, you can only rollback to the most recent snapshot. In order to rollback to an earlier snapshot, you must destroy all snapshots between the current time and that snapshot you wish to rollback to. If that's not enough, the filesystem must be unmounted before the rollback can begin. This means downtime.</li> </ul> <p>To rollback the \"tank/test\" dataset to the \"tuesday\" snapshot, we would issue:</p> <pre><code>$: zfs rollback tank/test@tuesday\ncannot rollback to 'tank/test@tuesday': more recent snapshots exist\nuse '-r' to force deletion of the following snapshots:\ntank/test@wednesday\ntank/test@thursday\n</code></pre> <p>As expected, we must remove the <code>@wednesday</code> and <code>@thursday</code> snapshots before we can rollback to the <code>@tuesday</code> snapshot.</p>"}, {"location": "linux/zfs/#learning", "title": "Learning", "text": "<p>I've found that learning about ZFS was an interesting, intense and time consuming task. If you want a quick overview check this video. If you prefer to read, head to the awesome Aaron Toponce articles and read all of them sequentially, each is a jewel. The docs on the other hand are not that pleasant to read. For further information check JRS articles.</p>"}, {"location": "linux/zfs/#resources", "title": "Resources", "text": "<ul> <li>Aaron Toponce articles</li> <li>Docs</li> <li>JRS articles</li> <li>ZFS basic introduction video</li> </ul>"}, {"location": "linux/zip/", "title": "zip", "text": "<p><code>zip</code> is an UNIX command line tool to package and compress files.</p>"}, {"location": "linux/zip/#usage", "title": "Usage", "text": ""}, {"location": "linux/zip/#create-a-zip-file", "title": "Create a zip file", "text": "<pre><code>zip -r {{ zip_file }} {{ files_to_save }}\n</code></pre>"}, {"location": "linux/zip/#split-files-to-a-specific-size", "title": "Split files to a specific size", "text": "<pre><code>zip -s {{ size }} -r {{ destination_zip }} {{ files }}\n</code></pre> <p>Where <code>{{ size }}</code> can be <code>950m</code></p>"}, {"location": "linux/zip/#compress-with-password", "title": "Compress with password", "text": "<pre><code>zip -er {{ zip_file }} {{ files_to_save }}\n</code></pre>"}, {"location": "linux/zip/#read-files-to-compress-from-a-file", "title": "Read files to compress from a file", "text": "<pre><code>cat {{ files_to_compress.txt }} | zip -er {{ destination_zip }} -@\n</code></pre>"}, {"location": "linux/zip/#uncompress-a-zip-file", "title": "Uncompress a zip file", "text": "<pre><code>unzip {{ zip_file }}\n</code></pre>"}, {"location": "linux/luks/luks/", "title": "LUKS", "text": "<p>LUKS definition</p> <p>The Linux Unified Key Setup (LUKS) is a disk encryption specification created by Clemens Fruhwirth in 2004 and was originally intended for Linux.</p> <p>While most disk encryption software implements different, incompatible, and undocumented formats, LUKS implements a platform-independent standard on-disk format for use in various tools. This not only facilitates compatibility and interoperability among different programs, but also assures that they all implement password management in a secure and documented manner.</p> <p>The reference implementation for LUKS operates on Linux and is based on an enhanced version of cryptsetup, using dm-crypt as the disk encryption backend.</p> <p>LUKS is designed to conform to the TKS1 secure key setup scheme.</p>"}, {"location": "linux/luks/luks/#luks-commands", "title": "LUKS Commands", "text": "<p>We use the <code>cryptsetup</code> command to interact with LUKS partitions.</p>"}, {"location": "linux/luks/luks/#header-management", "title": "Header management", "text": ""}, {"location": "linux/luks/luks/#get-the-disk-header", "title": "Get the disk header", "text": "<pre><code>cryptsetup luksDump /dev/sda3\n</code></pre>"}, {"location": "linux/luks/luks/#backup-header", "title": "Backup header", "text": "<pre><code>cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }}\n</code></pre>"}, {"location": "linux/luks/luks/#key-management", "title": "Key management", "text": ""}, {"location": "linux/luks/luks/#add-a-key", "title": "Add a key", "text": "<pre><code>cryptsetup luksAddKey --key-slot 1 {{ luks_device }}\n</code></pre>"}, {"location": "linux/luks/luks/#change-a-key", "title": "Change a key", "text": "<pre><code>cryptsetup luksChangeKey {{ luks_device }} -s 0\n</code></pre>"}, {"location": "linux/luks/luks/#test-if-you-remember-the-key", "title": "Test if you remember the key", "text": "<p>Try to add a new key and cancel the process</p> <pre><code>cryptsetup luksAddKey --key-slot 3 {{ luks_device }}\n</code></pre>"}, {"location": "linux/luks/luks/#delete-some-keys", "title": "Delete some keys", "text": "<pre><code>cryptsetup luksDump {{ device }}\ncryptsetup luksKillSlot {{ device }} {{ slot_number }}\n</code></pre>"}, {"location": "linux/luks/luks/#delete-all-keys", "title": "Delete all keys", "text": "<pre><code>cryptsetup luksErase {{ device }}\n</code></pre>"}, {"location": "linux/luks/luks/#encrypt-hard-drive", "title": "Encrypt hard drive", "text": "<ul> <li>Configure LUKS partition</li> </ul> <pre><code>cryptsetup -y -v luksFormat /dev/sdg\n</code></pre> <ul> <li>Open the container</li> </ul> <pre><code>cryptsetup luksOpen /dev/sdg crypt\n</code></pre> <ul> <li>Fill it with zeros</li> </ul> <pre><code>pv -tpreb /dev/zero | dd of=/dev/mapper/crypt bs=128M\n</code></pre> <ul> <li>Make filesystem   <pre><code>mkfs.ext4 /dev/mapper/crypt\n</code></pre></li> </ul>"}, {"location": "linux/luks/luks/#break-a-luks-password", "title": "Break a luks password", "text": "<p>You can use <code>bruteforce-luks</code></p>"}, {"location": "linux/luks/luks/#luks-debugging", "title": "LUKS debugging", "text": ""}, {"location": "linux/luks/luks/#resource-busy", "title": "Resource busy", "text": "<ul> <li>Umount the lv first</li> </ul> <pre><code>lvscan\nlvchange -a n {{ partition_name }}\n</code></pre> <ul> <li>Then close the luks device</li> </ul> <pre><code>cryptsetup luksClose {{ device_name }}\n</code></pre>"}, {"location": "linux/vim/vim_plugins/", "title": "Vim plugins", "text": ""}, {"location": "linux/vim/vim_plugins/#black", "title": "Black", "text": "<p>To install Black you first need <code>python3-venv</code>.</p> <pre><code>sudo apt-get install python3-venv\n</code></pre> <p>Add the plugin and configure it so vim runs it each time you save.</p> <p>File <code>~/.vimrc</code></p> <pre><code>Plugin 'psf/black'\n\n\" Black\nautocmd BufWritePre *.py execute ':Black'\n</code></pre> <p>A configuration issue exists for neovim. If you encounter the error <code>AttributeError: module 'black' has no attribute 'find_pyproject_toml'</code>, do the following:</p> <pre><code>cd ~/.vim/bundle/black\ngit checkout 19.10b0\n</code></pre> <p>As the default line length is 88 (ugly number by the way), we need to change the indent, python-mode configuration as well</p> <pre><code>\"\" python indent\nautocmd BufNewFile,BufRead *.py setlocal foldmethod=indent tabstop=4 softtabstop=4 shiftwidth=4 textwidth=88 smarttab expandtab\n\n\" python-mode\nlet g:pymode_options_max_line_length = 88\nlet g:pymode_lint_options_pep8 = {'max_line_length': g:pymode_options_max_line_length}\n</code></pre>"}, {"location": "linux/vim/vim_plugins/#ale", "title": "ALE", "text": "<p>ALE (Asynchronous Lint Engine) is a plugin providing linting (syntax checking and semantic errors) in NeoVim 0.2.0+ and Vim 8 while you edit your text files, and acts as a Vim Language Server Protocol client.</p> <p>ALE makes use of NeoVim and Vim 8 job control functions and timers to run linters on the contents of text buffers and return errors as text changes in Vim. This allows for displaying warnings and errors in files before they are saved back to a filesystem.</p> <p>In other words, this plugin allows you to lint while you type.</p> <p>ALE offers support for fixing code with command line tools in a non-blocking manner with the <code>:ALEFix</code> feature, supporting tools in many languages, like prettier, eslint, autopep8, and more.</p>"}, {"location": "linux/vim/vim_plugins/#installation", "title": "Installation", "text": "<p>Install with Vundle:</p> <pre><code>Plugin 'dense-analysis/ale'\n</code></pre>"}, {"location": "linux/vim/vim_plugins/#configuration", "title": "Configuration", "text": "<pre><code>let g:ale_sign_error                  = '\u2718'\nlet g:ale_sign_warning                = '\u26a0'\nhighlight ALEErrorSign ctermbg        =NONE ctermfg=red\nhighlight ALEWarningSign ctermbg      =NONE ctermfg=yellow\nlet g:ale_linters_explicit            = 1\nlet g:ale_lint_on_text_changed        = 'normal'\n\" let g:ale_lint_on_text_changed        = 'never'\nlet g:ale_lint_on_enter               = 0\nlet g:ale_lint_on_save                = 1\nlet g:ale_fix_on_save                 = 1\n\nlet g:ale_linters = {\n\\  'markdown': ['markdownlint', 'writegood', 'alex', 'proselint'],\n\\  'json': ['jsonlint'],\n\\  'python': ['flake8', 'mypy', 'pylint', 'alex'],\n\\  'yaml': ['yamllint', 'alex'],\n\\   '*': ['alex', 'writegood'],\n\\}\n\nlet g:ale_fixers = {\n\\   '*': ['remove_trailing_lines', 'trim_whitespace'],\n\\   'json': ['jq'],\n\\   'python': ['isort'],\n\\   'terraform': ['terraform'],\n\\}\ninoremap &lt;leader&gt;e &lt;esc&gt;:ALENext&lt;cr&gt;\nnnoremap &lt;leader&gt;e :ALENext&lt;cr&gt;\ninoremap &lt;leader&gt;p &lt;esc&gt;:ALEPrevious&lt;cr&gt;\nnnoremap &lt;leader&gt;p :ALEPrevious&lt;cr&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>let g:ale_linters_explicit</code>: Prevent ALE load only the selected linters.</li> <li>use <code>&lt;leader&gt;e</code> and <code>&lt;leader&gt;p</code> to navigate through the warnings.</li> </ul> <p>If you feel that it's too heavy, use <code>ale_lint_on_enter</code> or increase the <code>ale_lint_delay</code>.</p> <p>Use <code>:ALEInfo</code> to see the ALE configuration and any errors when running <code>:ALEFix</code> for the specific buffer.</p>"}, {"location": "linux/vim/vim_plugins/#flakehell", "title": "Flakehell", "text": "<p>Flakehell is not supported yet. Until that issue is closed we need the following configuration:</p> <pre><code>let g:ale_python_flake8_executable = flake8helled\nlet g:ale_python_flake8_use_global = 1\n</code></pre>"}, {"location": "linux/vim/vim_plugins/#toggle-fixers-on-save", "title": "Toggle fixers on save", "text": "<p>There are cases when you don't want to run the fixers in your code.</p> <p>Ale doesn't have an option to do it, but zArubaru showed how to do it. If you add to your configuration</p> <pre><code>command! ALEToggleFixer execute \"let g:ale_fix_on_save = get(g:, 'ale_fix_on_save', 0) ? 0 : 1\"\n</code></pre> <p>You can then use <code>:ALEToggleFixer</code> to activate an deactivate them.</p>"}, {"location": "linux/vim/vim_plugins/#vim-easymotion", "title": "vim-easymotion", "text": "<p>EasyMotion provides a much simpler way to use some motions in vim. It takes the <code>&lt;number&gt;</code> out of <code>&lt;number&gt;w</code> or <code>&lt;number&gt;f{char}</code> by highlighting all possible choices and allowing you to press one key to jump directly to the target.</p> <p>When one of the available motions is triggered, all visible text preceding or following the cursor is faded, and motion targets are highlighted.</p>"}, {"location": "linux/vim/vim_plugins/#installation_1", "title": "Installation", "text": "<p>Add to Vundle <code>Plugin 'easymotion/vim-easymotion'</code></p> <p>The configuration can be quite complex, but I'm starting with the basics:</p> <pre><code>\" Easymotion\nlet g:EasyMotion_do_mapping = 0 \" Disable default mappings\nlet g:EasyMotion_keys='asdfghjkl'\n\n\" Jump to anywhere you want with minimal keystrokes, with just one key binding.\n\" `s{char}{label}`\nnmap s &lt;Plug&gt;(easymotion-overwin-f)\n\n\" JK motions: Line motions\nmap &lt;Leader&gt;j &lt;Plug&gt;(easymotion-j)\nmap &lt;Leader&gt;k &lt;Plug&gt;(easymotion-k)\n</code></pre> <p>It's awesome to move between windows with <code>s</code>.</p>"}, {"location": "linux/vim/vim_plugins/#vim-fugitive", "title": "Vim Fugitive", "text": ""}, {"location": "linux/vim/vim_plugins/#add-portions-of-file-to-the-index", "title": "Add portions of file to the index", "text": "<p>To stage only part of the file to a commit, open it and launch <code>:Gdiff</code>. With <code>diffput</code> and <code>diffobtain</code> Vim's functionality you move to the index file (the one in the left) the changes you want to stage.</p>"}, {"location": "linux/vim/vim_plugins/#prepare-environment-to-write-the-commit-message", "title": "Prepare environment to write the commit message", "text": "<p>When I write the commit message I like to review what changes I'm commiting. To do it I find useful to close all windows and do a vertical split with the changes so I can write the commit message in one of the window while I scroll down in the other. As the changes are usually no the at the top of the file, I scroll the window of the right to the first change and then switch back to the left one in insert mode to start writing.</p> <p>I've also made some movement remappings:</p> <ul> <li><code>jj</code>, <code>kk</code>, <code>&lt;C-d&gt;</code> and <code>&lt;C-u&gt;</code> in insert mode will insert normal mode and go     to the window in the right to continue seeing the changes.</li> <li><code>i</code>, <code>a</code>, <code>o</code>, <code>O</code>: if you are in the changes window it will go to the commit message window     in insert mode.</li> </ul> <p>Once I've made the commit I want to only retain one buffer.</p> <p>Add the following snippet to do just that:</p> <pre><code>\" Open commit message buffer in fullscreen with a vertical split, and close it with\n\" leader q\nau BufNewFile,BufRead *COMMIT_EDITMSG call CommitMessage()\n\nfunction! RestoreBindings()\n  inoremap jj &lt;esc&gt;j\n  inoremap kk &lt;esc&gt;k\n  inoremap &lt;C-d&gt; &lt;C-d&gt;\n  inoremap &lt;C-u&gt; &lt;C-u&gt;\n  nnoremap i i\n  nnoremap a a\n  nnoremap o o\n  nnoremap O O\nendfunction\n\nfunction! CommitMessage()\n  \" Remap the saving mappings\n  \" Close buffer when saving\n  inoremap &lt;silent&gt; &lt;leader&gt;q &lt;esc&gt;:w&lt;cr&gt; \\| :only&lt;cr&gt; \\| :call RestoreBindings()&lt;cr&gt; \\|:Sayonara&lt;CR&gt;\n  nnoremap &lt;silent&gt; &lt;leader&gt;q &lt;esc&gt;:w&lt;cr&gt; \\| :only&lt;cr&gt; \\| :call RestoreBindings()&lt;cr&gt; \\|:Sayonara&lt;CR&gt;\n\n  inoremap jj &lt;esc&gt;:wincmd l&lt;cr&gt;j\n  inoremap kk &lt;esc&gt;:wincmd l&lt;cr&gt;k\n  inoremap &lt;C-d&gt; &lt;esc&gt;:wincmd l&lt;cr&gt;&lt;C-d&gt;\n  inoremap &lt;C-u&gt; &lt;esc&gt;:wincmd l&lt;cr&gt;&lt;C-u&gt;\n  nnoremap i :wincmd h&lt;cr&gt;i\n  nnoremap a :wincmd h&lt;cr&gt;a\n  nnoremap o :wincmd h&lt;cr&gt;o\n  nnoremap O :wincmd h&lt;cr&gt;O\n\n  \" Remove bad habits\n  inoremap jk &lt;nop&gt;\n  inoremap ZZ &lt;nop&gt;\n  nnoremap ZZ &lt;nop&gt;\n  \" Close all other windows\n  only\n  \" Create a vertical split\n  vsplit\n  \" Go to the right split\n  wincmd l\n  \" Go to the first change\n  execute \"normal! /^diff\\&lt;cr&gt;8j\"\n  \" Clear the search highlights\n  nohl\n  \" Go back to the left split\n  wincmd h\n  \" Enter insert mode\n  execute \"startinsert\"\nendfunction\n</code></pre> <p>I'm assuming that you save with <code>&lt;leader&gt;w</code> and that you're using Sayonara to close your buffers.</p>"}, {"location": "linux/vim/vim_plugins/#git-push-sets-the-upstream-by-default", "title": "Git push sets the upstream by default", "text": "<p>Add to your config:</p> <pre><code>nnoremap &lt;leader&gt;gp :Git -c push.default=current push&lt;CR&gt;\n</code></pre> <p>If you want to see the output of the push command, use <code>:copen</code> after the successful push.</p>"}, {"location": "linux/vim/vim_plugins/#vim-test", "title": "Vim-test", "text": "<p>A Vim wrapper for running tests on different granularities.</p> <p>Currently the following testing frameworks are supported:</p> Language Frameworks Identifiers C# .NET <code>dotnettest</code> Clojure Fireplace.vim <code>fireplacetest</code> Crystal Crystal <code>crystalspec</code> Elixir ESpec, ExUnit <code>espec</code>, <code>exunit</code> Erlang CommonTest <code>commontest</code> Go Ginkgo, Go <code>ginkgo</code>, <code>gotest</code> Java Maven <code>maventest</code> JavaScript Intern, Jasmine, Jest, Karma, Lab, Mocha, TAP, <code>intern</code>, <code>jasmine</code>, <code>jest</code>, <code>karma</code>, <code>lab</code>, <code>mocha</code>, <code>tap</code> Lua Busted <code>busted</code> PHP Behat, Codeception, Kahlan, Peridot, PHPUnit, PHPSpec <code>behat</code>, <code>codeception</code>, <code>kahlan</code>, <code>peridot</code>, <code>phpunit</code>, <code>phpspec</code> Perl Prove <code>prove</code> Python Django, Nose, Nose2, PyTest, PyUnit <code>djangotest</code>, <code>djangonose</code> <code>nose</code>, <code>nose2</code>, <code>pytest</code>, <code>pyunit</code> Racket RackUnit <code>rackunit</code> Ruby Cucumber, [M], [Minitest][minitest], Rails, RSpec <code>cucumber</code>, <code>m</code>, <code>minitest</code>, <code>rails</code>, <code>rspec</code> Rust Cargo <code>cargotest</code> Shell Bats <code>bats</code> VimScript Vader.vim, VSpec <code>vader</code>, <code>vspec</code>"}, {"location": "linux/vim/vim_plugins/#features", "title": "Features", "text": "<ul> <li>Zero dependencies</li> <li>Zero configuration required (it Does the Right Thing\u2122, see   Philosophy)</li> <li>Wide range of test runners which are automagically detected</li> <li>Polyfills for nearest tests (by constructing regexes)</li> <li>Wide range of execution environments (\"strategies\")</li> <li>Fully customized CLI options configuration</li> <li>Extendable with new runners and strategies</li> </ul> <p>Test.vim consists of a core which provides an abstraction over running any kind of tests from the command-line. Concrete test runners are then simply plugged in, so they all work in the same unified way.</p>"}, {"location": "linux/vim/vim_plugins/#issues", "title": "Issues", "text": ""}, {"location": "linux/vim/vim_plugins/#vim-abolish", "title": "Vim-Abolish", "text": "<ul> <li>Error adding elipsis instead of three     dots: Pope said that it's     not possible :(.</li> </ul>"}, {"location": "linux/vim/vim_plugins/#references", "title": "References", "text": "<ul> <li>ALE supported tools</li> </ul>"}]}